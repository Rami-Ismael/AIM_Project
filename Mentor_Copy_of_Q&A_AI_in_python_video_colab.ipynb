{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mentor Copy of Q&A AI in python video colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "90c7ccc29f8149a5821eb364d69207ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a904036b5f7b4eef8727c4125e74278f",
              "IPY_MODEL_5fc3cdf504b748d999512f7470dad76a",
              "IPY_MODEL_4a1dd08e86d64c25a45bb4dd51fc4f2d"
            ],
            "layout": "IPY_MODEL_fcc54e5bcdc243f893f79027f039dac5"
          }
        },
        "a904036b5f7b4eef8727c4125e74278f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df58dd9af0d44635b567348fdbd00cfb",
            "placeholder": "​",
            "style": "IPY_MODEL_23cb5c248d444ec0a4d45935c480de0b",
            "value": "Downloading: 100%"
          }
        },
        "5fc3cdf504b748d999512f7470dad76a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47455852643840a781191e44bd4b732c",
            "max": 790,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c0802b556194c829741c11a4c31748b",
            "value": 790
          }
        },
        "4a1dd08e86d64c25a45bb4dd51fc4f2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2497771b0cae41ed8f4fe9e65b4674b9",
            "placeholder": "​",
            "style": "IPY_MODEL_7caac61e953e45659e48b5cf4d7fad65",
            "value": " 790/790 [00:00&lt;00:00, 25.6kB/s]"
          }
        },
        "fcc54e5bcdc243f893f79027f039dac5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df58dd9af0d44635b567348fdbd00cfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23cb5c248d444ec0a4d45935c480de0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47455852643840a781191e44bd4b732c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c0802b556194c829741c11a4c31748b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2497771b0cae41ed8f4fe9e65b4674b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7caac61e953e45659e48b5cf4d7fad65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86d4eeb597074f209e572c8e07e49991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e115547fee384dfd9ecca1f4ca0472e7",
              "IPY_MODEL_6d51c3ab525340239fd0603295b07004",
              "IPY_MODEL_dbde7c6a90f246859b025973806e8210"
            ],
            "layout": "IPY_MODEL_69212e5009db42b89200568819bf82e2"
          }
        },
        "e115547fee384dfd9ecca1f4ca0472e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c37d9b79930400e8bdeb15496c8364e",
            "placeholder": "​",
            "style": "IPY_MODEL_3e7d6faacec6454ca451434937b707d2",
            "value": "Downloading: 100%"
          }
        },
        "6d51c3ab525340239fd0603295b07004": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_722d9cd33c8c48f398180f032e8e08f8",
            "max": 46749401,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_566e36c18cdb43c58968dfa3d83cf3dd",
            "value": 46749401
          }
        },
        "dbde7c6a90f246859b025973806e8210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6516b67485746f2a9f3ffa2e1b6a50f",
            "placeholder": "​",
            "style": "IPY_MODEL_c20e188c77e84588a49d4a9ff278f4fb",
            "value": " 44.6M/44.6M [00:02&lt;00:00, 29.5MB/s]"
          }
        },
        "69212e5009db42b89200568819bf82e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c37d9b79930400e8bdeb15496c8364e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e7d6faacec6454ca451434937b707d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "722d9cd33c8c48f398180f032e8e08f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "566e36c18cdb43c58968dfa3d83cf3dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6516b67485746f2a9f3ffa2e1b6a50f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c20e188c77e84588a49d4a9ff278f4fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f870599dce248bd9952b3456dba5afc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8314eaf2244f469bab8041e52b07721c",
              "IPY_MODEL_9f19aa4aff8248f0a0f8ecc1fe65cdcb",
              "IPY_MODEL_7ef6fee3ca794bd58cdf4b4e8014f5d9"
            ],
            "layout": "IPY_MODEL_cd96b61aff0f4f37bc3d2967836c6342"
          }
        },
        "8314eaf2244f469bab8041e52b07721c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc5a018900b64e4e8fdfca87ee822d1c",
            "placeholder": "​",
            "style": "IPY_MODEL_0ea48ee3dbe6440180fd70da94dedf17",
            "value": "Downloading: 100%"
          }
        },
        "9f19aa4aff8248f0a0f8ecc1fe65cdcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b559d507067d47d7915b692175f8b457",
            "max": 428,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4e17d0031884ba79e09d5ef5f519eeb",
            "value": 428
          }
        },
        "7ef6fee3ca794bd58cdf4b4e8014f5d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a4bcb2a1d33423185613fb661b4be4f",
            "placeholder": "​",
            "style": "IPY_MODEL_113643a2257d4bbfa1211c248d372bb8",
            "value": " 428/428 [00:00&lt;00:00, 12.1kB/s]"
          }
        },
        "cd96b61aff0f4f37bc3d2967836c6342": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc5a018900b64e4e8fdfca87ee822d1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ea48ee3dbe6440180fd70da94dedf17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b559d507067d47d7915b692175f8b457": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4e17d0031884ba79e09d5ef5f519eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a4bcb2a1d33423185613fb661b4be4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "113643a2257d4bbfa1211c248d372bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daac49eb8fb746d29d856894afa6fece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4b8640ffedf499ea29f285859e6e8f4",
              "IPY_MODEL_98eeda569dfb40b4bea631001bc949fd",
              "IPY_MODEL_ebb44e800f1f4edea1fd9bed9c6620fa"
            ],
            "layout": "IPY_MODEL_03c716fce1e0463d8191df758301d7fe"
          }
        },
        "b4b8640ffedf499ea29f285859e6e8f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe083b1b077c45869c1f65188fba2d20",
            "placeholder": "​",
            "style": "IPY_MODEL_b3f6076ada24496c818a3dc215f5889a",
            "value": "Downloading: 100%"
          }
        },
        "98eeda569dfb40b4bea631001bc949fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac16338ed6ce453bba79ecc0a67997d6",
            "max": 760289,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_781c1e3419e6483dbdf6a06811dec54a",
            "value": 760289
          }
        },
        "ebb44e800f1f4edea1fd9bed9c6620fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29c5a1e4482d41f6a85e56ea26a70dcb",
            "placeholder": "​",
            "style": "IPY_MODEL_bf7dd77c3c32481f8a62b90ae11a4697",
            "value": " 742k/742k [00:00&lt;00:00, 8.99MB/s]"
          }
        },
        "03c716fce1e0463d8191df758301d7fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe083b1b077c45869c1f65188fba2d20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3f6076ada24496c818a3dc215f5889a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac16338ed6ce453bba79ecc0a67997d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "781c1e3419e6483dbdf6a06811dec54a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29c5a1e4482d41f6a85e56ea26a70dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf7dd77c3c32481f8a62b90ae11a4697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8f350da1530406d93bb7e6e9426f0a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1602ac2d0c947b18ae1ff1d955aee1d",
              "IPY_MODEL_81604830546f4997bee8e32c658ac928",
              "IPY_MODEL_691a66462fdd4e1c911720eb7b2a54da"
            ],
            "layout": "IPY_MODEL_fda6eb02874b4eccb7a0dd651c2c0f5a"
          }
        },
        "e1602ac2d0c947b18ae1ff1d955aee1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4500fed31e7147bcb4db784d8188290a",
            "placeholder": "​",
            "style": "IPY_MODEL_45a5e2b366b5453db3435922f70130ce",
            "value": "Downloading: 100%"
          }
        },
        "81604830546f4997bee8e32c658ac928": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a78b826270c8442baec8239fccaa5fbe",
            "max": 1311010,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae9355298edd4acf843369814b79fbad",
            "value": 1311010
          }
        },
        "691a66462fdd4e1c911720eb7b2a54da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_919fdb88468a46a284ba536a7f3d4d67",
            "placeholder": "​",
            "style": "IPY_MODEL_aab69b9421cf4adcac64e36cfb996e21",
            "value": " 1.25M/1.25M [00:00&lt;00:00, 1.95MB/s]"
          }
        },
        "fda6eb02874b4eccb7a0dd651c2c0f5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4500fed31e7147bcb4db784d8188290a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45a5e2b366b5453db3435922f70130ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a78b826270c8442baec8239fccaa5fbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae9355298edd4acf843369814b79fbad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "919fdb88468a46a284ba536a7f3d4d67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aab69b9421cf4adcac64e36cfb996e21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65efa603049e4d439cc7b429ba6f30d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_820501fda6e74de8804e36da4c597255",
              "IPY_MODEL_42e779162bdb40439edf5333eae14585",
              "IPY_MODEL_1f73446a0157414a989a2f262349c11e"
            ],
            "layout": "IPY_MODEL_541f72c9205849a4ba0609aee073abfa"
          }
        },
        "820501fda6e74de8804e36da4c597255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed65fc9ed62d46018e5749e746a61f28",
            "placeholder": "​",
            "style": "IPY_MODEL_7c3b24e94a9f44fb8b60a99f37c12053",
            "value": "Downloading: 100%"
          }
        },
        "42e779162bdb40439edf5333eae14585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8755ce1358844a2c912262c7e50cd719",
            "max": 245,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1bf18db36054828bf920477c2ef49df",
            "value": 245
          }
        },
        "1f73446a0157414a989a2f262349c11e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_092d182ed34047a7af019783c6951550",
            "placeholder": "​",
            "style": "IPY_MODEL_d34f57f38e844d80ae558f904e7f74f6",
            "value": " 245/245 [00:00&lt;00:00, 7.48kB/s]"
          }
        },
        "541f72c9205849a4ba0609aee073abfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed65fc9ed62d46018e5749e746a61f28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c3b24e94a9f44fb8b60a99f37c12053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8755ce1358844a2c912262c7e50cd719": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1bf18db36054828bf920477c2ef49df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "092d182ed34047a7af019783c6951550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d34f57f38e844d80ae558f904e7f74f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dc0e82f2b0b4d4d96d1cd485eb3d0f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_652c6ce9bfed40ce98fb41de1620c1a7",
              "IPY_MODEL_eb4213d3175c4a938fdeacd08fc2dfcb",
              "IPY_MODEL_68fa75e5c1d748859c24efa02a14897c"
            ],
            "layout": "IPY_MODEL_e2a2f7354bfc4641a14e8b849433cf3c"
          }
        },
        "652c6ce9bfed40ce98fb41de1620c1a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9568886c2e2e47d39c769583e3d49a59",
            "placeholder": "​",
            "style": "IPY_MODEL_9eb3ffa6645342f896b9a6551c2e2091",
            "value": "Epoch: 100%"
          }
        },
        "eb4213d3175c4a938fdeacd08fc2dfcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_947afaf6f74f44bd978f1d380e46cd77",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39ec2bd23d5b43d5b14b1129f5128646",
            "value": 1
          }
        },
        "68fa75e5c1d748859c24efa02a14897c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a5a2b66e3314244866bd08430273853",
            "placeholder": "​",
            "style": "IPY_MODEL_b18a036ffa684e30a121cd7dc1c6a9df",
            "value": " 1/1 [00:07&lt;00:00,  7.52s/it]"
          }
        },
        "e2a2f7354bfc4641a14e8b849433cf3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9568886c2e2e47d39c769583e3d49a59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eb3ffa6645342f896b9a6551c2e2091": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "947afaf6f74f44bd978f1d380e46cd77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39ec2bd23d5b43d5b14b1129f5128646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a5a2b66e3314244866bd08430273853": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b18a036ffa684e30a121cd7dc1c6a9df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29665a6b7ce144e79618b6d9533f9ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b8fac9d29c44aae8eefef6da5dda887",
              "IPY_MODEL_c04785bd95eb433786c5ff1b560517a7",
              "IPY_MODEL_ae4a66c6a4d14552b8e27029c61453f5"
            ],
            "layout": "IPY_MODEL_6113136e759e4d92a1b4d8b0cd838af8"
          }
        },
        "6b8fac9d29c44aae8eefef6da5dda887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d6067d14528498fa5ac25466987ed05",
            "placeholder": "​",
            "style": "IPY_MODEL_6a4060d26626463089e86f31880bd10d",
            "value": "Iteration: 100%"
          }
        },
        "c04785bd95eb433786c5ff1b560517a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d42d12e3255e44f6a7a04516f6673c73",
            "max": 17,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16a6c91e1caa43d68b76aba32c222643",
            "value": 17
          }
        },
        "ae4a66c6a4d14552b8e27029c61453f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a8307046d064725a45a63a56f3ceaca",
            "placeholder": "​",
            "style": "IPY_MODEL_dde033de08a04ed2b2421d089de5ee01",
            "value": " 17/17 [00:07&lt;00:00,  2.30it/s]"
          }
        },
        "6113136e759e4d92a1b4d8b0cd838af8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d6067d14528498fa5ac25466987ed05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a4060d26626463089e86f31880bd10d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d42d12e3255e44f6a7a04516f6673c73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16a6c91e1caa43d68b76aba32c222643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a8307046d064725a45a63a56f3ceaca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dde033de08a04ed2b2421d089de5ee01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a12bfa112ac84ee0b8b509fb65998393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4726fede19ae4b2eb347c04204642db5",
              "IPY_MODEL_bc23e5310ee74c1383f5f5848bfdd726",
              "IPY_MODEL_8b757aa8c33d4415955a4b76b946e4ac"
            ],
            "layout": "IPY_MODEL_675a1e3451ff4ec6bf85e78dccd35a70"
          }
        },
        "4726fede19ae4b2eb347c04204642db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_198151522fb54fd8a43e6abf9fa3e437",
            "placeholder": "​",
            "style": "IPY_MODEL_84ae291ead2845549a4fa7c9b8e3fd52",
            "value": "100%"
          }
        },
        "bc23e5310ee74c1383f5f5848bfdd726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_651e59d62c024217b8940effb613be22",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_144ea1b331cf46c68c7690b7f601b456",
            "value": 9
          }
        },
        "8b757aa8c33d4415955a4b76b946e4ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c8ceda11d964734af9309f7e05ac28f",
            "placeholder": "​",
            "style": "IPY_MODEL_4664f8aea9db474d918d4c86887736c6",
            "value": " 9/9 [00:03&lt;00:00,  3.69it/s]"
          }
        },
        "675a1e3451ff4ec6bf85e78dccd35a70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "198151522fb54fd8a43e6abf9fa3e437": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84ae291ead2845549a4fa7c9b8e3fd52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "651e59d62c024217b8940effb613be22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "144ea1b331cf46c68c7690b7f601b456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c8ceda11d964734af9309f7e05ac28f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4664f8aea9db474d918d4c86887736c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install python library"
      ],
      "metadata": {
        "id": "Bxd04UusPJX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install bitsandbytes-cudacuda113\n",
        "!pip install -U sentence-transformers\n",
        "!pip install rich\n",
        "!pip install datasets\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install pinecone-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljOIORmNV1rv",
        "outputId": "3a378045-3c1a-4800-edf2-2ccf4c0f9043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement bitsandbytes-cudacuda113 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for bitsandbytes-cudacuda113\u001b[0m\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 22.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 15.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 49.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=55e42fb91d2ff4ee73eb1d2f39e71358a9bac4cea36d09abfe9a62299ebdf3e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.18.0\n",
            "Collecting rich\n",
            "  Downloading rich-12.2.0-py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 9.4 MB/s \n",
            "\u001b[?25hCollecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from rich) (4.1.1)\n",
            "Installing collected packages: commonmark, rich\n",
            "Successfully installed commonmark-0.9.1 rich-12.2.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 13.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.1)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 60.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 71.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 33.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 67.5 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 71.0 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 56.5 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.1.0 frozenlist-1.3.0 fsspec-2022.3.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-2.0.9-py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 14.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from pinecone-client) (1.25.11)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pinecone-client) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from pinecone-client) (2.8.2)\n",
            "Collecting dnspython>=2.0.0\n",
            "  Downloading dnspython-2.2.1-py3-none-any.whl (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 63.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from pinecone-client) (4.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.7/dist-packages (from pinecone-client) (6.0)\n",
            "Collecting loguru>=0.5.0\n",
            "  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pinecone-client) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pinecone-client) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pinecone-client) (2.10)\n",
            "Installing collected packages: loguru, dnspython, pinecone-client\n",
            "Successfully installed dnspython-2.2.1 loguru-0.6.0 pinecone-client-2.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "juFz0FXNPgEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pinecone\n",
        "from rich import print\n",
        "from rich.pretty import pprint\n"
      ],
      "metadata": {
        "id": "RcpryDEKWREi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google drive imports"
      ],
      "metadata": {
        "id": "Zs6rhCgVlQC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n"
      ],
      "metadata": {
        "id": "EHV4MXEz83SF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2710227d-cf11-4285-bf19-f213837591b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def get_dataset():\n",
        "  #/content/drive/MyDrive/AIM/discussion_dataset\n",
        "  file_path = \"/content/drive/MyDrive/discussion_dataset\"\n",
        "  ## determine the size of the dataset\n",
        "  files  = os.listdir(os.path.join(os.getcwd(),file_path))\n",
        "  print(files)\n",
        "  dataset_for_model = None\n",
        "\n",
        "  for discussion in files:\n",
        "    jsonString = \"json\"\n",
        "    if jsonString in discussion:\n",
        "      new_file_path = \"/content/drive/MyDrive/discussion_dataset\"\n",
        "      f = open(os.path.join(new_file_path , discussion))\n",
        "      data = json.load(f)\n",
        "      try:\n",
        "        if data[\"answer\"] is not None:\n",
        "          if dataset_for_model is not None:\n",
        "            dataset_object = [data[\"title\"] , data[\"bodyText\"] , data[\"answer\"][\"bodyText\"] , data[\"id\"] , data['answer'][\"url\"]]\n",
        "            dataset_object = np.asarray(dataset_object , dtype=object).reshape(1,5)\n",
        "            dataset_for_model = pd.concat([pd.DataFrame(dataset_object , columns=[\"question\" , \"context\" , \"answer\" , \"id\" , \"url\"] , dtype=object) , dataset_for_model] , ignore_index=True , axis = 0)\n",
        "          else:\n",
        "            dataset_object = [data[\"title\"] , data[\"bodyText\"] , data[\"answer\"][\"bodyText\"] , data[\"id\"] , data['answer'][\"url\"]]\n",
        "            dataset_object = np.asarray(dataset_object , dtype=object).reshape(1,5)\n",
        "            dataset_for_model = pd.DataFrame(dataset_object , columns=[\"question\" , \"context\" , \"answer\" , \"id\" , \"url\"] , dtype=object)\n",
        "      except:\n",
        "        continue\n",
        "  return dataset_for_model\n",
        "dataset_for_model  = get_dataset()\n",
        "  "
      ],
      "metadata": {
        "id": "Y7fzNyh6242p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9cd53ede-dbff-4e03-9694-6e84e26d4daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[32m'dicussion_data4.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6639.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11474.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8238.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9120.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11628.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7670.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data2701.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11977.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5755.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6932.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10679.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11969.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9601.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7482.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11358.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11135.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5786.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12400.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11109.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6334.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8534.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8711.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8461.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6189.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10241.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12090.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10747.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8855.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10956.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8917.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6680.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7097.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11057.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11354.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10967.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7423.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8042.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10692.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8710.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9325.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11032.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8734.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11437.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11914.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6236.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8356.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5793.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9321.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12394.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10221.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12219.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10534.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10210.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9466.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9527.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8153.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6979.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7604.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data1486.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11724.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5947.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8970.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6219.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6761.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6664.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6234.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12289.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7264.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7586.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data2109.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6631.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6949.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9689.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8280.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10517.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6678.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10070.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7092.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9465.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9016.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12294.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10261.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12097.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10716.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11963.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12313.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8797.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11742.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data1325.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11710.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7584.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data1456.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8923.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7116.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12250.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10593.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12654.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8919.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10652.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7495.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8211.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6832.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9509.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11787.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8757.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6453.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6708.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9351.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7620.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7804.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11613.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7870.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7327.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9134.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6550.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data678.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8882.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10669.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7153.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8812.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6883.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12562.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12272.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11278.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11152.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11320.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8027.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9057.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7660.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11544.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12056.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7275.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10136.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8083.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10563.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8565.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8910.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7536.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6799.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7375.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7903.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data4878.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11087.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12342.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7425.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data3716.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11905.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7137.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9873.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6501.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7859.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12166.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8955.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7934.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6996.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11051.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7956.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10817.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10758.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9723.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8736.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9776.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6452.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6774.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5819.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5811.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11839.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7953.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6486.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10049.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6022.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6635.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7940.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data680.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11258.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8492.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5350.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12359.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9332.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10259.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11107.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10399.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5886.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8449.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7485.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9750.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10945.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data3706.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data1334.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12242.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12154.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10063.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12306.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6919.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9593.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9159.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8746.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10126.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9807.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7763.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11928.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10314.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11722.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9660.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7419.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11829.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9391.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12283.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8543.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12580.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11686.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11435.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11285.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10583.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10025.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9830.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7828.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11384.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6830.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10046.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5407.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11447.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9651.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6856.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7473.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9646.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10489.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9070.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11079.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10380.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12023.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9703.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11019.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11276.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11873.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8806.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7463.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9251.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10383.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9137.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8254.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12481.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8213.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11766.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12244.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data3471.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6844.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6253.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11290.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8339.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9092.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11172.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6128.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data1958.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7732.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9728.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8112.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data2290.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7611.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10467.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8415.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12602.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8616.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10341.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6593.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6454.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8487.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10582.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6397.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11670.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8114.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7182.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8914.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9218.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5791.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11954.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7782.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7722.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8548.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8878.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7897.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5810.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8623.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5602.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11659.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11304.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11743.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8285.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9469.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6723.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6081.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10509.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6483.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11838.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11681.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7382.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9201.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data4702.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6369.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11849.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6354.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12548.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12511.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data4387.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6904.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11498.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8182.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5821.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6685.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9004.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10721.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6441.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8046.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12579.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11853.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8517.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8409.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9487.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8539.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8207.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11378.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11419.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9698.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9452.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8699.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7124.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10819.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10453.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5800.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11859.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11535.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9305.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10942.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10005.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10808.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8726.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9570.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9986.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6144.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12256.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5790.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12683.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9478.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10712.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6806.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12085.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7316.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11503.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9627.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7175.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8363.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data454.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8283.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8320.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7465.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7374.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12560.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5780.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11783.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8814.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7632.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5964.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7573.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11705.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9687.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12416.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8705.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5883.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5795.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10364.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12636.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5787.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10218.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5808.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10255.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8829.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6310.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11647.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10492.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12261.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data1170.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7801.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9181.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11631.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6740.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8342.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6953.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12322.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8866.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8592.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11007.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10257.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7444.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9245.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7314.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8451.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6653.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8921.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6329.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6698.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5816.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10663.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6367.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data1816.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8759.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5263.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5779.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6861.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6288.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11960.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9817.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11846.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9062.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9132.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6392.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12415.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8038.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11997.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10853.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8650.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8749.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10600.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11412.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7919.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6100.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11260.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11293.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10670.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6998.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8549.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11455.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7177.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12660.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8682.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11052.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6663.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8683.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7298.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11432.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data2509.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6938.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6192.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12547.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9773.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data3639.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7391.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10253.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6725.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10858.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6489.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6688.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9731.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12015.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7943.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11980.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7158.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10706.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10880.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data1974.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6456.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12397.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12574.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10338.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7845.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7702.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5812.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12187.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5729.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10363.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data2720.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data773.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9555.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7528.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5884.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12601.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6029.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7307.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8572.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5796.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12639.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8640.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12221.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8796.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5798.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data3436.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9998.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7281.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8607.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8665.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10321.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12531.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12569.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11197.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6218.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8380.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12488.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10159.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10812.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10753.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11851.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7081.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12513.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6751.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12551.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6371.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10741.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8848.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7511.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7312.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7514.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9740.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12510.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6412.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9745.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11702.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10733.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7800.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10392.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7886.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9790.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10651.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8990.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9282.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12310.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8630.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6711.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11272.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11800.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9874.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9861.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9006.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7703.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8298.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9036.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11896.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11055.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7009.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12339.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11718.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12080.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5814.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6986.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12523.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6684.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7440.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7820.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11879.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11086.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9449.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8724.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12347.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7412.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12146.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6596.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data3351.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8113.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7226.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9725.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8898.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6131.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10795.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9864.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data264.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12380.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6757.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11765.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11774.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8417.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7209.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12424.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7616.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8099.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9504.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5807.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9456.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10691.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6337.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5797.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6629.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6890.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11016.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9885.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data3455.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7743.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7393.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10356.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12165.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5803.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10140.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9853.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5977.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7549.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10963.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10852.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5794.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8516.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9108.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9197.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7720.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8760.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9530.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6509.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12041.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8772.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11028.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12457.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12073.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8690.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9795.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data3100.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5788.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10771.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12202.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12007.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6888.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7372.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6499.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12364.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6859.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11580.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12168.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8000.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8374.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9966.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7635.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11652.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9534.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10869.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7837.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9195.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10173.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9363.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12329.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5890.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9617.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11250.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9910.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6568.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8735.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10764.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12276.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8637.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5891.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6703.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12297.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7186.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9793.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9342.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12349.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10998.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7184.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6860.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6401.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data2149.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10212.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12017.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7693.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11779.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10731.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11656.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data268.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5809.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7450.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data0.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8809.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10672.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7950.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11273.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11297.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8703.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10910.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10396.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8524.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9076.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11160.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9048.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10206.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10925.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8561.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6780.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5820.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6433.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6644.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7624.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5792.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data4139.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12578.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9444.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11704.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11802.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12325.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8659.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8541.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7582.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10677.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8023.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6494.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10938.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7227.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12458.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10165.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6407.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8161.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7884.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6463.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6933.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9589.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9007.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5885.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5822.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9996.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6863.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7785.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5806.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6176.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11582.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data3301.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6223.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10772.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7356.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9955.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6827.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5813.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7739.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6340.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8669.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12239.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6492.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9486.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9736.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9629.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7155.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11215.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6602.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9459.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12035.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11563.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7352.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12178.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11865.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11477.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5818.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12399.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8647.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8996.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7094.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10523.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8264.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8267.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6226.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7373.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7310.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7068.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9804.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9268.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9961.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12598.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6497.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10284.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6468.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10178.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11874.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11667.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9185.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8460.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10561.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8611.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5571.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7972.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6133.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6559.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11941.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7641.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11770.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8349.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6174.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11243.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6787.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10789.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10007.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9836.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7305.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6713.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12621.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7768.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8018.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8937.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9888.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6350.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8221.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12480.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11392.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11707.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9551.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6583.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9511.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11024.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12199.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11926.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7059.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11324.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11692.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8795.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5802.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11508.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12207.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7730.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data1.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10152.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11916.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12246.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10223.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9685.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8445.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10477.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5784.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6759.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12606.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12516.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8429.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7005.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6785.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8581.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6280.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10015.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7200.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11413.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9619.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12615.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5805.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11063.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9845.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5942.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6251.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9925.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11281.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7516.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8413.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10433.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5815.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8841.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10723.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6779.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6912.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10664.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12048.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7745.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12465.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11279.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10166.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11104.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12076.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7855.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6769.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11938.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10973.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11049.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8980.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10801.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7669.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12189.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data2065.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9539.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8994.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11129.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12566.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8300.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8810.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data1884.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11763.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9249.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7249.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6459.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12544.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12132.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8778.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9732.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7766.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6796.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10400.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12066.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8240.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8915.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12092.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8511.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9259.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12390.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data4680.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7777.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11806.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6852.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10897.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6718.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data3915.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9882.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8695.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8730.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6518.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12121.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12484.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11277.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12680.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6748.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12311.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7461.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7133.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6709.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data1005.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10833.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5888.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8410.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9881.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6445.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7520.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10361.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8893.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9379.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10588.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6594.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8390.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6962.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8091.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6560.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7446.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8973.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5979.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12371.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11088.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6526.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8545.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12693.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9707.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8321.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6474.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6612.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8289.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8496.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9340.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11664.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7172.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12487.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7729.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7012.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11493.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12253.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8906.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data2346.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6290.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6940.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11318.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5308.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8274.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10642.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8368.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6502.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10263.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12687.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7812.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12027.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7951.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10586.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6440.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9323.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6284.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8143.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data3.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12634.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10862.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7754.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7921.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8142.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10382.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6693.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10086.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8554.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12220.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9633.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11331.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12620.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9079.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12395.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8306.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7557.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8977.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5801.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6991.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11932.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9831.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8791.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7225.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8354.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9226.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8853.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5246.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9060.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10883.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8648.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data2529.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5887.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6358.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7525.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11192.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data4518.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6132.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8686.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5655.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7727.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8177.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7332.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7435.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9307.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8054.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6125.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9174.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7922.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7645.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8418.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6957.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7569.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8798.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9149.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data3225.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8450.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6085.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12004.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6818.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11835.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10172.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9638.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6300.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11847.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10479.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7082.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8590.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7123.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7570.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9635.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9273.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6425.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7914.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11456.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12043.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11492.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7977.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7893.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11457.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6352.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data4828.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10893.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5684.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12278.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6945.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6356.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11460.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5889.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8452.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12273.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6697.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7658.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10599.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11356.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7512.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data226.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9751.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7476.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9774.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9610.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9334.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7852.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8696.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7135.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5785.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7541.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data151.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6738.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12320.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8438.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10464.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9179.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6182.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11059.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7169.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6851.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8879.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7649.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8505.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11936.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6473.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11598.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9543.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5833.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data217.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6726.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8793.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6240.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12442.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6199.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9912.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12522.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12581.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11265.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6521.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8619.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7647.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5823.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8007.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10840.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8081.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9797.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6135.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12575.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9278.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7349.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6341.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11251.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11306.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7725.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11989.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8871.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10386.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7818.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10133.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7162.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12137.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11855.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9027.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6628.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5978.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6715.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7365.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8978.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12635.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6126.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8860.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6482.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5728.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7588.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8504.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11491.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6479.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6450.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7824.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11500.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9927.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9157.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12264.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data4133.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6531.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data112.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10922.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10714.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12060.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7285.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6841.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10327.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6244.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8361.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10566.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9144.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7796.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11626.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5817.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10813.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8050.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6533.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12401.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11154.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6613.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9068.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8903.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10898.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data2.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10547.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8407.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10727.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7459.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10997.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7849.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8019.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10160.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10096.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5781.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7602.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10405.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11271.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10037.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11584.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8376.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11348.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6249.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7991.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data3429.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7438.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12269.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9727.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6175.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9708.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5799.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8966.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11005.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data6821.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9914.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7803.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11898.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12002.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8138.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12667.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10792.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9275.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11641.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8768.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10824.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9354.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12286.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10959.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8063.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data9236.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8481.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data5782.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data12518.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data10684.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7719.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data11727.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data7309.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8103.json'\u001b[0m,\n",
              "    \u001b[32m'dicussion_data8897.json'\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data4.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6639.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11474.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8238.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9120.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11628.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7670.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data2701.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11977.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5755.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6932.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10679.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11969.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9601.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7482.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11358.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11135.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5786.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12400.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11109.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6334.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8534.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8711.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8461.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6189.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10241.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12090.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10747.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8855.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10956.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8917.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6680.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7097.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11057.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11354.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10967.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7423.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8042.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10692.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8710.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9325.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11032.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8734.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11437.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11914.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6236.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8356.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5793.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9321.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12394.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10221.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12219.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10534.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10210.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9466.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9527.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8153.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6979.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7604.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data1486.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11724.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5947.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8970.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6219.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6761.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6664.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6234.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12289.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7264.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7586.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data2109.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6631.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6949.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9689.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8280.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10517.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6678.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10070.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7092.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9465.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9016.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12294.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10261.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12097.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10716.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11963.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12313.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8797.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11742.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data1325.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11710.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7584.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data1456.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8923.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7116.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12250.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10593.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12654.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8919.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10652.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7495.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8211.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6832.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9509.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11787.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8757.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6453.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6708.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9351.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7620.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7804.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11613.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7870.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7327.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9134.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6550.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data678.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8882.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10669.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7153.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8812.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6883.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12562.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12272.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11278.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11152.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11320.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8027.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9057.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7660.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11544.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12056.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7275.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10136.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8083.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10563.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8565.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8910.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7536.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6799.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7375.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7903.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data4878.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11087.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12342.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7425.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data3716.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11905.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7137.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9873.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6501.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7859.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12166.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8955.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7934.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6996.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11051.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7956.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10817.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10758.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9723.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8736.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9776.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6452.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6774.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5819.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5811.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11839.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7953.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6486.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10049.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6022.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6635.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7940.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data680.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11258.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8492.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5350.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12359.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9332.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10259.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11107.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10399.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5886.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8449.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7485.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9750.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10945.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data3706.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data1334.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12242.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12154.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10063.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12306.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6919.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9593.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9159.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8746.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10126.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9807.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7763.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11928.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10314.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11722.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9660.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7419.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11829.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9391.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12283.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8543.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12580.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11686.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11435.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11285.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10583.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10025.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9830.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7828.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11384.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6830.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10046.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5407.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11447.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9651.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6856.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7473.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9646.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10489.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9070.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11079.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10380.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12023.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9703.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11019.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11276.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11873.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8806.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7463.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9251.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10383.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9137.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8254.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12481.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8213.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11766.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12244.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data3471.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6844.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6253.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11290.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8339.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9092.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11172.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6128.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data1958.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7732.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9728.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8112.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data2290.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7611.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10467.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8415.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12602.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8616.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10341.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6593.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6454.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8487.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10582.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6397.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11670.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8114.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7182.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8914.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9218.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5791.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11954.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7782.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7722.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8548.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8878.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7897.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5810.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8623.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5602.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11659.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11304.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11743.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8285.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9469.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6723.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6081.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10509.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6483.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11838.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11681.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7382.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9201.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data4702.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6369.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11849.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6354.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12548.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12511.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data4387.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6904.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11498.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8182.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5821.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6685.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9004.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10721.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6441.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8046.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12579.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11853.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8517.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8409.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9487.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8539.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8207.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11378.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11419.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9698.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9452.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8699.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7124.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10819.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10453.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5800.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11859.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11535.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9305.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10942.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10005.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10808.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8726.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9570.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9986.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6144.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12256.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5790.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12683.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9478.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10712.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6806.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12085.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7316.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11503.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9627.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7175.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8363.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data454.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8283.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8320.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7465.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7374.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12560.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5780.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11783.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8814.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7632.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5964.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7573.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11705.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9687.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12416.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8705.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5883.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5795.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10364.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12636.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5787.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10218.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5808.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10255.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8829.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6310.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11647.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10492.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12261.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data1170.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7801.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9181.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11631.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6740.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8342.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6953.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12322.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8866.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8592.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11007.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10257.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7444.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9245.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7314.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8451.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6653.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8921.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6329.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6698.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5816.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10663.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6367.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data1816.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8759.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5263.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5779.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6861.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6288.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11960.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9817.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11846.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9062.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9132.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6392.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12415.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8038.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11997.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10853.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8650.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8749.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10600.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11412.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7919.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6100.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11260.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11293.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10670.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6998.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8549.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11455.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7177.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12660.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8682.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11052.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6663.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8683.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7298.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11432.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data2509.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6938.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6192.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12547.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9773.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data3639.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7391.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10253.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6725.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10858.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6489.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6688.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9731.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12015.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7943.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11980.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7158.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10706.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10880.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data1974.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6456.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12397.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12574.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10338.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7845.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7702.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5812.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12187.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5729.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10363.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data2720.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data773.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9555.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7528.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5884.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12601.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6029.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7307.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8572.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5796.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12639.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8640.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12221.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8796.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5798.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data3436.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9998.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7281.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8607.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8665.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10321.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12531.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12569.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11197.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6218.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8380.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12488.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10159.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10812.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10753.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11851.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7081.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12513.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6751.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12551.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6371.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10741.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8848.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7511.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7312.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7514.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9740.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12510.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6412.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9745.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11702.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10733.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7800.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10392.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7886.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9790.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10651.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8990.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9282.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12310.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8630.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6711.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11272.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11800.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9874.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9861.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9006.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7703.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8298.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9036.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11896.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11055.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7009.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12339.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11718.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12080.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5814.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6986.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12523.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6684.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7440.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7820.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11879.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11086.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9449.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8724.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12347.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7412.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12146.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6596.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data3351.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8113.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7226.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9725.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8898.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6131.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10795.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9864.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data264.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12380.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6757.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11765.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11774.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8417.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7209.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12424.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7616.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8099.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9504.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5807.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9456.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10691.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6337.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5797.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6629.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6890.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11016.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9885.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data3455.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7743.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7393.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10356.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12165.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5803.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10140.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9853.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5977.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7549.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10963.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10852.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5794.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8516.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9108.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9197.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7720.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8760.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9530.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6509.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12041.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8772.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11028.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12457.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12073.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8690.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9795.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data3100.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5788.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10771.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12202.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12007.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6888.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7372.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6499.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12364.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6859.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11580.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12168.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8000.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8374.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9966.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7635.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11652.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9534.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10869.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7837.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9195.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10173.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9363.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12329.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5890.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9617.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11250.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9910.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6568.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8735.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10764.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12276.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8637.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5891.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6703.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12297.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7186.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9793.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9342.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12349.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10998.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7184.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6860.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6401.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data2149.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10212.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12017.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7693.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11779.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10731.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11656.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data268.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5809.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7450.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data0.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8809.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10672.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7950.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11273.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11297.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8703.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10910.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10396.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8524.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9076.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11160.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9048.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10206.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10925.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8561.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6780.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5820.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6433.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6644.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7624.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5792.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data4139.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12578.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9444.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11704.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11802.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12325.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8659.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8541.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7582.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10677.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8023.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6494.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10938.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7227.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12458.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10165.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6407.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8161.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7884.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6463.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6933.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9589.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9007.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5885.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5822.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9996.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6863.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7785.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5806.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6176.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11582.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data3301.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6223.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10772.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7356.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9955.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6827.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5813.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7739.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6340.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8669.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12239.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6492.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9486.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9736.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9629.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7155.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11215.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6602.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9459.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12035.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11563.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7352.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12178.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11865.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11477.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5818.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12399.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8647.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8996.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7094.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10523.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8264.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8267.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6226.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7373.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7310.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7068.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9804.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9268.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9961.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12598.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6497.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10284.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6468.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10178.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11874.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11667.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9185.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8460.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10561.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8611.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5571.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7972.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6133.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6559.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11941.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7641.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11770.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8349.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6174.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11243.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6787.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10789.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10007.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9836.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7305.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6713.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12621.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7768.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8018.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8937.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9888.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6350.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8221.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12480.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11392.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11707.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9551.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6583.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9511.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11024.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12199.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11926.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7059.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11324.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11692.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8795.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5802.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11508.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12207.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7730.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data1.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10152.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11916.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12246.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10223.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9685.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8445.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10477.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5784.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6759.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12606.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12516.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8429.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7005.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6785.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8581.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6280.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10015.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7200.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11413.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9619.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12615.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5805.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11063.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9845.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5942.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6251.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9925.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11281.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7516.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8413.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10433.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5815.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8841.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10723.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6779.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6912.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10664.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12048.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7745.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12465.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11279.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10166.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11104.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12076.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7855.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6769.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11938.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10973.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11049.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8980.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10801.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7669.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12189.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data2065.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9539.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8994.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11129.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12566.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8300.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8810.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data1884.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11763.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9249.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7249.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6459.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12544.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12132.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8778.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9732.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7766.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6796.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10400.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12066.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8240.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8915.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12092.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8511.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9259.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12390.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data4680.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7777.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11806.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6852.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10897.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6718.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data3915.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9882.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8695.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8730.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6518.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12121.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12484.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11277.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12680.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6748.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12311.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7461.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7133.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6709.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data1005.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10833.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5888.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8410.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9881.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6445.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7520.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10361.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8893.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9379.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10588.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6594.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8390.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6962.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8091.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6560.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7446.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8973.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5979.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12371.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11088.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6526.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8545.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12693.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9707.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8321.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6474.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6612.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8289.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8496.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9340.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11664.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7172.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12487.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7729.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7012.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11493.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12253.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8906.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data2346.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6290.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6940.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11318.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5308.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8274.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10642.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8368.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6502.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10263.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12687.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7812.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12027.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7951.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10586.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6440.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9323.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6284.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8143.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data3.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12634.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10862.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7754.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7921.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8142.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10382.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6693.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10086.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8554.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12220.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9633.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11331.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12620.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9079.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12395.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8306.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7557.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8977.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5801.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6991.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11932.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9831.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8791.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7225.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8354.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9226.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8853.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5246.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9060.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10883.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8648.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data2529.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5887.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6358.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7525.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11192.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data4518.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6132.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8686.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5655.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7727.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8177.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7332.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7435.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9307.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8054.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6125.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9174.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7922.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7645.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8418.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6957.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7569.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8798.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9149.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data3225.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8450.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6085.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12004.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6818.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11835.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10172.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9638.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6300.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11847.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10479.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7082.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8590.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7123.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7570.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9635.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9273.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6425.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7914.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11456.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12043.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11492.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7977.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7893.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11457.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6352.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data4828.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10893.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5684.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12278.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6945.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6356.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11460.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5889.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8452.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12273.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6697.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7658.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10599.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11356.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7512.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data226.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9751.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7476.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9774.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9610.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9334.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7852.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8696.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7135.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5785.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7541.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data151.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6738.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12320.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8438.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10464.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9179.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6182.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11059.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7169.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6851.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8879.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7649.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8505.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11936.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6473.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11598.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9543.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5833.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data217.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6726.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8793.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6240.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12442.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6199.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9912.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12522.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12581.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11265.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6521.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8619.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7647.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5823.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8007.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10840.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8081.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9797.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6135.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12575.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9278.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7349.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6341.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11251.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11306.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7725.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11989.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8871.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10386.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7818.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10133.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7162.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12137.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11855.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9027.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6628.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5978.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6715.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7365.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8978.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12635.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6126.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8860.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6482.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5728.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7588.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8504.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11491.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6479.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6450.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7824.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11500.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9927.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9157.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12264.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data4133.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6531.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data112.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10922.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10714.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12060.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7285.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6841.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10327.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6244.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8361.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10566.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9144.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7796.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11626.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5817.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10813.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8050.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6533.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12401.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11154.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6613.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9068.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8903.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10898.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data2.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10547.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8407.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10727.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7459.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10997.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7849.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8019.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10160.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10096.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5781.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7602.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10405.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11271.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10037.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11584.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8376.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11348.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6249.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7991.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data3429.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7438.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12269.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9727.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6175.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9708.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5799.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8966.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11005.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data6821.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9914.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7803.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11898.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12002.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8138.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12667.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10792.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9275.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11641.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8768.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10824.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9354.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12286.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10959.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8063.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data9236.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8481.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data5782.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data12518.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data10684.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7719.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data11727.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data7309.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8103.json'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'dicussion_data8897.json'</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(dataset_for_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "jI_yOmfR0Dx9",
        "outputId": "28e0ae67-e2db-47ac-fbc0-8c8152280637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'pandas.core.frame.DataFrame'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'pandas.core.frame.DataFrame'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_for_model.head())\n",
        "print(dataset_for_model.size)\n",
        "print(dataset_for_model.shape)\n",
        "train, eval = train_test_split( dataset_for_model ,  test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "Hr8SGo7enUPS",
        "outputId": "f60c864f-33c7-4dc0-f165-c1ff82fd6362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                            question  \\\n",
              "\u001b[1;36m0\u001b[0m  Only Tensors created explicitly by the user \u001b[1m(\u001b[0mg\u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m1\u001b[0m                         How to test in low version   \n",
              "\u001b[1;36m2\u001b[0m  Loading from checkpoints re-downloads pre-trai\u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m3\u001b[0m                         Selecting one gpu from cli   \n",
              "\u001b[1;36m4\u001b[0m          CUDA OOM during validation of first epoch   \n",
              "\n",
              "                                             context  \\\n",
              "\u001b[1;36m0\u001b[0m  how can i solve this problem, my net is DB,  d\u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m1\u001b[0m  There is no train.test \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m in the lower version\u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m2\u001b[0m  I am defining a simple multi-class BERT classi\u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m3\u001b[0m  Hi, I have \u001b[1;36m4\u001b[0m gpus on my machine. I want to sel\u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m4\u001b[0m  hi all,\\nMy model validation code \u001b[1m(\u001b[0msee below\u001b[1m)\u001b[0m \u001b[33m...\u001b[0m   \n",
              "\n",
              "                                              answer  \\\n",
              "\u001b[1;36m0\u001b[0m  What Lightning version are you using?\\nThe fas\u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m1\u001b[0m  Hi\\nPyTorch Lightning \u001b[1;36m0.4\u001b[0m.\u001b[1;36m6\u001b[0m is extremely old. \u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m2\u001b[0m  It's because lightning instantiates the Lightn\u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m3\u001b[0m  python 4_pretrain_encoder.py --gpus \u001b[1;36m2\u001b[0m, --max_e\u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m4\u001b[0m  Dear @mishooax,\\nYou are returning the batch f\u001b[33m...\u001b[0m   \n",
              "\n",
              "                             id  \\\n",
              "\u001b[1;36m0\u001b[0m  MDEwOkRpc2N1c3Npb24zNTE3NTEx   \n",
              "\u001b[1;36m1\u001b[0m  MDEwOkRpc2N1c3Npb24zNDI2NzQy   \n",
              "\u001b[1;36m2\u001b[0m  MDEwOkRpc2N1c3Npb24zNTQ4NzY1   \n",
              "\u001b[1;36m3\u001b[0m  MDEwOkRpc2N1c3Npb24zNDIyMTky   \n",
              "\u001b[1;36m4\u001b[0m            D_kwDOCqWgoM4AONtA   \n",
              "\n",
              "                                                 url  \n",
              "\u001b[1;36m0\u001b[0m  \u001b[4;94mhttps://github.com/PyTorchLightning/pytorch-li...\u001b[0m  \n",
              "\u001b[1;36m1\u001b[0m  \u001b[4;94mhttps://github.com/PyTorchLightning/pytorch-li...\u001b[0m  \n",
              "\u001b[1;36m2\u001b[0m  \u001b[4;94mhttps://github.com/PyTorchLightning/pytorch-li...\u001b[0m  \n",
              "\u001b[1;36m3\u001b[0m  \u001b[4;94mhttps://github.com/PyTorchLightning/pytorch-li...\u001b[0m  \n",
              "\u001b[1;36m4\u001b[0m  \u001b[4;94mhttps://github.com/PyTorchLightning/pytorch-li...\u001b[0m  \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                            question  \\\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  Only Tensors created explicitly by the user <span style=\"font-weight: bold\">(</span>g<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                         How to test in low version   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  Loading from checkpoints re-downloads pre-trai<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>                         Selecting one gpu from cli   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>          CUDA OOM during validation of first epoch   \n",
              "\n",
              "                                             context  \\\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  how can i solve this problem, my net is DB,  d<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  There is no train.test <span style=\"font-weight: bold\">()</span> in the lower version<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  I am defining a simple multi-class BERT classi<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  Hi, I have <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> gpus on my machine. I want to sel<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>  hi all,\\nMy model validation code <span style=\"font-weight: bold\">(</span>see below<span style=\"font-weight: bold\">)</span> <span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "\n",
              "                                              answer  \\\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  What Lightning version are you using?\\nThe fas<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  Hi\\nPyTorch Lightning <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> is extremely old. <span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  It's because lightning instantiates the Lightn<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  python 4_pretrain_encoder.py --gpus <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, --max_e<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>  Dear @mishooax,\\nYou are returning the batch f<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "\n",
              "                             id  \\\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  MDEwOkRpc2N1c3Npb24zNTE3NTEx   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  MDEwOkRpc2N1c3Npb24zNDI2NzQy   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  MDEwOkRpc2N1c3Npb24zNTQ4NzY1   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  MDEwOkRpc2N1c3Npb24zNDIyMTky   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>            D_kwDOCqWgoM4AONtA   \n",
              "\n",
              "                                                 url  \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/PyTorchLightning/pytorch-li...</span>  \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/PyTorchLightning/pytorch-li...</span>  \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/PyTorchLightning/pytorch-li...</span>  \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/PyTorchLightning/pytorch-li...</span>  \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>  <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/PyTorchLightning/pytorch-li...</span>  \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m2140\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2140</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m(\u001b[0m\u001b[1;36m428\u001b[0m, \u001b[1;36m5\u001b[0m\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">428</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import InputExample\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "#InputExample is the data format we use when we're training with sentence transformer library\n",
        "#tqdm is the progress bar in the output\n",
        "train = []\n",
        "for index , row in dataset_for_model.iterrows():\n",
        "  print(type(row[\"context\"]))\n",
        "  train.append(InputExample(\n",
        "      texts= [row[\"question\"] , row[\"context\"]] #no label here, because we are training with multiple ranking loss\n",
        "  ))"
      ],
      "metadata": {
        "id": "JOSjFCdPWjGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Because we are using MNR loss, we should ensure each batch doesn't include duplicates\n",
        "from sentence_transformers import datasets\n",
        "batch_size = 24\n",
        "\n",
        "loader = datasets.NoDuplicatesDataLoader(\n",
        "    train, batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "SVOONhPDXceF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import models, SentenceTransformer\n",
        "\n",
        "#mpnet model is more accurate than the bert model \n",
        "#pooling layer is important!\n",
        "bert = models.Transformer('nreimers/albert-small-v2')\n",
        "pooler = models.Pooling(\n",
        "    bert.get_word_embedding_dimension(),\n",
        "    pooling_mode_mean_tokens=True\n",
        ")\n",
        "\n",
        "model = SentenceTransformer(modules=[bert, pooler])\n",
        "model"
      ],
      "metadata": {
        "id": "e20L6GQbZXoq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385,
          "referenced_widgets": [
            "90c7ccc29f8149a5821eb364d69207ce",
            "a904036b5f7b4eef8727c4125e74278f",
            "5fc3cdf504b748d999512f7470dad76a",
            "4a1dd08e86d64c25a45bb4dd51fc4f2d",
            "fcc54e5bcdc243f893f79027f039dac5",
            "df58dd9af0d44635b567348fdbd00cfb",
            "23cb5c248d444ec0a4d45935c480de0b",
            "47455852643840a781191e44bd4b732c",
            "6c0802b556194c829741c11a4c31748b",
            "2497771b0cae41ed8f4fe9e65b4674b9",
            "7caac61e953e45659e48b5cf4d7fad65",
            "86d4eeb597074f209e572c8e07e49991",
            "e115547fee384dfd9ecca1f4ca0472e7",
            "6d51c3ab525340239fd0603295b07004",
            "dbde7c6a90f246859b025973806e8210",
            "69212e5009db42b89200568819bf82e2",
            "7c37d9b79930400e8bdeb15496c8364e",
            "3e7d6faacec6454ca451434937b707d2",
            "722d9cd33c8c48f398180f032e8e08f8",
            "566e36c18cdb43c58968dfa3d83cf3dd",
            "b6516b67485746f2a9f3ffa2e1b6a50f",
            "c20e188c77e84588a49d4a9ff278f4fb",
            "8f870599dce248bd9952b3456dba5afc",
            "8314eaf2244f469bab8041e52b07721c",
            "9f19aa4aff8248f0a0f8ecc1fe65cdcb",
            "7ef6fee3ca794bd58cdf4b4e8014f5d9",
            "cd96b61aff0f4f37bc3d2967836c6342",
            "cc5a018900b64e4e8fdfca87ee822d1c",
            "0ea48ee3dbe6440180fd70da94dedf17",
            "b559d507067d47d7915b692175f8b457",
            "f4e17d0031884ba79e09d5ef5f519eeb",
            "1a4bcb2a1d33423185613fb661b4be4f",
            "113643a2257d4bbfa1211c248d372bb8",
            "daac49eb8fb746d29d856894afa6fece",
            "b4b8640ffedf499ea29f285859e6e8f4",
            "98eeda569dfb40b4bea631001bc949fd",
            "ebb44e800f1f4edea1fd9bed9c6620fa",
            "03c716fce1e0463d8191df758301d7fe",
            "fe083b1b077c45869c1f65188fba2d20",
            "b3f6076ada24496c818a3dc215f5889a",
            "ac16338ed6ce453bba79ecc0a67997d6",
            "781c1e3419e6483dbdf6a06811dec54a",
            "29c5a1e4482d41f6a85e56ea26a70dcb",
            "bf7dd77c3c32481f8a62b90ae11a4697",
            "b8f350da1530406d93bb7e6e9426f0a2",
            "e1602ac2d0c947b18ae1ff1d955aee1d",
            "81604830546f4997bee8e32c658ac928",
            "691a66462fdd4e1c911720eb7b2a54da",
            "fda6eb02874b4eccb7a0dd651c2c0f5a",
            "4500fed31e7147bcb4db784d8188290a",
            "45a5e2b366b5453db3435922f70130ce",
            "a78b826270c8442baec8239fccaa5fbe",
            "ae9355298edd4acf843369814b79fbad",
            "919fdb88468a46a284ba536a7f3d4d67",
            "aab69b9421cf4adcac64e36cfb996e21",
            "65efa603049e4d439cc7b429ba6f30d5",
            "820501fda6e74de8804e36da4c597255",
            "42e779162bdb40439edf5333eae14585",
            "1f73446a0157414a989a2f262349c11e",
            "541f72c9205849a4ba0609aee073abfa",
            "ed65fc9ed62d46018e5749e746a61f28",
            "7c3b24e94a9f44fb8b60a99f37c12053",
            "8755ce1358844a2c912262c7e50cd719",
            "c1bf18db36054828bf920477c2ef49df",
            "092d182ed34047a7af019783c6951550",
            "d34f57f38e844d80ae558f904e7f74f6"
          ]
        },
        "outputId": "78d27144-475b-4dc3-b188-1c85ad68c3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/790 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90c7ccc29f8149a5821eb364d69207ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/44.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86d4eeb597074f209e572c8e07e49991"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/428 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f870599dce248bd9952b3456dba5afc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/742k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "daac49eb8fb746d29d856894afa6fece"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.25M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8f350da1530406d93bb7e6e9426f0a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/245 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65efa603049e4d439cc7b429ba6f30d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: AlbertModel \n",
              "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize MNR loss for training; we get all the pairs together and we rank all the other contexts as dissimilar\n",
        "from sentence_transformers import losses\n",
        "\n",
        "loss = losses.MultipleNegativesRankingLoss(model)"
      ],
      "metadata": {
        "id": "RXQ-Dvb2btNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training!\n",
        "\n",
        "epochs = 1\n",
        "warmup_steps = int(len(loader) * epochs * 0.1)\n",
        "model.fit(\n",
        "    train_objectives = [(loader, loss)],\n",
        "    epochs=epochs,\n",
        "    warmup_steps = warmup_steps,\n",
        "    output_path='mpnet-mnr-squad2',\n",
        "    show_progress_bar=True\n",
        ")"
      ],
      "metadata": {
        "id": "UlfDzGcecEog",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "5dc0e82f2b0b4d4d96d1cd485eb3d0f6",
            "652c6ce9bfed40ce98fb41de1620c1a7",
            "eb4213d3175c4a938fdeacd08fc2dfcb",
            "68fa75e5c1d748859c24efa02a14897c",
            "e2a2f7354bfc4641a14e8b849433cf3c",
            "9568886c2e2e47d39c769583e3d49a59",
            "9eb3ffa6645342f896b9a6551c2e2091",
            "947afaf6f74f44bd978f1d380e46cd77",
            "39ec2bd23d5b43d5b14b1129f5128646",
            "4a5a2b66e3314244866bd08430273853",
            "b18a036ffa684e30a121cd7dc1c6a9df",
            "29665a6b7ce144e79618b6d9533f9ccf",
            "6b8fac9d29c44aae8eefef6da5dda887",
            "c04785bd95eb433786c5ff1b560517a7",
            "ae4a66c6a4d14552b8e27029c61453f5",
            "6113136e759e4d92a1b4d8b0cd838af8",
            "7d6067d14528498fa5ac25466987ed05",
            "6a4060d26626463089e86f31880bd10d",
            "d42d12e3255e44f6a7a04516f6673c73",
            "16a6c91e1caa43d68b76aba32c222643",
            "9a8307046d064725a45a63a56f3ceaca",
            "dde033de08a04ed2b2421d089de5ee01"
          ]
        },
        "outputId": "437445e7-588e-466a-8d5e-c4460e6c5c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5dc0e82f2b0b4d4d96d1cd485eb3d0f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Iteration:   0%|          | 0/17 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29665a6b7ce144e79618b6d9533f9ccf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "f9kvWk1xaeeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model"
      ],
      "metadata": {
        "id": "Zj9CK8lwbyir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "\n",
        "eval_df = pd.DataFrame()\n",
        "for index , row in dataset_for_model.iterrows():\n",
        "  eval_df = eval_df.append({\n",
        "      \"question\": row[\"question\"],\n",
        "      \"context\": row[\"context\"],\n",
        "      \"id\":row[\"id\"]\n",
        "  } , ignore_index = True)\n"
      ],
      "metadata": {
        "id": "XZb4T4rqb0Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "id": "S5EUGiujzZwg",
        "outputId": "61570b8c-ab71-4347-8d1c-2837a5d3c1a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            question  \\\n",
              "0  Only Tensors created explicitly by the user (g...   \n",
              "1                         How to test in low version   \n",
              "2  Loading from checkpoints re-downloads pre-trai...   \n",
              "3                         Selecting one gpu from cli   \n",
              "4          CUDA OOM during validation of first epoch   \n",
              "\n",
              "                                             context  \\\n",
              "0  how can i solve this problem, my net is DB,  d...   \n",
              "1  There is no train.test () in the lower version...   \n",
              "2  I am defining a simple multi-class BERT classi...   \n",
              "3  Hi, I have 4 gpus on my machine. I want to sel...   \n",
              "4  hi all,\\nMy model validation code (see below) ...   \n",
              "\n",
              "                             id  \n",
              "0  MDEwOkRpc2N1c3Npb24zNTE3NTEx  \n",
              "1  MDEwOkRpc2N1c3Npb24zNDI2NzQy  \n",
              "2  MDEwOkRpc2N1c3Npb24zNTQ4NzY1  \n",
              "3  MDEwOkRpc2N1c3Npb24zNDIyMTky  \n",
              "4            D_kwDOCqWgoM4AONtA  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c3d38a4-7126-44d5-a30c-866c0acdbe38\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Only Tensors created explicitly by the user (g...</td>\n",
              "      <td>how can i solve this problem, my net is DB,  d...</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNTE3NTEx</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How to test in low version</td>\n",
              "      <td>There is no train.test () in the lower version...</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNDI2NzQy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Loading from checkpoints re-downloads pre-trai...</td>\n",
              "      <td>I am defining a simple multi-class BERT classi...</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNTQ4NzY1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Selecting one gpu from cli</td>\n",
              "      <td>Hi, I have 4 gpus on my machine. I want to sel...</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNDIyMTky</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CUDA OOM during validation of first epoch</td>\n",
              "      <td>hi all,\\nMy model validation code (see below) ...</td>\n",
              "      <td>D_kwDOCqWgoM4AONtA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c3d38a4-7126-44d5-a30c-866c0acdbe38')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2c3d38a4-7126-44d5-a30c-866c0acdbe38 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2c3d38a4-7126-44d5-a30c-866c0acdbe38');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# remove duplication in the dataset"
      ],
      "metadata": {
        "id": "5G5PfiQpt5TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_dupe = eval_df.drop_duplicates(\n",
        "    subset='context',\n",
        "    keep='first'\n",
        ")\n",
        "# also drop question column\n",
        "no_dupe = no_dupe.drop(columns=['question'])\n",
        "# and give each context a slightly unique ID\n",
        "no_dupe['id'] = no_dupe['id'] + 'con'\n",
        "no_dupe.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "CLGkj9a3t7dl",
        "outputId": "cb6ab968-a0ba-4d23-ace3-e48f5abcffaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             context  \\\n",
              "0  how can i solve this problem, my net is DB,  d...   \n",
              "1  There is no train.test () in the lower version...   \n",
              "2  I am defining a simple multi-class BERT classi...   \n",
              "3  Hi, I have 4 gpus on my machine. I want to sel...   \n",
              "4  hi all,\\nMy model validation code (see below) ...   \n",
              "\n",
              "                                id  \n",
              "0  MDEwOkRpc2N1c3Npb24zNTE3NTExcon  \n",
              "1  MDEwOkRpc2N1c3Npb24zNDI2NzQycon  \n",
              "2  MDEwOkRpc2N1c3Npb24zNTQ4NzY1con  \n",
              "3  MDEwOkRpc2N1c3Npb24zNDIyMTkycon  \n",
              "4            D_kwDOCqWgoM4AONtAcon  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a98d20bf-6591-4550-bdbe-3892c8e51714\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>how can i solve this problem, my net is DB,  d...</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNTE3NTExcon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>There is no train.test () in the lower version...</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNDI2NzQycon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I am defining a simple multi-class BERT classi...</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNTQ4NzY1con</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hi, I have 4 gpus on my machine. I want to sel...</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNDIyMTkycon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hi all,\\nMy model validation code (see below) ...</td>\n",
              "      <td>D_kwDOCqWgoM4AONtAcon</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a98d20bf-6591-4550-bdbe-3892c8e51714')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a98d20bf-6591-4550-bdbe-3892c8e51714 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a98d20bf-6591-4550-bdbe-3892c8e51714');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_df  = eval_df.merge(no_dupe , how=\"inner\" , on=\"context\")\n"
      ],
      "metadata": {
        "id": "VwskiKDsuQPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "Z4Bmtqglwv9j",
        "outputId": "780840f7-75fe-4ee0-aed5-6014f7bcc276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              question  \\\n",
              "0    Only Tensors created explicitly by the user (g...   \n",
              "1                           How to test in low version   \n",
              "2    Loading from checkpoints re-downloads pre-trai...   \n",
              "3                           Selecting one gpu from cli   \n",
              "4            CUDA OOM during validation of first epoch   \n",
              "..                                                 ...   \n",
              "423                                Problems in Pruning   \n",
              "424             About `ModelCheckpoint` starting point   \n",
              "425                       Multi-GPU Training GPU Usage   \n",
              "426  Link arguments from Datamodule into init_args ...   \n",
              "427                                CNN dimension error   \n",
              "\n",
              "                                               context  \\\n",
              "0    how can i solve this problem, my net is DB,  d...   \n",
              "1    There is no train.test () in the lower version...   \n",
              "2    I am defining a simple multi-class BERT classi...   \n",
              "3    Hi, I have 4 gpus on my machine. I want to sel...   \n",
              "4    hi all,\\nMy model validation code (see below) ...   \n",
              "..                                                 ...   \n",
              "423  Hello, I am trying to get pruning to work with...   \n",
              "424  My ModelCheckpoint callback:\\nckpt_callback = ...   \n",
              "425  ❓ Multi-GPU Training GPU Usage\\nBefore asking:...   \n",
              "426  Hey!\\nI'm trying to use LightningArgumentParse...   \n",
              "427  Hi there!\\nI am trying to build a very basic C...   \n",
              "\n",
              "                             id_x                             id_y  \n",
              "0    MDEwOkRpc2N1c3Npb24zNTE3NTEx  MDEwOkRpc2N1c3Npb24zNTE3NTExcon  \n",
              "1    MDEwOkRpc2N1c3Npb24zNDI2NzQy  MDEwOkRpc2N1c3Npb24zNDI2NzQycon  \n",
              "2    MDEwOkRpc2N1c3Npb24zNTQ4NzY1  MDEwOkRpc2N1c3Npb24zNTQ4NzY1con  \n",
              "3    MDEwOkRpc2N1c3Npb24zNDIyMTky  MDEwOkRpc2N1c3Npb24zNDIyMTkycon  \n",
              "4              D_kwDOCqWgoM4AONtA            D_kwDOCqWgoM4AONtAcon  \n",
              "..                            ...                              ...  \n",
              "423  MDEwOkRpc2N1c3Npb24zMzEzODI5  MDEwOkRpc2N1c3Npb24zMzEzODI5con  \n",
              "424            D_kwDOCqWgoM4AO0kZ            D_kwDOCqWgoM4AO0kZcon  \n",
              "425  MDEwOkRpc2N1c3Npb244MjI2MA==  MDEwOkRpc2N1c3Npb244MjI2MA==con  \n",
              "426            D_kwDOCqWgoM4AOoRX            D_kwDOCqWgoM4AOoRXcon  \n",
              "427  MDEwOkRpc2N1c3Npb24zNDQwNDk0  MDEwOkRpc2N1c3Npb24zNDQwNDk0con  \n",
              "\n",
              "[428 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fbe69b02-945c-44be-b6fd-5987efa9ce78\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>id_x</th>\n",
              "      <th>id_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Only Tensors created explicitly by the user (g...</td>\n",
              "      <td>how can i solve this problem, my net is DB,  d...</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNTE3NTEx</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNTE3NTExcon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How to test in low version</td>\n",
              "      <td>There is no train.test () in the lower version...</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNDI2NzQy</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNDI2NzQycon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Loading from checkpoints re-downloads pre-trai...</td>\n",
              "      <td>I am defining a simple multi-class BERT classi...</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNTQ4NzY1</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNTQ4NzY1con</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Selecting one gpu from cli</td>\n",
              "      <td>Hi, I have 4 gpus on my machine. I want to sel...</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNDIyMTky</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNDIyMTkycon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CUDA OOM during validation of first epoch</td>\n",
              "      <td>hi all,\\nMy model validation code (see below) ...</td>\n",
              "      <td>D_kwDOCqWgoM4AONtA</td>\n",
              "      <td>D_kwDOCqWgoM4AONtAcon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>Problems in Pruning</td>\n",
              "      <td>Hello, I am trying to get pruning to work with...</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zMzEzODI5</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zMzEzODI5con</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>About `ModelCheckpoint` starting point</td>\n",
              "      <td>My ModelCheckpoint callback:\\nckpt_callback = ...</td>\n",
              "      <td>D_kwDOCqWgoM4AO0kZ</td>\n",
              "      <td>D_kwDOCqWgoM4AO0kZcon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>Multi-GPU Training GPU Usage</td>\n",
              "      <td>❓ Multi-GPU Training GPU Usage\\nBefore asking:...</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb244MjI2MA==</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb244MjI2MA==con</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>Link arguments from Datamodule into init_args ...</td>\n",
              "      <td>Hey!\\nI'm trying to use LightningArgumentParse...</td>\n",
              "      <td>D_kwDOCqWgoM4AOoRX</td>\n",
              "      <td>D_kwDOCqWgoM4AOoRXcon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>CNN dimension error</td>\n",
              "      <td>Hi there!\\nI am trying to build a very basic C...</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNDQwNDk0</td>\n",
              "      <td>MDEwOkRpc2N1c3Npb24zNDQwNDk0con</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>428 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fbe69b02-945c-44be-b6fd-5987efa9ce78')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fbe69b02-945c-44be-b6fd-5987efa9ce78 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fbe69b02-945c-44be-b6fd-5987efa9ce78');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mapping question to index id "
      ],
      "metadata": {
        "id": "-ogSpX5nwVOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ir_queries = {\n",
        "    row['id_x']: row['question'] for i, row in eval_df.iterrows()\n",
        "}\n",
        "ir_queries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vHgmTiKwXsD",
        "outputId": "56832afd-0499-4bb6-cc25-7f8f2bf38454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'D_kwDOCqWgoM4AN-cL': 'ModelCheckpoint save nothing',\n",
              " 'D_kwDOCqWgoM4AN-x1': 'Issue in fitting model and finding optimal learning rate parameter',\n",
              " 'D_kwDOCqWgoM4AN16B': 'How to save/load only part of the weights in the model?',\n",
              " 'D_kwDOCqWgoM4AN2Gd': 'Example on training with TPU does not run at all',\n",
              " 'D_kwDOCqWgoM4AN49x': 'Exporting PyTorch Lightning model to ONNX format not working',\n",
              " 'D_kwDOCqWgoM4AN4Of': 'How to save and load LightningModule whose input containing the pretrained moduel?',\n",
              " 'D_kwDOCqWgoM4AN4Z4': 'How to disable logging temporarily?',\n",
              " 'D_kwDOCqWgoM4AN5Lv': 'The call of training_step and validation_step .etc.',\n",
              " 'D_kwDOCqWgoM4AN637': 'multiple on_train_epoch_start callbacks but only one on_train_epoch_end?',\n",
              " 'D_kwDOCqWgoM4AN6JX': \"'NeuralNetwork' object has no attribute 'log'\",\n",
              " 'D_kwDOCqWgoM4AN6mz': 'Model with best validation accuracy',\n",
              " 'D_kwDOCqWgoM4AN6vA': 'Getting the test score after restoring a pretrained model',\n",
              " 'D_kwDOCqWgoM4AN7kb': 'MisconfigurationException: Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged. You can fix this by setting an attribute for the metric in your `LightningModule`.',\n",
              " 'D_kwDOCqWgoM4AN88O': 'How to pass gradients to `backward()`',\n",
              " 'D_kwDOCqWgoM4AN8UH': 'Saving one single inference example per validation stage',\n",
              " 'D_kwDOCqWgoM4AN_7D': 'Questions about Fault-tolerant Training',\n",
              " 'D_kwDOCqWgoM4AN_8r': 'Lightning is very slow - Performance divided by ~4 compared to Pytorch. 10s wait between epochs.',\n",
              " 'D_kwDOCqWgoM4AN_cS': 'LightningCLI  - instantiate model from config',\n",
              " 'D_kwDOCqWgoM4ANn3D': 'Inheritance and `save_hyperparameters`',\n",
              " 'D_kwDOCqWgoM4ANo9F': 'How can I save and restore the trained model when I call fit() at pytorch_lightning every time?',\n",
              " 'D_kwDOCqWgoM4ANogY': 'How to define an interval validate callbacks in lightning',\n",
              " 'D_kwDOCqWgoM4ANqDI': \"ValueError('signal only works in main thread')\",\n",
              " 'D_kwDOCqWgoM4ANrAK': 'Confusion in training_step_end() API',\n",
              " 'D_kwDOCqWgoM4ANrY-': 'How to ignore certain \"parameters\" with model checkpointing?',\n",
              " 'D_kwDOCqWgoM4ANrkB': 'Does PL support customizing indicators in checkpoint callback？',\n",
              " 'D_kwDOCqWgoM4ANrmA': 'How to have a silent lr_find()',\n",
              " 'D_kwDOCqWgoM4ANuM0': 'How to pass arrays to callbacks?',\n",
              " 'D_kwDOCqWgoM4ANuQ1': 'How to apply uniform length batching(smart batching)?',\n",
              " 'D_kwDOCqWgoM4ANuaZ': \"KeyError: 'Trying to restore training state but checkpoint contains only the model. This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.'\",\n",
              " 'D_kwDOCqWgoM4ANv6a': 'How to load a released PL model in my directory?',\n",
              " 'D_kwDOCqWgoM4ANv6b': 'How to load a released PL model in my directory?',\n",
              " 'D_kwDOCqWgoM4ANvwZ': 'DDP failing when using multiple nodes',\n",
              " 'D_kwDOCqWgoM4ANwT_': 'Optimization in a dual encoder LitModel',\n",
              " 'D_kwDOCqWgoM4ANwn_': 'Custom scheduler',\n",
              " 'D_kwDOCqWgoM4ANx-L': 'Update Adam learning rate after 10 epochs',\n",
              " 'D_kwDOCqWgoM4ANxgv': 'String “best” at argument “ckpt_path” for test method of Trainer class',\n",
              " 'D_kwDOCqWgoM4ANxrd': 'How to calculate AUC over an entire test set?',\n",
              " 'D_kwDOCqWgoM4ANymu': 'pytorch-lightning output embeddings completely differnt than pytorch vanilla',\n",
              " 'D_kwDOCqWgoM4ANzLv': 'What are ones options for manually defining the parallelization?',\n",
              " 'D_kwDOCqWgoM4AO0Fs': 'Logging multiple scalars to a single wandb chart',\n",
              " 'D_kwDOCqWgoM4AO0KS': 'How to flag certain modules as non-deterministic',\n",
              " 'D_kwDOCqWgoM4AO0kZ': 'About `ModelCheckpoint` starting point',\n",
              " 'D_kwDOCqWgoM4AO1KV': \"import pytorch_lightning fails with ModuleNotFoundError: No module named 'tensorboard'\",\n",
              " 'D_kwDOCqWgoM4AO1fM': 'How to call torch.distributed.get_rank() in model building phase',\n",
              " 'D_kwDOCqWgoM4AO1rw': 'Clarification on reload_dataloaders_every_epoch',\n",
              " 'D_kwDOCqWgoM4AO2uC': 'Loading Lightning model in PyTorch',\n",
              " 'D_kwDOCqWgoM4AO2wI': \"AttributeError: 'Trainer' object has no attribute 'lr_find'\",\n",
              " 'D_kwDOCqWgoM4AO3Et': 'self.local_rank in LightningDataModule',\n",
              " 'D_kwDOCqWgoM4AO3U-': 'Save predictions in `test_step`',\n",
              " 'D_kwDOCqWgoM4AO4EP': \"AttributeError: 'Trainer' object has no attribute 'run_evaluation'\",\n",
              " 'D_kwDOCqWgoM4AO4uE': 'Logging Multi-Label Metrics',\n",
              " 'D_kwDOCqWgoM4AO6Tv': 'fast_dev_run does not execute pl.LightningModule.test_step()',\n",
              " 'D_kwDOCqWgoM4AO7KD': 'Pytorch-lightning is not able to load the model checkpoint',\n",
              " 'D_kwDOCqWgoM4AO7Ye': 'When are buffers moved to gpu?',\n",
              " 'D_kwDOCqWgoM4AO7sj': 'NCCL WARN Failed to open libibverbs.so[.1]',\n",
              " 'D_kwDOCqWgoM4AO7tD': 'what does `step` mean in `max_steps` ?',\n",
              " 'D_kwDOCqWgoM4AO89y': 'New error in Trainer started appearing recently in a previously running code.',\n",
              " 'D_kwDOCqWgoM4AO97e': 'overfit_batches duplicates entire train DataLoader causing out of memory',\n",
              " 'D_kwDOCqWgoM4AO9hZ': \"If tuner.scale_batch_size() accepts a train_dataloader, why can't this be used independently of trainer.fit(model, datamodule)?\",\n",
              " 'D_kwDOCqWgoM4AOAac': 'Early stop saving best checkpoints',\n",
              " 'D_kwDOCqWgoM4AOCkt': 'How to access `LightningDataModule` in `LightningModule`',\n",
              " 'D_kwDOCqWgoM4AODJU': 'Test results different between logging in test_step and logging in test_epoch_end',\n",
              " 'D_kwDOCqWgoM4AOE7x': 'Trainer.fit validating before finishing current training epoch',\n",
              " 'D_kwDOCqWgoM4AOGUx': 'What is the relationship beween accumulate_grad_batches and lr_scheduler?',\n",
              " 'D_kwDOCqWgoM4AOGVE': 'After updating to 1.5.2, NotImplementedError: `train_dataloader`',\n",
              " 'D_kwDOCqWgoM4AOHL0': '[RFC] Thoughts on `on_init_start` and `on_init_end` hooks',\n",
              " 'D_kwDOCqWgoM4AOH_m': 'How does LightningLite handle the grad scaler state dict of torch.amp?',\n",
              " 'D_kwDOCqWgoM4AOI4h': 'UserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.',\n",
              " 'D_kwDOCqWgoM4AOI5p': 'What if I load data in __init__ function of LightningDataModule',\n",
              " 'D_kwDOCqWgoM4AOIRx': 'Doing the sampling of batch indexes inside lightning',\n",
              " 'D_kwDOCqWgoM4AOIa1': 'After changing to pytorch-lightning 1.5.2, omitting the argument of trainer.test() does not work.',\n",
              " 'D_kwDOCqWgoM4AOIuB': 'How to do model comparison with pytorch lightning',\n",
              " 'D_kwDOCqWgoM4AOJ8D': 'Is there a way to save only part of the Lightning sub-modules to the checkpoint file?',\n",
              " 'D_kwDOCqWgoM4AOJpq': 'Does .predict() also use the best weights?',\n",
              " 'D_kwDOCqWgoM4AOKTv': 'RichProgressBar is hard to read in light theme',\n",
              " 'D_kwDOCqWgoM4AOKiQ': 'checkpoint every module in a different ckpt file',\n",
              " 'D_kwDOCqWgoM4AOKtd': 'How many effective workers does my code use?',\n",
              " 'D_kwDOCqWgoM4AOL5N': 'Not able to Generate Predictions with Trainer.predict()',\n",
              " 'D_kwDOCqWgoM4AOL9I': 'Dataloader pickle torchio',\n",
              " 'D_kwDOCqWgoM4AOLA6': 'Any guide on how the callbacks and hooks workflow works?',\n",
              " 'D_kwDOCqWgoM4AOLTw': 'Found mistake in pytorch-lightning DQN example. How do I upload a fix?',\n",
              " 'D_kwDOCqWgoM4AOM_J': 'Uninstalling pytorch-lightning',\n",
              " 'D_kwDOCqWgoM4AOMiz': 'how to use Apex DistributedDataParallel with Lightining?',\n",
              " 'D_kwDOCqWgoM4AOMkC': 'Logging tensorboard not showing loss',\n",
              " 'D_kwDOCqWgoM4AONiM': 'Best practices: CLI and Loading DataModule from config.yaml',\n",
              " 'D_kwDOCqWgoM4AONtA': 'CUDA OOM during validation of first epoch',\n",
              " 'D_kwDOCqWgoM4AOO-Y': \"AttributeError: module 'pytorch_lightning' has no attribute 'metrics'\",\n",
              " 'D_kwDOCqWgoM4AOOuk': 'How to compute a squad metric?',\n",
              " 'D_kwDOCqWgoM4AOOv9': 'Why accumulate_grad_batches cannot be used with manual optimization?',\n",
              " 'D_kwDOCqWgoM4AOP1k': 'Accessing available values to monitor when saving checkpoints',\n",
              " 'D_kwDOCqWgoM4AOPZa': 'How can one use an external optimizer with LightningCLI?',\n",
              " 'D_kwDOCqWgoM4AOPhE': 'Do LightningDataModule support multi train dataloader like LightningModule?',\n",
              " 'D_kwDOCqWgoM4AORWw': 'Can I turn off Validation step when overfit_batches=X?',\n",
              " 'D_kwDOCqWgoM4AOSKC': 'Combine outputs in test epochs when using DDP',\n",
              " 'D_kwDOCqWgoM4AOSKg': 'Save checkpoints without overwrite',\n",
              " 'D_kwDOCqWgoM4AOT-T': 'Save checkpoint with specific monitor criteria',\n",
              " 'D_kwDOCqWgoM4AOUM0': 'Multiple Validation Sets',\n",
              " 'D_kwDOCqWgoM4AOUfJ': 'val_check_interval every N global steps?',\n",
              " 'D_kwDOCqWgoM4AOVh6': 'How should the number of steps be set against to processed data when using ddp and multi GPU',\n",
              " 'D_kwDOCqWgoM4AOW_y': 'training on gpu becomes non-deterministic',\n",
              " 'D_kwDOCqWgoM4AOXgl': 'Does LightningLite still support various callbacks?',\n",
              " 'D_kwDOCqWgoM4AOXqQ': 'ddp: how to combine multi-gpus outputs like \"training_step_end\" which is only used in dp/ddp2?',\n",
              " 'D_kwDOCqWgoM4AOY0y': \"Accuracy doesn't show up in progress bar\",\n",
              " 'D_kwDOCqWgoM4AOYR-': 'How to access the strategy of the trainer',\n",
              " 'D_kwDOCqWgoM4AOYvd': 'using EMA with model checkpoints',\n",
              " 'D_kwDOCqWgoM4AOZEQ': '`save_hyperparameters()` is very slow when there are many hyperparameters, any speed up?',\n",
              " 'D_kwDOCqWgoM4AOZMw': 'model inference but self.training is save true',\n",
              " 'D_kwDOCqWgoM4AOa1k': 'self.manual_backward() vs. loss.backward() when optimizing manually',\n",
              " 'D_kwDOCqWgoM4AOaI0': 'Where to transform and inverse-transform',\n",
              " 'D_kwDOCqWgoM4AObNj': 'Why is on_before_optimizer_step incompatible with accumulate_grad_batches ?',\n",
              " 'D_kwDOCqWgoM4AOb_Z': 'PyTorch Lightning Optimizer_Step() prevents training_step() from running',\n",
              " 'D_kwDOCqWgoM4AOd26': 'How to show the validation loss in progress bar?',\n",
              " 'D_kwDOCqWgoM4AOd6S': 'Gradient Clipping with mix precision in case of NaN loss',\n",
              " 'D_kwDOCqWgoM4AOdFs': \"what's the difference between `load_from_checkpoint ` and `resume_from_checkpoint`\",\n",
              " 'D_kwDOCqWgoM4AOdZ5': 'val_dataloader` has `shuffle=True` though its false',\n",
              " 'D_kwDOCqWgoM4AOeB9': 'Unable to load pretrained weight into custom model in Pytorch Lightning',\n",
              " 'D_kwDOCqWgoM4AOemV': 'trainer.test( ) not working',\n",
              " 'D_kwDOCqWgoM4AOgOw': 'LightningCLI: how to configure logger using cmd-line args?',\n",
              " 'D_kwDOCqWgoM4AOgPG': 'Access a registered buffer is very slow',\n",
              " 'D_kwDOCqWgoM4AOgZD': 'WGAN - discriminator and generator updates inconsistency',\n",
              " 'D_kwDOCqWgoM4AOkGz': '\"resume from checkpoint\" lead to CUDA out of memory',\n",
              " 'D_kwDOCqWgoM4AOoRX': 'Link arguments from Datamodule into init_args of lr_scheduler',\n",
              " 'D_kwDOCqWgoM4AOp8R': 'How to access validation step outputs of complete epoch in a `on_validation_epoch_end` hook for a custom callback ?',\n",
              " 'D_kwDOCqWgoM4AOqUS': 'Disabling find_unused_parameters',\n",
              " 'D_kwDOCqWgoM4AOqoj': 'Get batch’s datapoints across all GPUs',\n",
              " 'D_kwDOCqWgoM4AOqpi': 'Question about the log system',\n",
              " 'D_kwDOCqWgoM4AOrGl': 'Gradient accumulation + DeepSpeed LR scheduler',\n",
              " 'D_kwDOCqWgoM4AOsH4': 'Hook for Fully Formed Checkpoints',\n",
              " 'D_kwDOCqWgoM4AOsH5': 'Hook for Fully Formed Checkpoints',\n",
              " 'D_kwDOCqWgoM4AOsPR': 'Enabling dropout during trainer.predict',\n",
              " 'D_kwDOCqWgoM4AOsmI': 'Iterating over task for Continual Learning.',\n",
              " 'D_kwDOCqWgoM4AOtq7': 'CacheDataset with DDP and Multi-GPUs',\n",
              " 'D_kwDOCqWgoM4AOtvt': 'Confusion about NeptuneLogger.save_dir implementation',\n",
              " 'D_kwDOCqWgoM4AOuBR': 'Multi-GPU Tensor Initialization Question',\n",
              " 'D_kwDOCqWgoM4AOuUN': 'Saving checkpoint, hparams & tfevents after training to separate folder',\n",
              " 'D_kwDOCqWgoM4AOuWg': 'Is it okay to feed optimizer to `configure_optimizers`',\n",
              " 'D_kwDOCqWgoM4AOwH-': 'Remove parameters from autograd backward hook',\n",
              " 'D_kwDOCqWgoM4AOwQY': 'When doing `fit()`, `self.training` in `forward()` keeps turning into False?',\n",
              " 'D_kwDOCqWgoM4AOwcv': 'Using Multiple Optimisers gives Index Error?',\n",
              " 'D_kwDOCqWgoM4AOwkL': 'Code not printing values in trained_epoch_end',\n",
              " 'D_kwDOCqWgoM4AOxVN': 'lowest val/loss ckpt != highest val/Accuracy',\n",
              " 'D_kwDOCqWgoM4AOxzH': 'Odd Performance Using Multi-GPU + Azure',\n",
              " 'D_kwDOCqWgoM4AOy9z': 'AttributeError: \\'Trainer\\' object has no attribute \\'running_sanity_check\"',\n",
              " 'D_kwDOCqWgoM4AOyUn': 'Validation step: error when trying to return object',\n",
              " 'D_kwDOCqWgoM4AOzN_': 'ValueError: Expected positive integer total_steps, but got -1',\n",
              " 'D_kwDOCqWgoM4AOzRX': \"trainer.fit(strategy='ddp') executes code repeatedly\",\n",
              " 'D_kwDOCqWgoM4APAR6': 'Torch accuracy and sklearn accuracy is v different',\n",
              " 'D_kwDOCqWgoM4APBbi': 'the process would be blocking in Validation step',\n",
              " 'D_kwDOCqWgoM4APCns': 'How to evaluate every X steps?',\n",
              " 'D_kwDOCqWgoM4APFBW': 'Lighting Module Loaded From Checkpoint Generates Different Output Each Time',\n",
              " 'D_kwDOCqWgoM4APFIR': 'Error with loading model checkpoint',\n",
              " 'D_kwDOCqWgoM4APFNC': 'Run Trainer.fit multiple times under DDP mode',\n",
              " 'D_kwDOCqWgoM4APFqd': 'How to switch dataloader every n training steps',\n",
              " 'D_kwDOCqWgoM4APGS0': 'Instantiate data augmentations through CLI',\n",
              " 'D_kwDOCqWgoM4APGyZ': 'help defining new training_step() on a callback',\n",
              " 'D_kwDOCqWgoM4APH-L': 'Should I configure FP16, optimizers, batch_size in DeepSpeed config of Pytorch-Lightning?',\n",
              " 'D_kwDOCqWgoM4APJ2u': 'Training based on iterations',\n",
              " 'D_kwDOCqWgoM4APM3c': 'Option for disable tf32',\n",
              " 'D_kwDOCqWgoM4APNwR': 'Loading model for prediction yields RuntimeError: Error(s) in loading state_dict',\n",
              " 'D_kwDOCqWgoM4APO41': 'model weight file corrupted when training on multi-gpus',\n",
              " 'D_kwDOCqWgoM4APOIS': 'Manually averaging metrics when logging',\n",
              " 'D_kwDOCqWgoM4APOMH': 'hyper parameters not restored while resuming training',\n",
              " 'D_kwDOCqWgoM4APPdA': 'Trainer: loss stagnates, whereas custom train implementation continues converging ??',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI0MA==': 'extremely slow training with multiple GPUs',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI0MQ==': 'How to gather results on multiple GPUs while testing? ddp',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI0Ng==': 'slow new epoch start with setting ddp, num_workers, gpus',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI1MQ==': 'Is it possible for SLURM auto submit to work on DP?',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI1Mg==': 'How to use pytorch-lightning distributed training without SLURM?',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI1NQ==': \"online hard-example mining/examining under Multi-GPU ='dp'\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjI1Nw==': 'Does Pytorch-Lightning have a multiprocessing (or Joblib) module?',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI2MA==': 'Multi-GPU Training GPU Usage',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI3MQ==': 'How do I set the steps_per_epoch parameter of a lr scheduler in multi-GPU environment?',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI3NA==': \"iterations/gpu don't scale when using custom sampler\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjI3Ng==': 'How to scale learning rate with batch size for DDP training?',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI3OA==': 'Test step with DDP',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI4Mg==': 'Expectations for custom data parallel implementations',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI4OQ==': 'Attribute is reset per batch in `dp` mode',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI5Nw==': 'DDP specifics: Single node function execution, test loss sync, num_workers, sync_batchnorm, precision',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI5OA==': 'DDP NCCL freezes in docker AWS Jupyter',\n",
              " 'MDEwOkRpc2N1c3Npb244MjIwNw==': 'How to analysis the time cost of each part',\n",
              " 'MDEwOkRpc2N1c3Npb244MjIyMA==': \"How to get gpu id corresponding to each process in 'ddp'?\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjIyNw==': 'Why MultiGPU dp seems slower?',\n",
              " 'MDEwOkRpc2N1c3Npb244MjMwMA==': 'logging question in DDP',\n",
              " 'MDEwOkRpc2N1c3Npb244MjMwMw==': 'Can the learning rate find by dp using one more gpu be used in ddp?',\n",
              " 'MDEwOkRpc2N1c3Npb244MzU0MA==': 'Trainer.test() on ddp can not get entire dataset.',\n",
              " 'MDEwOkRpc2N1c3Npb24xNjMyMDI5': 'trainer.check',\n",
              " 'MDEwOkRpc2N1c3Npb24xOTI2Mzgz': 'Is pytorch lightning using MKLDNN when no GPU is available ?',\n",
              " 'MDEwOkRpc2N1c3Npb24yMTc5Njk3': 'Override .ckpt in ModelCheckpoint Callback',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkxNTQ4': 'Print weights summary',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkxODc3': 'How to accumulate metrics for multiple validation dataloaders',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkxOTMy': 'Code stuck after running 1 epoch on TPU',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkxOTU0': 'how to set find_unused_parameters=True?',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyMjEx': 'Logging accuracy with batch accumulation',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyMjYx': 'Precision and Recall over validation step',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyMzU2': 'Call or Forward?',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyMzkw': 'Restore the best model',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNDcx': 'Multiple optimizers but only one loss',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNDgy': 'Unfreezing layers during training?',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTAz': 'Training/Validation split in minimal example',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTEy': 'About the Weight Initialization in PL',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTIz': 'Simple way to get the best scores at the end of a training?',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTM0': 'Running Average of my accuracy, losses etc.',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTMw': 'How set number of epochs',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTQ2': 'What is hparams exactly?',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTU3': 'Where is EarlyStopping searching for metrics?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMTc5NTk3': 'Logging RL results and tracking them with ModelCheckpoint(monitor=...)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMTc5Njcw': 'How to remove hp_metric initial -1 point and x=0 points?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjIzNDA4': 'Trainer cannot handle 1d tensor when return results from test_epoch_end',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjMyNTUw': 'To find r2score of my model',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjMyNzk1': 'Access and change models optimizer after setup',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjMzMTgw': 'How to apply a nn.Module (i.e. CNN) across an axis (i.e. Video input) in a parallelizable way',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQ4NTU2': 'How to save a checkpoint every n steps and overwrite the previous saved checkpoints?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQ5MTMx': 'How to make PyTorch Lightning quiet?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQ5ODE4': 'Deterministic DataLoader on DDP reads same data on all subprocesses',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQyMjE4': 'Logging metrics with compute()',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQyMjUy': 'How to “lightninfy” the official PyTorch sentiment analysis tutorial?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQyNzg5': 'How do I incorporate a scheduler with \"step\" and \"batch step\"?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQzMTM4': 'Error importing pytorch lighting',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQzNDM5': \"TPU on Colab: unexpected keyword argument 'num_tpu_cores'\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjU0ODEx': 'How to print metric value every epoch ?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjU1MDkx': 'Access datamodule in custom callbacks',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjU1NDQ5': 'import pytorch_lightning as pl does not work on colab',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjU2NDQ1': 'changing val_check_interval from 0.01 to 0.005 changes number of steps in an epoch',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjU4OTQz': 'Scheduler.step() only called on the end of validation',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjUwMjU0': 'How to sequentially call fit() and test() in DDP',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjUwMzc1': 'How to correctly apply metrics API in binary use case',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjUxMDg5': 'Bug in SLURMConnector?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjY0NDkw': 'Pretrain some sections of a model, then initialize those parts when training a full model',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjY1OTM5': 'Is Trainer.validate not available now in the latest version?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjY5NDYx': 'How to continue training with a different learning rate',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjYwMjk0': 'wrong global rank when trying multi-nodes',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjYwODA5': 'Custom training loop for LightningModule',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjYyMjcy': 'How to get global step from checkpoint?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjc3NDg5': 'Add AWS DataParellism',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjcwMTE3': 'Proper way to log things when using DDP',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjcwMTQ5': 'Datamodule without Trainer (for inference)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjczNjgz': 'Train Discriminator less than Generator',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjg1ODg0': 'Accessing the best validation loss so far in validation_epoch_ends',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjg2MjM2': 'Clarification on reload_dataloaders_every_epoch',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjg5OTA3': 'ModelCheckpoint with multiple validation dataloaders',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjgxNTg5': 'Is it possible to disable CheckpointConnector?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk1NTY1': 'set_to_none=True and accumulate_grad_batches',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk2NjUz': \"When use multiple optimizer, TypeError: unsupported operand type(s) for /: 'NoneType' and 'int'\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk2ODM0': 'How does a gpu cluster system like SLRUM use ddp training ?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk3MTA2': 'Manually call model.eval() and model.train() inside the training loop',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk3NTY3': 'Access trainer parameters from LightningModule',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk3NjM1': 'embedding manual control location CPU vs GPU',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk5MDkz': 'LightningModule.log does not work for validation metrics',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjkxODc0': 'load_from_checkpoint giving different validation results',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjkyOTQz': 'How to not create lightning_logs when using a external logger like wandb ?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjkzNzMx': 'Progress Bar Variables from Validation Step',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzA0NjU2': 'Error when training: \"closure_loss\" is NoneType',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzA1ODA1': 'How to train two optimizer with one loss?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzA1OTk3': 'How to set Checkpoints to be used in the automatically generated `version_N` directories?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzA4MTQy': 'Questions and problems with different precisions',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzAwMjAw': 'Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters. This flag results in an extra traversal of the autograd graph every iteration, which can adversely affect performance.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzAxMTc3': 'New ModelCheckpoint Code Update Breaking Model Saving Functionality : v1.2.5->v.1.2.6',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzAxNjgw': 'TPU Training: No TPU devices were found.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzAyNDAy': 'Can custom logger extending `LightningLoggerBase` be imported from separate library?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzE0MDUx': 'How to do this test in a lightning way?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzE1NTUz': 'Can a pl.LightningModule be used from native pytorch?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzEwODM0': \"cannot assign 'int' as child module 'precision' (torch.nn.Module or None expected) Error\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzExMzgw': 'why load_from_checkpoint is not defaultly inplace?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzExNDI0': 'How to use Accuracy with ignore class?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzEyMzgz': 'How to Log Metrics (eg. Validation Loss, Accuracy) To TensorBoard Hparams?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzEzODI5': 'Problems in Pruning',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzI1MzIx': 'Error with predict()',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzI2NTIw': 'Example in domain_templates: computer_vision_fine_tuning',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzI2NTUw': 'Why is my gpu-util low?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzI3MTU3': 'trainer.tune causes \"No `train_dataloader()` method defined.\" error',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzI5MjQ0': 'Re-enabling gradients in Validation loop?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzIwNDE5': 'How to disable the automatic reduce/mean while using dp?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzM0MTk2': 'How to monitor tensorboard logged scalar in modelcheckpoint?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzM0MTk4': 'Better undestanding how data is loded in datamodule setup method for multi GPU setting in NLP',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzM4MDM3': \"Control log_dict's on_epoch log name\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzM5NjY1': 'Is the use of data methods on LightningModule standard?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzMyMTA0': 'how to save the last epoch only?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzMyNzMx': \"Pytorch Lightning doesn't have CUDA?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzMzNzUx': 'Should the total epoch size be less when using multi-gpu DDP?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzMzODc0': 'How to stop wandblogger uploading the checkpoint?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ1Njc0': 'Forward run: best practices',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ2Mzk5': 'Stop training if high enough accuracy isn’t reached',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ2ODUx': 'where to add preprocessing initialization',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ3MzEy': 'How to save a copy of the running script into the log folder?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ3MzM1': 'To delete',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ3NDQz': 'what is the default scheduler?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ5MDYx': 'How to shuffle training data in every epoch?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQxMjI5': 'How to combine multiple lightning module and save hyperparameters',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQzNzY5': 'Problem in multi-node training',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU0NTUy': 'Lightning CLI, PyTorch Profiler, Improved Early Stopping',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU1MTUx': 'How to get the perfect reproducibility',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU1MjUz': '\"Error: \\'MyDataModule\\' object is not iterable\" when calling trainer.predict(model, data)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU4MTIx': 'Select GPU from cli',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU4NDI1': 'MLFlow logger step vs epoch',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU5MDcx': 'trying to train  Efficient net',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUwMzU1': 'How to implement channels last memory format callback',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUwODY2': 'Computing expensive metrics less frequently than using validation_step()',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUxNDc0': 'Can not log metric which is a tensor',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUxNDcy': 'What is the purpose of reload_dataloaders_every_epoch?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUxNTAw': 'Generating new data while training.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUxNTEz': 'on_train_epoch_end() runs int he middle of epochs',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzY0MTE1': 'How to display progress on IterableDataset?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzY1MjM1': \"How to hold 'validation_step' until training one epoch finished\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzY1OTU1': 'How to customize training loop?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzY2NjQz': 'Patience Parameter in Trainer',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzY4OTgy': 'Error while using custom DistributedSampler',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzYwODg1': \"Shouldn't predictions accumulate on CPU, not GPU?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzYzNDA2': 'Do you need to have more than one accuracy metric for different splits?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzc0OTYx': 'accumulate_grad_batches and DDP',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzc4OTQz': 'save checkpoint issue in nested trainer',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzcwNzMw': 'Intel deep learning boost compatibility',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzcwOTA4': 'what will be the closest replacement for @auto_move_data?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzcxOTM0': 'Compute Loss After Sharing Tensor Across GPUs',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzczODE0': 'Not passing optimizer to manual backward causes downstream error',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzg0MDEz': 'What are all of the differences between `.validate()` and `.test()`?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzg1NzEw': 'Checkpoints not getting saved',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzg3NTAx': 'WGANGP not working as expected',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzg5NjMy': 'Does LearningRateMonitor work with deepspeed?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzgwMjMw': 'Is it possible to call dist.all_reduce manually in train_step?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzgyODA2': 'How to put all but some vars to GPU',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzgzNzgx': 'How to design a multi-task architecture?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk1MDQ3': 'Loading checkpoint for LightningModule that defines a system',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk1MjAw': 'UserWarning: cleaning up ddp environment...',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk2MjAy': 'move_metrics_to_cpu does not work',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk4NjAw': 'Backward twice in one training_step',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk5MTEw': 'Patience reset in EarlyStopping once loss has improved',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk5NDIy': \"predict with multiple GPUs doesn't aggregate the predictions even with on_predict_end or on_predict_batch_end\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzkwNzAx': 'Custom gather method',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzkyMjc2': 'find_unused_parameters in the lightning trainer (1.3.2)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzkyNDAw': \"TypeError: setup() got an unexpected keyword argument 'stage'\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA1OTg5': 'How to tell Lightning which data to load onto the GPU (for 3rd party compatibility)',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA3MTg3': 'Is there an all_gather before training_step_end when using DDP?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA3NzMw': 'Global parameters yaml file',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA3ODk2': \"TypeError: __init__() got an unexpected keyword argument 'row_log_interval'\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA4NjYw': 'ddp replicates whole script',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA5MDY5': 'How to remove version number when saving model checkpoint?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA5MzIw': 'code hangs when creating a dir',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDAxNjI5': 'Processing in predict_step() requires access to a DataModule attribute',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDAyOTgx': 'How to customize the version or name of log file',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDE5NjM3': 'How to use predict function to return predictions',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDE5OTIz': 'I want to apply custom learning rate scheduler.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI1NDc2': 'Best way to bypass requirement of a DataLoader when inputs are generated stochastically on-demand',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI1OTU0': 'forward() takes 1 positional argument but 2 were given',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI2NjI5': 'NN output within a numba jitted function',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI2NzQy': 'How to test in low version',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI3OTg1': 'how to load dataset only once on the same machine?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI3OTk4': 'Training slows down significantly for small dataset sizes',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI4MDcz': 'Accessing DataModule or DataLoaders within model hooks',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDIwNDgw': 'share steps among test and validation steps',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDIxMTQ4': 'load checkpoint model error',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDIxNDU1': 'Can I set the epoch/step initial value to 1?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDIyMTky': 'Selecting one gpu from cli',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDMxMzAw': \"What's the difference between on_fit_start and on_train_start in LightningModule?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDMxNDM4': 'RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDMzMTY2': 'Can training step return dictionary?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDMzNzM4': 'how to use pl to process tfrecords data?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ1MDEz': 'GPU usage does not remain high for lightweight models when loaded CIFAR-10 as a custom dataset',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ1Mzg2': 'How to accumulate grad batches for GANs in `pytorch_lightning >= 1.3.3`?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ2Mjkz': 'In load_from_checkpoint, \"TypeError: __init__ () missing 1 required positional argument:\\'cfg\\'\"',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ2Njgz': 'Help understanding data module error',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ3MDYz': 'ModelCheckpoint creating unexpected subfolders',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ5MjE2': 'ImportError: _XLAC.cpython: undefined symbol',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQwNDk0': 'CNN dimension error',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQwNTU3': 'Getting Validation Accuracy with validation_epoch_end',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQxODI1': 'how to plot confusion_matrix with lightning tensorboard?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQzODk2': 'Earlystopping callback metrics in different devices with single gpu',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDU0NTI5': 'I cannot assign the GPU index by ddp or dp backends..',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDU2MjU5': 'how to put ```trainer.fit()``` in for loop?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDU5MDQx': 'Interpret the output (logs) during training',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDU5Mzc4': 'Multiple dataloaders in training',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDU5NjMw': 'Save model into separate files',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDUxOTU2': 'Getting error after completion of 1st epoch',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDY2OTI3': 'When does loss.backward() get called when accumulating gradients?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDY5NjEy': 'DDP with shared file system',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDYzNjQy': 'Loss Module with inner Network',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDc2ODEz': 'How to sync buffers in multi-gpu training',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDc3MDE4': 'RuntimeError: Trying to backward through the graph a second time',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDc5ODU4': 'Multiple models, one dataloader?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDcwNjIz': 'Data augmentation and reload_dataloaders_every_epoch',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDcwNzE3': 'How to implement a deep ensemble',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDcxODUx': '`init_process_group` not called when training on multiple-GPUs',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDczMDQy': 'RuntimeError: unable to open shared memory object </torch_91130_1372465664> in read-write mode',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDg2Mzk2': 'Is it possible to access the global rank and world size outside of `LightningModule`?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDg4MDgy': 'How are callback calls handled in multi-gpu mode?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDg4Mjgx': 'What is the best practice to share a massive CPU tensor over multiple processes in pytorch-lightning DDP mode (read-only + single machine)?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDg5NzQ0': 'Emulating multiple devices with a single GPU',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDgwNTAw': 'How to reinit wanbd in a for loop with PL Trainer',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDkwMDQ4': 'Optimizers for nested modules',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDkwOTk3': 'Using callbacks for datamodule setup preprocessing logic?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTA0NjA3': 'training_epoch_end only returning the last train batch',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTA2MDQ1': 'How to handle pretrained models without training them',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTA3ODQ0': 'Why Trainer resume_from_checkpoint only resume when fit.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTA5MDI4': \"What's the difference between `on_step` and `on_epoch` of `pl_module.log`\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNTAzNDg3': 'Why is the default implementation for train_dataloader in DataHooks a warning?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTAzNTI2': 'on_post_move_to_device is leaky',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTE0MTA0': 'Changing the computed gradients before backprop',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTE2MjI1': 'Create 2 learning rate schedulers for 1 optimizer',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTE3NDMw': 'Saving and loading HF transformer model fine tuned with PL?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTE3NTEx': 'Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTE4OTIx': \"AttributeError: Can't get attribute 'DataReader' on <module '__main__' (built-in)>\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNTExMzgz': 'How to ensure objects saved as model attributes are saved in the checkpoint file?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTEzMDQ5': 'Train on cpu but gives error \"You have asked for native AMP on CPU, but AMP is only available on GPU\"',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTI3MDA1': 'Problem with dictionary of DataLoaders in training_step.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTI4Mzcw': 'Installation with conda?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTIxMjUy': 'How save deepspeed stage 3 model with pickle or torch',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTIxNDY5': \"TypeError: 'Subset' object is not callable\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNTIyMjk1': 'Trainer flags: amp_level vs. precision',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTM0Nzc4': 'Why would GPU memory always surge after training and cause CUDA memory error?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTM1Mzkx': 'Validation crashes when setting seed or with val_num_workers > 0 with CUDA initialization error',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTM2MDY5': 'Calculation of inference time',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTM4ODc2': 'Hyperparameter Tuning in Lightning CLI',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTMxMDk5': 'Get best metrics after call to trainer.fit without evaluating best model on val set again',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQ0NDA4': '__about__.py \"version\" field automatically updated (unwanted behavior)',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQ0NzQ3': 'How to set experiment name such that it can be some unique name instead of version_0, ... etc.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQ4NzY1': 'Loading from checkpoints re-downloads pre-trained BERT model',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQwNzY5': 'Run specific code only once (which generates randomized values) before starting DDP',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQxNDIw': 'DeepSpeedPlugin with activation checkpoint fails',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQxODQw': 'NeptunePossibleLegacyUsageException  when logging accuracy',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQxOTY5': 'Cross Entropy Loss and loss of PyTorch Lightning does not matches',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQzNzYy': 'Can we get the value of `self.log()`?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTU1NjYz': 'Pytorch-lightning CPU-only installation',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTU2MTc0': 'return_result role in training_batch_loop.py',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTU3MTEw': 'Keep only the best and the latest artifacts in wandb logger',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTU4NjIx': 'How to redefine optimizer for pl.LightningModule?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTUxNDE3': 'Setting random seed before training',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTY2Nzgy': 'Lightning CLI is incompatible with models defined by data',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTY4NDY4': 'ValueError: `Dataloader` returned 0 length. Please make sure that it returns at least 1 batch',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTYyMjU2': 'How to collect batched predictions?'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ir_corpus = {\n",
        "    row['id_y']: row['context'] for i, row in eval_df.iterrows()\n",
        "}\n",
        "ir_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWKRs5fWyQqA",
        "outputId": "e0277098-df23-44f4-d17e-c700110bf1ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'D_kwDOCqWgoM4AN-cLcon': \"I want to use ModelCheckpoint to save mode while training, however, nothing has been saved. The following is my code. I don't know what leads to this problem, any suggestions?\",\n",
              " 'D_kwDOCqWgoM4AN-x1con': 'following is the error: NotImplementedError: `val_dataloader` must be implemented to be used with the Lightning Trainer\\n\\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\\n---------------------------------------------------------------------------\\nNotImplementedError                       Traceback (most recent call last)\\n<ipython-input-11-263e8be26564> in <module>()\\n      3     tft,\\n      4     train_dataloader=train_dataloader,\\n----> 5     val_dataloaders=val_dataloader,\\n      6 )\\n\\n11 frames\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/hooks.py in val_dataloader(self)\\n    590             will have an argument ``dataloader_idx`` which matches the order here.\\n    591         \"\"\"\\n--> 592         raise NotImplementedError(\"`val_dataloader` must be implemented to be used with the Lightning Trainer\")\\n    593 \\n    594     def predict_dataloader(self) -> EVAL_DATALOADERS:\\n\\nNotImplementedError: `val_dataloader` must be implemented to be used with the Lightning Trainer\\ntrainer.fit(\\n    tft,\\n    train_dataloader=train_dataloader,\\n    val_dataloaders=val_dataloader,\\n)',\n",
              " 'D_kwDOCqWgoM4AN16Bcon': \"For example, part of my model's parameters are frozen, no need to train, no need to save\",\n",
              " 'D_kwDOCqWgoM4AN2Gdcon': 'I am currently try this colab notebook https://colab.research.google.com/github/PytorchLightning/lightning-tutorials/blob/publication/.notebooks/lightning_examples/mnist-tpu-training.ipynb#scrollTo=2772a2e1 provided by PL teams to get some experience with TPU training. But when I try to execute the third cell, there is some Import error with the _XLAC module.',\n",
              " 'D_kwDOCqWgoM4AN49xcon': 'I am using Jupyter Lab to run. It has pre-installed tf2.3_py3.6 kernel installed in it. It has 2 GPUS in it.\\nPyTorch Lightning Version (e.g., 1.3.0): \\'1.4.6\\'\\nPyTorch Version (e.g., 1.8): \\'1.6.0+cu101\\'\\nPython version: 3.6\\nOS (e.g., Linux): system=\\'Linux\\'\\nCUDA/cuDNN version: 11.2\\nHow you installed PyTorch (conda, pip, source): pip\\n\\nHere is the screenshot of my model and it got interrupted due to connection issue.\\nI am saving the best model in checkpoint.\\nI am doing multi-label classification using Hugging face model. After training the model I want to export the model using ONNX format.\\nHere is the DataModule Class\\n\\nN_EPOCHS = 30\\nBATCH_SIZE = 10\\n\\nclass  SRDataModule(pl.LightningDataModule):\\n    \\n    def __init__(self, X_train,y_train, X_test,y_test, tokenizer, batch_size=8, max_token_len=512):\\n        super().__init__()\\n        self.batch_size = batch_size\\n        self.train_df = X_train\\n        self.test_df = X_test\\n        self.train_lab = y_train\\n        self.test_lab = y_test\\n        self.tokenizer = tokenizer\\n        self.max_token_len = max_token_len\\n\\n    def setup(self, stage=None):\\n        self.train_dataset = SRDataset(\\n          self.train_df,\\n          self.train_lab,\\n          self.tokenizer,\\n          self.max_token_len\\n        )\\n\\n        self.test_dataset = SRDataset(\\n          self.test_df,\\n          self.test_lab,\\n          self.tokenizer,\\n          self.max_token_len\\n    )\\n\\n    def train_dataloader(self):\\n        return DataLoader(\\n          self.train_dataset,\\n          batch_size=self.batch_size,\\n          shuffle=True,\\n          num_workers=10\\n        )\\n\\n    def val_dataloader(self):\\n        return DataLoader(\\n          self.test_dataset,\\n          batch_size=self.batch_size,\\n          num_workers=10\\n        )\\n\\n    def test_dataloader(self):\\n        return DataLoader(\\n          self.test_dataset,\\n          batch_size=self.batch_size,\\n          num_workers=10\\n        )\\n\\n\\nHere is the model class:\\nclass SRTagger(pl.LightningModule):\\n\\n  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\\n    super().__init__()\\n    self.save_hyperparameters()\\n    self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\\n    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\\n    self.n_training_steps = n_training_steps\\n    self.n_warmup_steps = n_warmup_steps\\n    self.criterion = nn.BCELoss()\\n\\n  def forward(self, input_ids, attention_mask, labels=None):\\n    output = self.bert(input_ids, attention_mask=attention_mask)\\n    output = self.classifier(output.pooler_output)\\n    output = torch.sigmoid(output)    \\n    loss = 0\\n    if labels is not None:\\n        loss = self.criterion(output, labels)\\n    return loss, output\\n\\n  def training_step(self, batch, batch_idx):\\n    input_ids = batch[\"input_ids\"]\\n    attention_mask = batch[\"attention_mask\"]\\n    labels = batch[\"labels\"]\\n    loss, outputs = self(input_ids, attention_mask, labels)\\n    \\n    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\\n    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\\n\\n  def validation_step(self, batch, batch_idx):\\n    input_ids = batch[\"input_ids\"]\\n    attention_mask = batch[\"attention_mask\"]\\n    labels = batch[\"labels\"]\\n    loss, outputs = self(input_ids, attention_mask, labels)\\n    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\\n    return loss\\n\\n  def test_step(self, batch, batch_idx):\\n    input_ids = batch[\"input_ids\"]\\n    attention_mask = batch[\"attention_mask\"]\\n    labels = batch[\"labels\"]\\n    loss, outputs = self(input_ids, attention_mask, labels)\\n    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\\n    return loss\\n\\n  def training_epoch_end(self, outputs):\\n    \\n    labels = []\\n    predictions = []\\n    for output in outputs:\\n      for out_labels in output[\"labels\"].detach().cpu():\\n        labels.append(out_labels)\\n      for out_predictions in output[\"predictions\"].detach().cpu():\\n        predictions.append(out_predictions)\\n\\n    labels = torch.stack(labels).int()\\n    predictions = torch.stack(predictions)\\n\\n    for i, name in enumerate(LABEL_COLUMNS):\\n      class_roc_auc = auroc(predictions[:, i], labels[:, i])\\n      self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\\n\\n\\n  def configure_optimizers(self):\\n\\n    optimizer = optim.RAdam(self.parameters(), lr=2e-4)\\n\\n    scheduler = get_linear_schedule_with_warmup(\\n      optimizer,\\n      num_warmup_steps=self.n_warmup_steps,\\n      num_training_steps=self.n_training_steps\\n    )\\n\\n    return dict(\\n      optimizer=optimizer,\\n      lr_scheduler=dict(\\n        scheduler=scheduler,\\n        interval=\\'step\\'\\n      )\\n    )\\n\\nSample Data\\nsample_batch = next(iter(DataLoader(train_dataset, batch_size=10, num_workers=2)))\\nsample_batch[\"input_ids\"].shape, sample_batch[\"attention_mask\"].shape\\n\\n(torch.Size([10, 512]), torch.Size([10, 512]))\\n\\nsample_batch.keys()\\ndict_keys([\\'text_data\\', \\'input_ids\\', \\'attention_mask\\', \\'labels\\'])\\n\\nModel\\nmodel = SRTagger(\\n  n_classes=100,\\n  n_warmup_steps=warmup_steps,\\n  n_training_steps=total_training_steps \\n)\\n\\n\\nONNX code\\n# # Export the model\\ntorch.onnx.export(model,                     # model being run\\n                  ##since model is in the cuda mode, input also need to be\\n                  (sample_batch[\"input_ids\"],sample_batch[\"attention_mask\"]),              # model input (or a tuple for multiple inputs)\\n                  \"model_torch_export.onnx\", # where to save the model (can be a file or file-like object)\\n                  export_params=True,        # store the trained parameter weights inside the model file\\n                  opset_version=10,          # the ONNX version to export the model to\\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\\n                  input_names = [\\'input\\'],   # the model\\'s input names\\n                  output_names = [\\'output\\'], # the model\\'s output names\\n                  dynamic_axes={\\'input\\' : {0 : \\'batch_size\\'},    # variable lenght axes\\n                                \\'output\\' : {0 : \\'batch_size\\'}})\\n\\nError\\nRuntimeError: output 1 (0\\n[ CPULongType{} ]) of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.',\n",
              " 'D_kwDOCqWgoM4AN4Ofcon': 'Hi,\\nI\\'m applying Pytorch Lightning module to VAE and our model We first train VAE and give the best checkpoint of pretrained VAE as the initial weight of our model.\\n# STEP 1. Train VAE\\nvae = VAE(...)\\n\\ntrainer = Trainer(...)\\ntrainer.fit(vae)\\n\\n# STEP 2.\\nvae = VAE.load_from_checkpoint(...)\\n\\nclass Model(LightningModule):\\n    def __init__(self, encoder, decoder, learning_rate):\\n       super().__init__()\\n        self.encoder = encoder\\n        self.decoder = decoder\\n        \\n        self.save_hyperparameters(\"learning_rate\")\\n        ...\\n\\nencoder = copy.deepcopy(vae.encoder)\\ndecoder = copy.deepcopy(vae.decoder)\\n\\nmodel = Model(\\n    encoder=encoder,\\n    decoder=decoder,\\n    ...\\n)\\ntrainer.fit(model)\\nThe problem is when I load the model after train ends. Since the torch modules are contained in input arguments of Model, the common approach\\nmodel = Model.load_from_checkpoint(...) \\nyields following error messages. TypeError: __init__() missing 2 required positional arguments: \\'encoder\\' and \\'decoder\\' \\nSo, what is the best practice for saving and loading the model which uses the pre-trained model?',\n",
              " 'D_kwDOCqWgoM4AN4Z4con': \"My LightningModule.training_step includes calls to self.log and finally returns the loss value. What is the best way to run training_step outside of a Trainer context, for debugging purposes (such as manual gradient inspection, etc)? Without the instrumentation by Trainer, logger is not defined and self.log calls cause an exception. I was trying to mock the logger to turn them to no-ops, but I wasn't successful.\",\n",
              " 'D_kwDOCqWgoM4AN5Lvcon': 'Anyone help me where did these funcs been called in the core parts? I expect it to be called in the loop instance of trainer however not. Quite confused about this.',\n",
              " 'D_kwDOCqWgoM4AN637con': \"I thought the number of on_train_epoch_start and on_train_epoch_end should be equal to the number of epochs. But when I passed the following callback function:\\nclass MyPrintingCallback(Callback):\\n\\n    def on_train_epoch_start(self, trainer, pl_module):\\n        print('Train epoch start for epoch: ', pl_module.current_epoch)\\n        \\n    def on_train_epoch_end(self, trainer, pl_module):\\n        # will run only once in the beginning\\n        print('Train step end for epoch: ', pl_module.current_epoch)\\n\\non_train_epoch_end is only called in the 0th epoch:\\n\\nTraining: -1it [00:00, ?it/s]Train epoch start for epoch:  0\\nEpoch 0: : 4875it [00:33, 143.57it/s, Train step end for epoch:  0\\nTrain epoch start for epoch:  1\\nEpoch 1: : 0it [00:00, 7096.96it/s, loss=2.13e+09, v_num=37] Train epoch start for epoch:  2\\nEpoch 2: : 0it [00:00, 11335.96it/s, loss=2.13e+09, v_num=37]Train epoch start for epoch:  3\\nEpoch 3: : 0it [00:00, 12052.60it/s, loss=2.13e+09, v_num=37]Train epoch start for epoch:  4\\nEpoch 4: : 0it [00:00, 4301.85it/s, loss=2.13e+09, v_num=37]\\n\\nAny idea why this is happening?\",\n",
              " 'D_kwDOCqWgoM4AN6JXcon': 'Hello I am trying to train a neural network using pytorch lightning. I have run into an issue with the trainer when I try to run the program. I am getting the following issue:\\nGPU available: False, used: False\\nTPU available: False, using: 0 TPU cores\\nTraceback (most recent call last):\\n\\n  File \"/home/PytorchLightningGRUtraining.py\", line 179, in <module>\\n    main(args)\\n\\n  File \"/home/PytorchLightningGRUtraining.py\", line 171, in main\\n    trainer.fit(model, dm.train_dataloader(), dm.val_dataloader())\\n\\n  File \"/home/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 449, in fit\\n    self.train_loop.setup_fit(model, train_dataloader, val_dataloaders, datamodule)\\n\\n  File \"/home/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 123, in setup_fit\\n    self.trainer.callback_connector.attach_model_logging_functions(model)\\n\\n  File \"/home/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py\", line 123, in attach_model_logging_functions\\n    callback.log = model.log\\n\\n  File \"/home/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in __getattr__\\n    raise AttributeError(\"\\'{}\\' object has no attribute \\'{}\\'\".format(\\n\\nAttributeError: \\'NeuralNetwork\\' object has no attribute \\'log\\'\\n\\nI defined the logger for the trainer after I created an instance of the model class. What am I doing wrong?',\n",
              " 'D_kwDOCqWgoM4AN6mzcon': 'Is there a way to save the model with the best validation accuracy when using early stopping? I believe right now, the model weights are the weights from the latest snapshot; but i am looking for a way to access the model with the best performance on validation.',\n",
              " 'D_kwDOCqWgoM4AN6vAcon': 'When I load the saved Lightning model and test on test data, I saw different epochs are running starting from 0, suggesting the model is being trained once again. Please suggest how to get the test score on the test data.\\nLet me illustrate in detail. I have a  Lightning model like the following\\nclass  MyLightningModule(pl.LightningModule):\\n    def __init__(self,loss_fn, lr=1e-1):\\n        super().__init__()\\n        self.loss_fn = loss_fn\\n        self.lr = lr\\n        self.save_hyperparameters(\"lr\")\\n    \\n    @staticmethod\\n    def add_model_specific_args(parent_parser):\\n        parser = parent_parser.add_argument_group(\"SemanticPO\")\\n        parser.add_argument(\"--lr\", type=float, default=0.1)\\n        return parent_parser\\n    def forward(self,x):\\n        return self.layer1(x)   \\n    def training_step(self, batch, batch_idx):\\n        x,y,sol = batch\\n        y_hat =  self(x)\\n        loss = self.loss_fn(y,y_hat)\\n        self.log(\"train_loss\",loss, prog_bar=True, on_step=True, on_epoch=True )\\n        return loss\\n    def validation_step(self, batch, batch_idx):\\n        x,y,sol = batch\\n        y_hat =  self(x)\\n        val_loss=  self.loss_fn(y,y_hatl)\\n        self.log(\"val_loss\", val_loss, prog_bar=True, on_step=True, on_epoch=True, )\\n        return val_loss\\n    def test_step(self, batch, batch_idx):\\n        # Here we just reuse the validation_step for testing\\n        return self.validation_step(batch, batch_idx)\\n    def configure_optimizers(self):\\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\\n        return optimizer \\n\\nI am using three separate data for training. testing and validation.\\ntrain_dl = DataLoader(train_df, batch_size= 64)\\nvalid_dl = data_utils.DataLoader(valid_df, batch_size= 64)\\ntest_dl = data_utils.DataLoader(test_df, batch_size= 64)\\n\\nFirst, I train and validate in the following manner\\nmodel = MyLightningModule(loss_fn= Myloss)\\ncheckpoint_callback = ModelCheckpoint(\\n    monitor=\"val_loss\",\\n    dirpath=\"ckpt_dir/\",\\n    filename=\"mymodel-{epoch:02d}-{val_loss:.2f}\",\\n    save_top_k=2, mode=\"min\",\\n)\\ntrainer = pl.Trainer(max_epochs= 25, callbacks=[checkpoint_callback])\\ntrainer.fit(model, train_dl, valid_dl)\\n\\nThe training is complete and the checkpoints are saved as expected.\\nThen I want to restore the saved model and test it on test data. This is done on a separate file like this\\nmodel = MyLightningModule.load_from_checkpoint(\\'ckpt_dir/mymodel-epoch=15-val_loss=44.31.ckpt\\')\\ntest_dl = data_utils.DataLoader(test_df, batch_size= 64)\\ntrainer = pl.Trainer()\\nresult = trainer.test(model,\\nckpt_path=\\'ckpt_dir/mymodel-epoch=15-val_loss=44.31.ckpt\\',dataloaders=test_dl)\\nprint(result)\\n\\nI expect it to just test on test data and display the result. But I see the model is running once again for a number of epochs.\\nMay be I am doing something wrong. Please suggest the best way to obtain the test result after restoring a saved model.\\n\\npytorch-lightning==1.4.9',\n",
              " 'D_kwDOCqWgoM4AN7kbcon': 'Throws this Exception. IDK what to do. Can anyone help me?\\nCode:\\nimport enum\\nimport os\\nimport torch\\nimport torchmetrics\\n\\nfrom pytorch_lightning import Trainer\\nfrom pytorch_lightning.callbacks import EarlyStopping\\n\\n\\nfrom torch.utils.data import Dataset, DataLoader\\nimport pytorch_lightning as pl\\n\\nfrom dataclasses import dataclass, asdict\\nimport pytorch_lightning_spells as pls\\nimport typer\\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\\n\\nfrom Cord19Dataset import Cord19Dataset, DATASET_DIR, Parts\\nfrom t2t import BaseConfig, T5BaseModel, masked_cross_entropy_loss\\nfrom pathlib import Path\\nimport pandas as pd\\n\\n\\nclass Corpus(enum.Enum):\\n    CORD19 = \"cord19\"\\n\\nCACHE_DIR = \\n\\n@dataclass\\nclass Config(BaseConfig):\\n    dataset: Corpus = Corpus.CORD19\\n\\nfrom pathlib import Path\\nimport pandas as pd\\n\\n\\n\\n\\nclass T5Model(T5BaseModel):\\n    def __init__(self, config: Config, **kwargs):\\n        model = T5ForConditionalGeneration.from_pretrained(config.base_t5_model)\\n        tokenizer = T5Tokenizer.from_pretrained(config.base_t5_model)\\n        super().__init__(config, model, tokenizer)\\n        self.config = config\\n        # log the config values\\n        self.save_hyperparameters(asdict(config))\\n        self.train_dataset = Cord19Dataset(Parts.TRAIN)\\n        print(\"Train dataset: \", len(self.train_dataset))\\n        self.valid_dataset = Cord19Dataset(Parts.VALID)\\n        print(\"Valid dataset: \", len(self.valid_dataset))\\n        \\nprint(Cord19Dataset(Parts.TRAIN))\\n\\ndef main(\\n        t5_model: str = \"t5-small\", lr: float = 1e-4,  # 3^4\\n        epochs: int = 5, fp16: bool = False,\\n        #dataset: Corpus = Corpus.CORD19, batch_size: int = 16,\\n        dataset: Corpus = Corpus.CORD19, batch_size: int = 8,\\n        max_len: int = 64, grad_accu: int = 1,\\n        num_gpus: int = 1\\n      \\n):\\n    pl.seed_everything(int(os.environ.get(\"SEED\", 738)))\\n    config = Config(\\n        base_t5_model=t5_model,\\n        learning_rate=lr,\\n        epochs=epochs,\\n        dataset=dataset,\\n        max_len=max_len,\\n        grad_accu=grad_accu,\\n        batch_size=batch_size,\\n        fp16=fp16,\\n        weight_decay=0,\\n        num_gpus=num_gpus,\\n        loss_fn=masked_cross_entropy_loss\\n    )\\n\\n    pl_module = T5Model(config)\\n\\n    callbacks = [\\n        pl.callbacks.ModelCheckpoint(\\n            dirpath=str(DATASET_DIR / \"model_checkpoints\"),\\n            monitor=\\'val_loss\\',\\n            mode=\"min\",\\n            filename=\\'{step:06d}-{val_loss:.4f}\\',\\n            save_top_k=1,\\n            save_last=False\\n        ),\\n        pl.callbacks.LearningRateMonitor(logging_interval=\\'step\\'),\\n    ]\\n    trainer = pl.Trainer(\\n        accelerator=\\'dp\\' if num_gpus > 1 else None,\\n        # amp_backend=\"apex\", amp_level=\\'O2\\',\\n        precision=16 if config.fp16 else 32,\\n        gpus=config.num_gpus,\\n        val_check_interval=0.25,\\n        gradient_clip_val=10,\\n        max_epochs=epochs,\\n        # max_steps=steps,\\n        callbacks=callbacks,\\n        accumulate_grad_batches=grad_accu,\\n        # auto_scale_batch_size=\\'power\\' if batch_size is None else None,\\n        logger=[\\n            pl.loggers.TensorBoardLogger(str(DATASET_DIR / \"tb_logs\"), name=\"\"),\\n            pls.loggers.ScreenLogger(),\\n            # pl.loggers.WandbLogger(project=\"t5-paraphrase\")\\n        ],\\n        log_every_n_steps=100\\n    )\\n\\n    trainer.fit(pl_module)\\n\\n#     pl_module.model.save_pretrained(CACHE_DIR / f\"{config.base_t5_model}_last\")\\n#     pl_module.tokenizer.save_pretrained(CACHE_DIR / f\"{config.base_t5_model}_last\")\\n#     print(\"Last model saved\")\\n\\n    assert isinstance(callbacks[0], pl.callbacks.ModelCheckpoint)\\n    print(callbacks[0].best_model_path)\\n    pl_module = T5Model.load_from_checkpoint(\\n        callbacks[0].best_model_path,\\n        config=config\\n    )\\n    pl_module.model.save_pretrained(DATASET_DIR / f\"{config.base_t5_model}_best\")\\n    pl_module.tokenizer.save_pretrained(DATASET_DIR / f\"{config.base_t5_model}_best\")\\n    print(\"Best model saved\")\\n    pl_module.model.save_pretrained(path+t5+\"model/\")\\n    pl_module.tokenizer.save_pretrained(path+t5+\"tokenizer/\")\\n\\n\\nThis gives me:\\nGlobal seed set to 738\\nSome weights of the model checkpoint at t5-small were not used when initializing T5ForConditionalGeneration: [\\'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\\']\\n- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\\n- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\\n  stream(template_mgs % msg_args)\\n100%|████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 175.62it/s]\\n100%|█████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 24.88it/s]\\nLoading train dataset into memory...\\nUsed File: [\\'dataset_1.jbl\\', \\'dataset_2.jbl\\', \\'dataset_3.jbl\\', \\'dataset_4.jbl\\', \\'dataset_5.jbl\\', \\'dataset_6.jbl\\']\\n\\n/opt/anaconda/envs/venv/lib/python3.7/[...]Pretraining-T5-PyTorch-Lightning/model_checkpoints exists and is not empty.\\n  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\\n\\nGPU available: True, used: True\\nTPU available: False, using: 0 TPU cores\\nIPU available: False, using: 0 IPUs\\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\\nValid dataset:  124\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\\n  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\\n\\n  | Name  | Type                       | Params\\n-----------------------------------------------------\\n0 | model | T5ForConditionalGeneration | 60.5 M\\n-----------------------------------------------------\\n60.5 M    Trainable params\\n0         Non-trainable params\\n60.5 M    Total params\\n242.026   Total estimated model params size (MB)\\nSteps per epochs: 46\\nValidation sanity check: 0%\\n0/2 [00:00<?, ?it/s]\\n---------------------------------------------------------------------------\\nMisconfigurationException                 Traceback (most recent call last)\\n<ipython-input-10-b1e689bd5891> in <module>\\n      2 # if __name__ == \"__main__\":\\n      3 #     typer.run(main)\\n----> 4 main()\\n\\n<ipython-input-9-576858e8663d> in main(t5_model, lr, epochs, fp16, dataset, batch_size, max_len, grad_accu, num_gpus)\\n     56     )\\n     57 \\n---> 58     trainer.fit(pl_module)\\n     59 \\n     60 #     pl_module.model.save_pretrained(CACHE_DIR / f\"{config.base_t5_model}_last\")\\n\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader)\\n    551         self.checkpoint_connector.resume_start()\\n    552 \\n--> 553         self._run(model)\\n    554 \\n    555         assert self.state.stopped\\n\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run(self, model)\\n    916 \\n    917         # dispatch `start_training` or `start_evaluating` or `start_predicting`\\n--> 918         self._dispatch()\\n    919 \\n    920         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\\n\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _dispatch(self)\\n    984             self.accelerator.start_predicting(self)\\n    985         else:\\n--> 986             self.accelerator.start_training(self)\\n    987 \\n    988     def run_stage(self):\\n\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\\n     90 \\n     91     def start_training(self, trainer: \"pl.Trainer\") -> None:\\n---> 92         self.training_type_plugin.start_training(trainer)\\n     93 \\n     94     def start_evaluating(self, trainer: \"pl.Trainer\") -> None:\\n\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_training(self, trainer)\\n    159     def start_training(self, trainer: \"pl.Trainer\") -> None:\\n    160         # double dispatch to initiate the training loop\\n--> 161         self._results = trainer.run_stage()\\n    162 \\n    163     def start_evaluating(self, trainer: \"pl.Trainer\") -> None:\\n\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_stage(self)\\n    994         if self.predicting:\\n    995             return self._run_predict()\\n--> 996         return self._run_train()\\n    997 \\n    998     def _pre_training_routine(self):\\n\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run_train(self)\\n   1029             self.progress_bar_callback.disable()\\n   1030 \\n-> 1031         self._run_sanity_check(self.lightning_module)\\n   1032 \\n   1033         # enable train mode\\n\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run_sanity_check(self, ref_model)\\n   1113             # run eval step\\n   1114             with torch.no_grad():\\n-> 1115                 self._evaluation_loop.run()\\n   1116 \\n   1117             self.on_sanity_check_end()\\n\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/loops/base.py in run(self, *args, **kwargs)\\n    109             try:\\n    110                 self.on_advance_start(*args, **kwargs)\\n--> 111                 self.advance(*args, **kwargs)\\n    112                 self.on_advance_end()\\n    113                 self.iteration_count += 1\\n\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py in advance(self, *args, **kwargs)\\n    109 \\n    110         dl_outputs = self.epoch_loop.run(\\n--> 111             dataloader_iter, self.current_dataloader_idx, dl_max_batches, self.num_dataloaders\\n    112         )\\n    113 \\n\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/loops/base.py in run(self, *args, **kwargs)\\n    109             try:\\n    110                 self.on_advance_start(*args, **kwargs)\\n--> 111                 self.advance(*args, **kwargs)\\n    112                 self.on_advance_end()\\n    113                 self.iteration_count += 1\\n\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py in advance(self, dataloader_iter, dataloader_idx, dl_max_batches, num_dataloaders)\\n    109         with self.trainer.profiler.profile(\"evaluation_step_and_end\"):\\n    110             output = self.evaluation_step(batch, batch_idx, dataloader_idx)\\n--> 111             output = self.evaluation_step_end(output)\\n    112 \\n    113         self.batch_progress.increment_processed()\\n\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py in evaluation_step_end(self, *args, **kwargs)\\n    159         \"\"\"Calls the `{validation/test}_step_end` hook\"\"\"\\n    160         hook_name = \"test_step_end\" if self.trainer.testing else \"validation_step_end\"\\n--> 161         output = self.trainer.call_hook(hook_name, *args, **kwargs)\\n    162         return output\\n    163 \\n\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in call_hook(self, hook_name, *args, **kwargs)\\n   1222             if is_overridden(hook_name, model_ref):\\n   1223                 hook_fx = getattr(model_ref, hook_name)\\n-> 1224                 output = hook_fx(*args, **kwargs)\\n   1225 \\n   1226             # call the accelerator hook\\n\\n/home/[...]Pretraining-T5-PyTorch-Lightning/t2t/__init__.py in validation_step_end(self, outputs)\\n    227                 outputs[\\'target\\'][\\'ids\\'].view(-1).cpu()\\n    228             )\\n--> 229             self.log(\"val_\" + name, metric)\\n    230 \\n    231     def _should_log(self, flag):\\n\\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py in log(self, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, tbptt_reduce_fx, tbptt_pad_token, enable_graph, sync_dist, sync_dist_op, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\\n    432                 if not self._metric_attributes:\\n    433                     raise MisconfigurationException(\\n--> 434                         \"Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.\"\\n    435                         \" You can fix this by setting an attribute for the metric in your `LightningModule`.\"\\n    436                     )\\n\\nMisconfigurationException: Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged. You can fix this by setting an attribute for the metric in your `LightningModule`.',\n",
              " 'D_kwDOCqWgoM4AN88Ocon': 'In my experiment, I have a loss function, which is not defined by an expression. But I have a formulation of the gradient (formally a subgradient) so I have to pass the gradient manually. In pytorch, I implement it in the following way and it is working fine.\\nself.optimizer.zero_grad()\\ny_hat = self.model(x_train)\\ngrad =  compute_grad(y_hat, y)\\ny_hat.backward(gradient=grad)\\nself.optimizer.step()\\nWould the following be a correct implementation in lightning?\\ndef training_step(self, batch, batch_idx):\\n        opt = self.optimizers()\\n        x,y = batch\\n        y_hat =  self(x)\\n        grad =  compute_grad(y_hat, y)\\n        opt.zero_grad()\\n        y_hat.backward(gradient= grad)\\n        opt.step()',\n",
              " 'D_kwDOCqWgoM4AN8UHcon': \"I would like to save one single inference example per validation stage.  To this end I came up with:\\nclass Model(pl.LightningModule):\\n\\n    ...\\n\\n    def validation_epoch_end(self, validation_step_outputs):\\n        # self.trainer.log_dir is not set during fast_dev_test\\n        if self.trainer.log_dir is not None:\\n            x, y = (d.unsqueeze(0) for d in self.trainer.datamodule.valid_set.dataset_pair())\\n            y_hat = self.model(x)\\n            x, y, y_hat = (d.squeeze().cpu().numpy() for d in (x, y, y_hat))\\n\\n            save_dir = Path(self.trainer.log_dir) / 'wavs'\\n            save_dir.mkdir(exist_ok=True)\\n\\n            soundfile.write(save_dir/f'{self.global_step:06}_input.wav',\\n                    x, 44100, subtype='PCM_24')\\n            soundfile.write(save_dir/f'{self.global_step:06}_output.wav',\\n                    y_hat, 44100, subtype='PCM_24')\\n            soundfile.write(save_dir/f'{self.global_step:06}_target.wav',\\n                    y, 44100, subtype='PCM_24')\\n\\nThis works OK when running on a single device (CPU) but when running on GPUs on a slurm cluster I get:\\nRuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\\n\\nI guess because the data has not been sent to the same device as the model.\\nThis seems like a problem that must have been solved before.  Can anyone suggest a typical pytorch-lightning way of doing this?\\nThanks for your input\\nLeo\",\n",
              " 'D_kwDOCqWgoM4AN_7Dcon': \"Hi! I'm working on a SLURM cluster with preemption, so I'm really excited to see the support of Fault-tolerant Training in 1.5.0. However, when I upgrade package and try PL_FAULT_TOLERANT_TRAINING=1 python train.py xxx in the cluster, it doesn't seem to work.\\nI look into the code of Trainer, it seems that the code responsible for fault-tolerant is here. I assume preemption is a BaseException so the code will go to here and finally here so that we save a checkpoint?\\nHowever, when set some print in the code, when I use ctrl+C to interrupt code, it indeed goes to this KeyBoardInterrupt. But if I use scontrol requeue to simulate a preemption, the code didn't got to BaseException. And that's why it didn't save a checkpoint for Fault-tolerant Training.\\nIs there anything wrong with my code? I assume interruptions like scancel requeue are considered in this case. Can anyone help me? Thank you in advance!\\nEDIT: I've looked in the code a little bit more, it seems that when I do scancel or scontrol requeue, the code directly exit, without throwing an exception, and that's why it didn't go to the except  _on_exception section. Is this expected behavior? Or is there anyway to solve it?\\nI think that's related to the signal that SLURM sent to my program, and I already see a SignalConnector dealing with SLURM in pytorch-lightning here. I also see this answer about the signal of SLURM. Maybe I should set it in the sbatch script? Any suggestions?\",\n",
              " 'D_kwDOCqWgoM4AN_8rcon': 'I converted some Pytorch code to Lightning. The dataset is loaded lazily by the train & eval dataloaders.\\nHowever, when moving the code to Lightning, I noticed a huge slowdown. After digging around, I noticed that there was a ~10 seconds delay between each epoch. For comparison, on my vanilla Pytorch, an epoch takes ~4s.\\nI first thought it was a data loading problem, but during the 10s delay, no data is loaded (at least that\\'s what my print tell me).\\nI think the issue is related to the number of workers, because setting n_workers=0 solves the problem (but is slower in the end, since only one worker is not enough). I know starting workers is slow, however I have persistent_workers=True and this does not happen in normal Pytorch. My data loaders also have pin_memory=True (removing pin_memory does not solve the problem).\\nSince this is company code, I cannot disclose the before/after, but I\\'ll try to \"anonymize\" some code if necessary. Here is the lightning module:\\nclass RawModule(pl.LightningModule):\\n    def __init__(self):\\n        super(RawModule, self).__init__()\\n\\n        self.encoder1 = nn.Sequential(...)\\n        self.encoder2 = nn.Sequential(...)\\n\\n    def forward(self, data1, data2):\\n        result1 = self.encoder1(data1)\\n        result2 = self.encoder2(data2)\\n\\n        result1 = result1 .view(result1 .size(0), -1)\\n        result2 = result2 .view(result2 .size(0), -1)\\n\\n        result1 = F.normalize(result1 , p=2, dim=1)\\n        result2 = F.normalize(result2 , p=2, dim=1)\\n\\n\\n        return result1, result2\\n\\n    \\n    def calculate_loss(self, batch):\\n        x, r, y = batch\\n        a, v = self.forward(r, x)\\n\\n        d = nn.functional.cosine_similarity(a, v)\\n        loss = logloss(d.unsqueeze(1), y)\\n\\n        return loss\\n\\n\\nclass Module(RawModule):\\n    def training_step(self, batch, batch_idx):\\n        loss = self.calculate_loss(batch)\\n        self.log(\"train_loss\", loss)\\n        return loss\\n\\n    def validation_step(self, batch, batch_idx):\\n        loss = self.calculate_loss(batch)\\n        self.log(\"validation_loss\", loss)\\n        return loss\\n\\n    def configure_optimizers(self):\\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\\n        return optimizer\\n\\n\\nif __name__ == \\'__main__\\':\\n    # stuff...\\n\\n    train_loader = data_utils.DataLoader(\\n        train_dataset, batch_size=256, shuffle=True,\\n        num_workers=5, persistent_workers=True,\\n        pin_memory=True,\\n    )\\n\\n    val_loader = data_utils.DataLoader(\\n        test_dataset, batch_size=256,\\n        num_workers=2, persistent_workers=True,\\n        pin_memory=True,\\n    )\\n\\n    # Model\\n    load_from_pytorch = True\\n\\n    if checkpoint_path is None:\\n        model = Module()\\n\\n        if load_from_pytorch:\\n            if not checkpoint_path:\\n                raise ValueError(\"Please provide a checkpoint path\")\\n            model.load_state_dict(torch.load(checkpoint_path)[\\'state_dict\\'])\\n    else:\\n        model = Module.load_from_checkpoint(checkpoint_path)\\n\\n\\n    trainer = pl.Trainer(\\n        gpus=1,\\n        max_epochs=5,\\n        check_val_every_n_epoch=10,\\n        log_every_n_steps=5,\\n    )\\n    trainer.fit(model, train_loader, val_loader)\\nHere is the result of profiler=\"simple\":\\nAction                                  |  Mean duration (s)    |Num calls              |  Total time (s)       |  Percentage %         |\\n----------------------------------------------------------------------------------------------------------------------------------------\\nTotal                                   |  -                    |_                      |  48.813               |  100 %                |\\n----------------------------------------------------------------------------------------------------------------------------------------\\nrun_training_epoch                      |  27.922               |1                      |  27.922               |  57.202               |\\nfetch_next_sanity_check_batch           |  4.4013               |3                      |  13.204               |  27.05                |\\nget_sanity_check_batch                  |  4.4013               |3                      |  13.204               |  27.05                |\\nfetch_next_train_batch                  |  1.2734               |10                     |  12.734               |  26.087               |\\nget_train_batch                         |  1.2734               |10                     |  12.734               |  26.087               |\\nrun_training_batch                      |  0.47733              |9                      |  4.296                |  8.8009               |\\noptimizer_step_with_closure_0           |  0.40089              |9                      |  3.608                |  7.3915               |\\nvalidation_step                         |  0.664                |2                      |  1.328                |  2.7206               |\\nevaluation_step_and_end                 |  0.664                |2                      |  1.328                |  2.7206               |\\ntraining_step_and_backward              |  0.12644              |9                      |  1.138                |  2.3313               |\\nbackward                                |  0.096889             |9                      |  0.872                |  1.7864               |\\ntraining_step                           |  0.029556             |9                      |  0.266                |  0.54494              |\\nmodel_forward                           |  0.029556             |9                      |  0.266                |  0.54494              |\\non_train_start                          |  0.016                |1                      |  0.016                |  0.032778             |\\n\\nHere is the result of profiler=\"advanced\": https://pastebin.com/q3C5P826.\\nFinally, here is a video demonstrating the problem. I\\'m printing each piece of data loading, to prove it\\'s not the issue.\\nhttps://user-images.githubusercontent.com/30944236/140587623-ae184fa3-370a-42be-8593-200026d11ba4.mp4\\nRandom informations:\\n\\nI\\'m on Windows 10\\nCPU: AMD Ryzen 5 5600X 6 Core\\nGPU: Nvidia RTX 3070\\nPytorch version: 1.10.0\\nPytorch Lightning version: 1.5.0\\n\\nAny idea on how to find the source of the problem?',\n",
              " 'D_kwDOCqWgoM4AN_cScon': \"So lets say i have a model and i'm using the newest CLI API to train it: The config uses sub modules and can look something like:\\n# config.yaml\\nmodel:\\n  class_path: pl_models_2.ModelPL\\n  init_args:\\n    margin: 0.3\\n    basemodel:\\n      class_path: pl_models_2.ModelBackbone\\n      init_args:\\n        base_model: resnet50\\n        pooling: both\\n\\ndata:\\n  batch_size: 32\\n  image_size: 224\\n  augmentation_strategy: medium2\\n\\nNow i want  to run some scripts  or notebooks using the model i trained with this config and i would like to  instantiate the model using this config file.\\nf/e. model = load_model(config.yaml)\\nI was digging how this happens in LightningCLI and jsonargsparse but gave up after a while.. and started trying to hack this around with importlib, which works but feels like reinventing the wheel - i mean this logic has to be somewhere already :) i just cant find it.\",\n",
              " 'D_kwDOCqWgoM4ANn3Dcon': 'Hello Lightning folks!\\nSuppose I have a base model class that I\\'d like to inherit from as follows:\\nimport pytorch_lightning as pl\\n\\n\\nclass ParentModel(pl.LightningModule):\\n    def __init__(\\n        self,\\n        lr: float = 0.001,\\n    ):\\n        super(ParentModel, self).__init__()\\n        self.lr = lr\\n\\nclass ChildModel(ParentModel):\\n    def __init__(\\n        self,\\n        lr: float = 0.005,\\n        loss: str = \"mse\",\\n    ):\\n        super(ParentModel, self).__init__()\\n        self.lr = lr\\n        self.loss = loss\\nI would like to be able to access the hyperparameters of ChildModel and one way to do that is by including save_hyperparameters() in the __init__ as follows:\\nclass ChildModel(ParentModel):\\n    def __init__(\\n        self,\\n        lr: float = 0.005,\\n        loss: str = \"mse\",\\n    ):\\n        super(ParentModel, self).__init__()\\n        self.lr = lr\\n        self.loss = loss\\n        self.save_hyperparameters()\\nHowever, I would like to avoid the need to call save_hyperparameters() in every class that inherits from ParentModel and I was wondering whether it is possible to do this in PyTorch Lightning somehow?\\nOne idea I have in mind is something like a __post_init__  that calls save_hyperparameters() after the __init__ is called, but this doesn\\'t seem to be supported.\\nThanks!',\n",
              " 'D_kwDOCqWgoM4ANo9Fcon': 'Hi, everyone!\\nI want to load model from checkpoint when start a training, and save it to disk when finished every epoch automatically,\\nIs there any nice way to do that correctly?\\nShall we modify the Trianer code, or just use a special hook?',\n",
              " 'D_kwDOCqWgoM4ANogYcon': 'I want to train my model in 20000 steps and run evaluation on each 1000 steps\\nI try to define the callback like this:\\nclass IntervalStepValidate(Callback):\\n    def __init__(self, config):\\n        self.config = config\\n        self.total_steps = 20000\\n        self.validation_interval = 1000\\n\\n    def on_batch_end(self, trainer, pl_module):\\n        if self.total_steps % self.validation_interval == 0:\\n            trainer.run_evaluation()\\n\\nBut I find out that there is no run_evaluation() in the latest version of Pytorch-Lightning :(\\nHow can I update this code to get the function I want?',\n",
              " 'D_kwDOCqWgoM4ANqDIcon': \"Has anyone else run into this error:\\nValueError('signal only works in main thread')\\nI'm running a hyper parameter sweep using Weights and Biases's framework.\\nRunning on a GPU on Google Colab which causes all launched runs to fail. Running it locally (Mac OS) prompts 'signal only works in main thread' to be printed to stdout (which also happens on Colab) but it doesn't crash.\\nAny ideas? It seems people using Ray with PL have come across this. The hacky solution presented there (os.environ['SLURM_JOB_NAME'] = 'bash') doesn't work in my case (neither on Mac OS or Colab).\",\n",
              " 'D_kwDOCqWgoM4ANrAKcon': \"Hi! I am playing around with pytorch-lightning.\\nProblem\\nI tried to use 2 gpus and manually merge training loss described in Lightning in 2 steps.\\nBut when I call training_step_end(), it just gives me only one gpu's loss, not all gpus loss.\\n\\nQuestion\\nDo I have to reduce loss myself in training_step_end()?\\nMy code\\nimport torch\\nfrom torch import nn\\nfrom torch import optim\\nfrom torch.utils.data import DataLoader\\nfrom torchvision import transforms, datasets\\nimport torch.nn.functional as F\\nimport pytorch_lightning as pl\\n\\n\\nclass BaseImageClassificationSystem(pl.LightningModule):\\n    def __init__(self):\\n        super().__init__()\\n        self.backbone = nn.Sequential(nn.Conv2d(1, 64, 3), nn.AdaptiveAvgPool2d((1, 1)))\\n        self.fc = nn.Linear(64, 10)\\n\\n    def forward(self, x):\\n        return self.fc(torch.flatten(self.backbone(x), 1))\\n\\n    def training_step(self, batch, batch_idx):\\n        x, y = batch\\n        y_hat = self.fc(torch.flatten(self.backbone(x), 1))\\n        loss = F.cross_entropy(y_hat, y)\\n        self.log('train/loss', loss)\\n        return loss\\n\\n    def training_step_end(self, losses):\\n        print(losses)\\n        return (losses[0] + losses[1]) / 2\\n\\n    def configure_optimizers(self):\\n        return optim.SGD(self.parameters(), lr=0.01)\\n\\n\\ntrain_dl = DataLoader(datasets.MNIST(root='./', train=True, transform=transforms.ToTensor(), download=True),\\n                      batch_size=128)\\nmodel = BaseImageClassificationSystem()\\ntrainer = pl.Trainer(num_processes=8, gpus='1, 2', accelerator='ddp', max_epochs=100)\\ntrainer.fit(model, train_dl)\\nOutput\\ntensor(2.3002, device='cuda:2', grad_fn=<NllLossBackward>)\\ntensor(2.2930, device='cuda:1', grad_fn=<NllLossBackward>)\",\n",
              " 'D_kwDOCqWgoM4ANrY-con': \"I have some data that I store on my LightningModule during validation.  I want to prevent this from being saved by the model checkpoint.  They are not actually parameters and do not affect the state at all.  I want to maintain other parts of the state, I don't want to use weights only.\\nIs it possible to do this?\",\n",
              " 'D_kwDOCqWgoM4ANrkBcon': \"this is my val func:\\n    def validation_step(self, batch, batch_idx):\\n        data = batch\\n\\n        output = self.forward(data['img'])\\n        boxes, scores = self.postprocess(output.cpu().numpy(), batch['shape'])\\n        raw_metric = self.metric(batch, (boxes, scores))\\n\\n        return raw_metric\\n\\n    def validation_epoch_end(self, outputs):\\n\\n        metric = self.metric.gather_measure(outputs)\\n        self.log('recall', value=metric['recall'].avg)\\n        self.log('precision', value=metric['precision'].avg)\\n        self.log('hmean', value=metric['fmeasure'].avg)\\n        return {'hmean': metric['fmeasure'].avg}\\nthis is my checkpoint callback:\\ncheckpoint_callback = ModelCheckpoint(\\n    monitor='hmean',\\n    mode='max',\\n    dirpath='../det_weights',\\n    filename='{epoch:02d}-{hmean:.2f}',\\n    save_last=True,\\n)\\ncheckpoint_callback.FILE_EXTENSION = '.pt'\\nuse this callback, i can only get a epoch=00-hmean=0.00.pt file, It does not record the value of heman\",\n",
              " 'D_kwDOCqWgoM4ANrmAcon': 'Is there a way to make lr_find not print anything while searching?',\n",
              " 'D_kwDOCqWgoM4ANuM0con': 'In a previous version of pytorch lightning I could return a dictionary in the method validation_epoch_end and then the content of the dictionary would automatically populate trainer.callback_metrics. I can then use this in my callbacks.\\nHowever, if I try this in 1.4.8, trainer.callback_metrics is an empty dictionary.\\nDo you suggest any alternative? Calling self.log is not an option, because it cannot log np.ndarrays:\\nself.log(val_output, [[array]])` was called, but `ndarray` values cannot be logged',\n",
              " 'D_kwDOCqWgoM4ANuQ1con': \"How to apply uniform length batching(smart batching)?\\n.\\nHi all! I have a question about applying a smart batching system like the above picture.\\nTo implement smart batching system, I write the code like below:\\n\\nDataset Class\\n\\nclass ExampleDataset(Dataset):\\n    def __init__(self, datas, tokenizer):\\n        super(ExampleDataset, self).__init__()\\n        self.tokenizer = tokenizer\\n\\n        tokenized = [self.tokenize(data) for data in tqdm(datas, desc='Tokenizing..')]\\n        self.input_ids, self.attention_masks, self.labels = list(zip(*tokenized))\\n\\n    def tokenize(self, data):\\n        encodings_dict = self.tokenizer(data)\\n        return [\\n            encodings_dict['input_ids'],\\n            encodings_dict['attention_mask'],\\n            encodings_dict['input_ids']\\n        ]\\n\\n    def __len__(self):\\n        return len(self.input_ids)\\n\\n    def __getitem__(self, idx):\\n        return self.input_ids[idx], self.attention_masks[idx], self.labels[idx]\\n\\nSampler Class\\n\\nclass SmartBatchingSampler(Sampler):\\n    def __init__(self, data_source: torch.utils.data.Dataset, batch_size=1):\\n        super(SmartBatchingSampler, self).__init__(data_source)\\n        self.batch_size = batch_size\\n        self.data_source = data_source\\n\\n        sentence_lengths = [len(sentence[0]) for sentence in data_source]\\n        sentence_indices = [idx for idx in range(len(data_source))]\\n\\n        pack_by_length = list(zip(sentence_lengths, sentence_indices))\\n        sort_by_length = sorted(pack_by_length)\\n        sentence_lengths, sentence_indices = zip(*sort_by_length)\\n\\n        self.bins = [\\n            sentence_indices[i: i + batch_size]\\n            for i in range(0, len(sentence_indices), batch_size)\\n        ]\\n        self.bins = list(chain.from_iterable(self.bins))\\n        self.drop_last = drop_last\\n\\n    def __iter__(self):\\n        for ids in self.bins:\\n            yield ids\\n\\n    def __len__(self):\\n        return len(self.bins)\\n\\n    def shuffle(self, epoch):\\n        np.random.shuffle(self.bins)\\n\\ncollate_fn function\\n\\ndef collate_fn(batch):\\n    def seq_length_(p):\\n        return len(p[0])\\n\\n    max_seq_sample = max(batch, key=seq_length_)[0]\\n    max_seq_size = len(max_seq_sample)\\n\\n    batch_size = len(batch)\\n\\n    input_ids = torch.zeros(batch_size, max_seq_size).fill_(0).long()\\n    attention_masks = torch.zeros(batch_size, max_seq_size).fill_(0).long()\\n    labels = torch.zeros(batch_size, max_seq_size).fill_(0).long()\\n\\n    for idx in range(batch_size):\\n        sample = batch[idx]\\n        sample_input_ids = sample[0]\\n        sample_attention_masks = sample[1]\\n        sample_labels = sample[2]\\n\\n        input_ids[idx].narrow(0, 0, len(sample_input_ids)).copy_(torch.LongTensor(sample_input_ids))\\n        attention_masks[idx].narrow(0, 0, len(sample_attention_masks)).copy_(torch.LongTensor(sample_attention_masks))\\n        labels[idx].narrow(0, 0, len(sample_labels)).copy_(torch.LongTensor(sample_labels))\\n\\n    return input_ids, attention_masks, labels\\n\\nLightningDataModule\\n\\nclass ExampleDataModule(pl.LightningDataModule):\\n    ...\\n    ...\\n\\n    def train_dataloader(self):\\n        sampler = SmartBatchingSampler(self.dataset['train'], batch_size=self.batch_size)\\n        return DataLoader(\\n            dataset=self.dataset['train'],  # ExampleDataset class\\n            sampler=sampler,\\n            collate_fn=collate_fn,\\n        )\\nI have three questions.\\n\\nCan I apply it like this?  If not, let me know how to apply it.\\nIf it is done in the same way as above, the batch size must be determined in advance. If 'auto_scale_batch_size' is performed, how can I know the determined batch size?\\nIf I designate SmartBatchingSampler that the batch size is 32, and 'auto_scale_batch_size' has set the batch size to 128, how does this work?\\n\\nThank you.\",\n",
              " 'D_kwDOCqWgoM4ANuaZcon': \"this is my code:\\ncheckpoint_callback = ModelCheckpoint(\\n    monitor='hmean',\\n    mode='max',\\n    dirpath='../weights',\\n    filename='DB-{epoch:02d}-{hmean:.2f}',\\n    save_last=True,\\n    save_weights_only=True,\\n)\\ntrainer = pl.Trainer(\\n    # open this, must drop last\\n    benchmark=True,\\n    checkpoint_callback=True,\\n    gpus=[0],\\n    max_epochs=1200,\\n    min_epochs=300,\\n    logger=[logger],\\n    callbacks=[early_stop, checkpoint_callback],\\n    resume_from_checkpoint='../weights/DB-epoch=130-hmean=0.70.ckpt'\\n)\\n\\nwhen i try resume train from checkpoint, i got this error: KeyError: 'Trying to restore training state but checkpoint contains only the model. This is probably due to ModelCheckpoint.save_weights_only being set to True.'\",\n",
              " 'D_kwDOCqWgoM4ANv6bcon': \"Hello! I’m new to PyTorch Lightning. Recently, I am developing a PyTorch project. I want to run an open-source DeepSpeech code in Github and load its checkpoint. I clone their repository and run successfully. Then, I want to add their model to my project.\\nIn PyTorch, this is easy. I only need to copy their model definition files and checkpoint files to my project directory. However, when I did these, I received an error when I ran load_from_checkpoint.\\n\\nModuleNotFoundError: No module named deepspeech_pytorch\\n\\nThen, I try to directly call torch.load('librispeech_pretrained_v3.ckpt'). The error is still raised.\\nThus, I was wondering if I can load their model in my own directory without copying their whole repository?\",\n",
              " 'D_kwDOCqWgoM4ANvwZcon': \"Hi, I'm trying to use multi-node DDP, but my training job never gets past the following errors:\\nstore_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\\n1: [10/01/21 13:46:59] INFO: Waiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\\n1: [10/01/21 13:46:59] INFO: Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\\n\\nMy code works fine using DDP with multiple GPUs on a single node.\\nI'm defining NODE_RANK, WORLD_SIZE, MASTER_PORT, MASTER_ADDR as environment variables, but I'm not defining LOCAL_RANK, GROUP_RANK, GLOBAL_RANK or anything else. I was assuming lightning would handle creating the processes and would then set those variables accordingly. Am I mistaken?\\nEdit:\\nTo add some more details, it looks like what is happening (in some cases) is that the rank 1 node is setup before the rank 0 node and this causes issues:\\n1: [10/02/21 13:14:57] INFO: Node index: 1\\n1: initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4\\n1: initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4\\n...\\n0: [10/02/21 13:15:27] INFO: Node index: 0\\n0: initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4\\n0: initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4\\n1: [10/02/21 13:15:29] INFO: Added key: store_based_barrier_key:1 to store for rank: 2\\n1: [10/02/21 13:15:29] INFO: Added key: store_based_barrier_key:1 to store for rank: 3\\n1: [10/02/21 13:15:39] INFO: Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\\n1: [10/02/21 13:15:39] INFO: Waiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\\n1: [10/02/21 13:15:49] INFO: Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\\n1: [10/02/21 13:15:49] INFO: Waiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\\n\\nHowever, I also get this error if I ensure the rank 0 node is setup first:\\n0: [10/02/21 15:42:10] INFO: Node index: 0\\n0: GPU available: True, used: True\\n0: TPU available: False, using: 0 TPU cores\\n0: initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4\\n...\\n0: [10/02/21 15:42:12] INFO: num_nodes_to_use: 2\\n0: [10/02/21 15:42:12] INFO: num_gpus_to_use: 2\\n0: [10/02/21 15:42:12] INFO: Node index: 0\\n0: initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4\\n...\\n1: [10/02/21 15:43:29] INFO: num_nodes_to_use: 2\\n1: [10/02/21 15:43:29] INFO: num_gpus_to_use: 2\\n1: [10/02/21 15:43:29] INFO: Node index: 1\\n1: GPU available: True, used: True\\n1: TPU available: False, using: 0 TPU cores\\n1: initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4\\n...\\n1: [10/02/21 15:44:22] INFO: num_nodes_to_use: 2\\n1: [10/02/21 15:44:22] INFO: num_gpus_to_use: 2\\n1: [10/02/21 15:44:22] INFO: Node index: 1\\n1: initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4\\n0: [10/02/21 16:12:13] INFO: Added key: store_based_barrier_key:1 to store for rank: 0\\n...\\nline 199, in _env_rendezvous_handler\\n0:     store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)\\n0: RuntimeError: connect() timed out.\\n...\\n0: [10/02/21 16:12:23] INFO: Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\\n0: [10/02/21 16:12:33] INFO: Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\\n0: [10/02/21 16:12:43] INFO: Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\\n...\\nline 199, in _env_rendezvous_handler\\n1:     store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)\\n1: RuntimeError: connect() timed out.\",\n",
              " 'D_kwDOCqWgoM4ANwT_con': 'Hello,\\nCurrently, I am working in a Lit Model, which has two encoders. Each of them has its optimizer, scheduler, and loss, as shown below:\\nimport importlib\\n\\nimport torch\\nfrom pytorch_lightning.core.lightning import LightningModule\\nfrom hydra.utils import instantiate\\n\\nfrom source.metric.ULMRRMetric import ULMRRMetric\\n\\n\\nclass LitModel(LightningModule):\\n\\n    def __init__(self, hparams):\\n\\n        super(LitModel, self).__init__()\\n        self.save_hyperparameters(hparams)\\n\\n        # encoders\\n        self.x1_encoder = instantiate(hparams.x1_encoder)\\n        self.x2_encoder = instantiate(hparams.x2_encoder)\\n\\n        # loss function\\n        self.x1_loss = instantiate(hparams.x1_loss)\\n        self.x2_loss = instantiate(hparams.x2_loss)\\n\\n\\n    def forward(self, x1, x2):\\n        x1_repr = self.x1_encoder(x1)\\n        x2_repr = self.x2_encoder(x2)\\n        return x1_repr, x2_repr\\n\\n    def training_step(self, batch, batch_idx, optimizer_idx):\\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\\n        x1_repr, x2_repr = self(x1, x2)\\n        x1_loss=self.x1_loss(x1_repr, x2_repr)\\n        x2_loss = self.x2_loss(x1_repr, x2_repr)\\n\\n        # what to return here?\\n        return\\n\\n    def validation_step(self, batch, batch_idx):\\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\\n        x1_repr, x2_repr = self(x1, x2)\\n        self.log(\"val_x1_LOSS\", self.x1_loss(x1_repr, x2_repr), prog_bar=True)\\n        self.log(\"val_x2_LOSS\", self.x2_loss(x1_repr, x2_repr), prog_bar=True)\\n\\n\\n\\n\\n    # Alternating schedule for optimizer steps\\n    def optimizer_step(\\n            self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure,\\n            on_tpu=False, using_native_amp=False, using_lbfgs=False,\\n    ):\\n        # update x1 encoder every even step\\n        if optimizer_idx == 0:\\n            if batch_idx % 2 == 0:\\n                optimizer.step(closure=optimizer_closure)\\n\\n        # update x2 encoder every odd step\\n        if optimizer_idx == 1:\\n            if batch_idx % 2 != 0:\\n                optimizer.step(closure=optimizer_closure)\\n\\n\\n\\n    def configure_optimizers(self):\\n        # optimizers\\n        optimizers = [\\n            torch.optim.AdamW(self.x1_encoder.parameters(), lr=self.hparams.lr, betas=(0.9, 0.999), eps=1e-08,\\n                              weight_decay=self.hparams.weight_decay, amsgrad=True),\\n\\n            torch.optim.AdamW(self.x2_encoder.parameters(), lr=self.hparams.lr, betas=(0.9, 0.999), eps=1e-08,\\n                              weight_decay=self.hparams.weight_decay, amsgrad=True)\\n        ]\\n\\n        # schedulers\\n        step_size_up = round(0.03 * self.num_training_steps)\\n        schedulers = [\\n            torch.optim.lr_scheduler.CyclicLR(optimizers[0], mode=\\'triangular2\\', base_lr=self.hparams.base_lr,\\n                                              max_lr=self.hparams.max_lr, step_size_up=step_size_up,\\n                                              cycle_momentum=False),\\n            torch.optim.lr_scheduler.CyclicLR(optimizers[1], mode=\\'triangular2\\', base_lr=self.hparams.base_lr,\\n                                              max_lr=self.hparams.max_lr, step_size_up=step_size_up,\\n                                              cycle_momentum=False)\\n        ]\\n\\n        return optimizers, schedulers\\n\\n\\n    @property\\n    def num_training_steps(self) -> int:\\n        \"\"\"Total training steps inferred from datamodule and number of epochs.\"\"\"\\n        steps_per_epochs = len(self.train_dataloader()) / self.trainer.accumulate_grad_batches\\n        max_epochs = self.trainer.max_epochs\\n        return steps_per_epochs * max_epochs\\nMy intention is to update each encoder in alternate steps (even steps: x1_encoder; odd steps: x2_encoder).\\nAfter reading the documentation, it was not clear to me how it would be possible to update the parameters of each encoder from the loss of each one of them. For instance, I would like to update x1_encoder\\'s parameters based on the x1_loss value and leveraging optimizer_1. Respectively, I would like to update x2_encoder\\'s parameters based on the x2_loss value and employing optimizer_2.\\nI appreciate any help you can provide.',\n",
              " 'D_kwDOCqWgoM4ANwn_con': 'I would like to provide my own learning rate scheduler. What I would love is do is doing this in the lightning style, e.g. implementing some hooks:\\nclass MyScheduler(pl.LightningScheduler):\\n    ...\\n    def on_step(self...):\\n        ...\\n    def on_epoch(self...):\\n        ...\\nIs something like this possible? How do other people handle custom schedulers?\\nPS: I asked this question before on the [deprecated forum.]\\n(https://forums.pytorchlightning.ai/t/custom-scheduler-class/1238)',\n",
              " 'D_kwDOCqWgoM4ANx-Lcon': 'hello,\\nhow can i update the learning rate of adam optimizer after 10 epochs ?\\nmy code is like this\\nself.lr_decay_epoch = [15,]\\nif epoch in self.lr_decay_epoch:\\n     self.lr = self.lr * 0.1\\n     self.optimizer = Adam(filter(lambda p: p.requires_grad, self.net.parameters()), lr=self.lr, weight_decay=self.wd)',\n",
              " 'D_kwDOCqWgoM4ANxgvcon': 'Hi,\\nThe test method of the Trainer class, has the input argument ckpt_path. According to the docs:\\n\\nckpt_path (Optional[str]) – Either best or path to the checkpoint you wish to test. If None and the model instance was passed, use the current weights. Otherwise, the best model from the previous trainer.fit call will be loaded.\\n\\nAlso, in the documentation of PyTorch Lightning for the test set, using Trainer, there is the following:\\n# run full training\\ntrainer.fit(model)\\n\\n# (1) load the best checkpoint automatically (lightning tracks this for you)\\ntrainer.test(ckpt_path=\"best\")\\nMy question is, according to what the \"best\" checkpoint is decided? That is, is the \"best\" decided on maximising or minimising some value? What would be that value? Can someone configure the policy (i.e. minimising or maximising) and the value? How one should use this \"best\" string?\\nLinks for reference:\\n\\nTest set documentation\\nTest method of Trainer class documentation\\n\\nP.S. Please note that I\\'m not referring to using the ModelChekpoint callback, but explicitly to the above, which seems that the ModelCheckpoint callback is not used.',\n",
              " 'D_kwDOCqWgoM4ANxrdcon': \"So I have code like this:\\ndef test_step(self, batch, batch_idx):\\n    x = batch['src']\\n    y = batch['label']\\n    mask = batch['mask']\\n\\n    x = self.base_model(x, mask)\\n    x = self.linear(x).mean(axis=1).squeeze(1)\\n    \\n    loss = F.binary_cross_entropy_with_logits(input=x,\\n                           target=y)\\n    try:\\n        auc = roc_auc_score(y_true=y.cpu().numpy(), y_score=torch.sigmoid(x).cpu().numpy())\\n    except ValueError:\\n        auc = 0\\n    metrics = {'test_loss': loss, 'test_auc': auc}\\n    self.log_dict(metrics)\\n    return metrics\\n\\nI'm trying to calculate the AUC for the model over a test set,  but I assume this function is only calculating the AUC for a single batch. So I guess my question is whether the test set is treated as a single batch and if not, how would I go about doing this? Would I have to average the AUC over all the batches? If so, I'm not sure how to do that. But more importantly, that doesn't seem like the ideal way to calculate this metric to me, given how much variance there would be over each batch.\",\n",
              " 'D_kwDOCqWgoM4ANymucon': 'Hey pytorch-lightning team,\\nI am trying to build an autoencoder architecture via pytorch-lightning (newbie), which I observe its advantages while running complex models in comparison to pytorch implementation.\\nHowever in this AE vanilla model, I observe significant difference in the embedding output of pytorch-lightning vs pytorch where I expect that pytorch-lightning should give the exact UMAP embedding as in pytorch\\nand here is an example:\\nPytorch implementation\\n## AE Architecture\\nclass Autoencoder(nn.Module):\\n    def __init__(self):\\n        super().__init__()        \\n        self.encoder = nn.Sequential(\\n            #nn.Flatten(),\\n            nn.Linear(nfeatures_rna, 200),\\n            nn.LeakyReLU(negative_slope=0.01),\\n            nn.BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\\n            nn.Linear(200, 10), # (N, 784) -> (N, 128)\\n            nn.Dropout(0.2)\\n        )\\n        \\n        self.decoder = nn.Sequential(\\n            nn.Linear(10, 200), \\n            nn.LeakyReLU(negative_slope=0.01),\\n            nn.BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\\n            nn.Linear(200, nfeatures_rna), # (N, 784) -> (N, 128)\\n        )\\n    def forward(self, x):\\n        x = torch.flatten(x, start_dim=1)\\n        encoded = self.encoder(x)\\n        decoded = self.decoder(encoded)\\n        return decoded\\nmodelV = Autoencoder()\\nlr = 1e-2\\noptimizer = torch.optim.Adam(modelV.parameters(), lr=lr)\\nnum_epochs = 50\\noutputs = []\\nfor epoch in range(num_epochs):\\n    for x, y in data_loader:\\n        #x = x.reshape(-1, nfeatures_rna) # -> use for Autoencoder_Linear\\n        recon = modelV(x)\\n        \\n        loss = criterion(recon, x)\\n        \\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n    print(f\\'Epoch:{epoch+0}, Loss:{loss.item():.4f}\\')\\n    outputs.append((epoch, x, recon))\\ndef get_encodings(model, dl):\\n    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n    model.eval()\\n    with torch.no_grad():\\n        encodings = [model.encoder(x) for x, _ in dl]\\n    return torch.cat(encodings, dim=0)\\nencodings = get_encodings(modelV, data_loader)\\nencodings = encodings.cpu().numpy()\\nencodings.shape\\n## UMAP\\nimport umap\\nembedding = umap.UMAP(random_state=0).fit_transform(encodings)\\nplot_df = pd.DataFrame({\\'A\\' : []})\\n#plot_df = metadata.copy()\\nplot_df[\"UMAP1\"] = embedding[:, 0]\\nplot_df[\"UMAP2\"] = embedding[:, 1]\\n\\nHere is the UMAP plot for pytorch embedding:\\n\\nPytorch-lightning implementation for the same Pytorch AE model above\\nimport pytorch_lightning as pl\\nimport torch\\nfrom torch import nn\\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\\nlr = 1e-2\\nclass Autoencoder(pl.LightningModule):\\n    def __init__(self):\\n        super().__init__()        \\n        \\n        ## Encoder Arch\\n        \\n        self.encoder = nn.Sequential(\\n            # nn.Flatten(),\\n            nn.Linear(nfeatures_rna, 200),\\n            nn.LeakyReLU(negative_slope=0.01),\\n            nn.BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\\n            nn.Linear(200, 10), # (N, 784) -> (N, 128)\\n            nn.Dropout(0.2)\\n        )\\n        \\n        ## Decoder Arch\\n        \\n        self.decoder = nn.Sequential(\\n            nn.Linear(10, 200), \\n            nn.LeakyReLU(negative_slope=0.01),\\n            nn.BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\\n            nn.Linear(200, nfeatures_rna), # (N, 784) -> (N, 128)\\n        )\\n    \\n    ## Traning step\\n    \\n    def training_step(self, batch, batch_idx):\\n        x, y = batch\\n        # encode\\n        #x = x.view(x.size(0), -1)\\n        z = self.encoder(x)\\n    \\n    \\n        # decode\\n        recons = self.decoder(z)\\n        \\n        # reconstruction #1#\\n        reconstruction_loss_1 = nn.functional.mse_loss(x, recons)\\n        return reconstruction_loss_1\\n    def configure_optimizers(self):\\n        return torch.optim.Adam(self.parameters(), lr=lr)\\n    \\n## Fotward step\\n    \\n    def forward(self, x):\\n        z = self.encoder(x)\\n        recons = self.decoder(z)\\n        embedding = z\\n        return embedding \\nautoencoder = Autoencoder()\\ntrainer = pl.Trainer(gpus=1, max_epochs=50)\\n# trainer = pl.Trainer(gpus=1)\\ntrainer.fit(autoencoder, data_loader)\\ndef get_encodings(model, dl):\\n    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n    model.eval()\\n    with torch.no_grad():\\n        encodings = [model.encoder(x) for x, _ in dl]\\n    return torch.cat(encodings, dim=0)\\nmodel = Autoencoder()\\nencodings = get_encodings(model, data_loader)\\nencodings = encodings.cpu().numpy()\\nencodings.shape\\nembedding = umap.UMAP(random_state=0).fit_transform(encodings) \\nplot_df = pd.DataFrame({\\'A\\' : []})\\nplot_df[\"UMAP1\"] = embedding[:, 0]\\nplot_df[\"UMAP2\"] = embedding[:, 1]\\n##\\ncustom = sns.scatterplot(data=plot_df, x=\\'UMAP1\\', \\n                      y=\\'UMAP2\\', \\n                      #hue=\\'Gen_and_RealCells\\',\\n                      palette=\"deep\", \\n                      #style=\"Gen_and_RealCells\"\\n                     )\\nplt.legend(bbox_to_anchor=(1.02, 1), loc=\\'upper left\\', borderaxespad=0)\\n\\nHere is the UMAP plot for pytorch-lightning embedding :\\n\\nSince I have experience with this data type and the data generative process, which give me confidence to believe in the  pytorch output\\nAny hints or explanations would be highly appreciated! Thank you in advance for your help.\\nBest wishes,\\nAbdelrahman',\n",
              " 'D_kwDOCqWgoM4ANzLvcon': '(Q1) Does PyTorch Lightning enable parallelization across multiple dimension or does it only allow data parallelism?\\nThe FlexFlow implements the parallelism across 4 different dimensions (\"SOAP\": the sample, operator, attribute and parameter dimensions). (Q2) Over which of these does PyTorch-Lightning do parallelization? (Q3) Does PyTorch-Lightning\\'s API give an option to manually control the parallelization for each and every layer individually?',\n",
              " 'D_kwDOCqWgoM4AO0Fscon': \"During fit() I want to log multiple scalars to a single chart using the wandb logger. I am trying to achieve this with this example code:\\nself.log('val/top-k', {'1': 0.6, '5': 0.8, '10': 0.9})\\nOn wandb this creates three charts with the headings val/top-k.1 and (...).5 and (...).10 resp.\\nAm I doing this wrong or do I need to call the wandb logger directly (which I try to avoid to keep the code logger agnostic).\\nThanks!\",\n",
              " 'D_kwDOCqWgoM4AO0KScon': \"Hey,\\nQuestion: How can I set a module/layer in my model-class to always be non-deterministic (irrespective of the deterministic flag in pl.Trainer())?\\nContext: I use pl to train a simple AutoEncoder that uses bilinear upscaling in the decoder part. For debugging, I use the deterministic flag of the pl.Trainer(). However, I receive the following error\\nRuntimeError: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.\\n\\nUnfortunately, the error does not hint at how to set the module to be non-deterministic and neither does the documentation.\\nCheers\\ndsethz\",\n",
              " 'D_kwDOCqWgoM4AO0kZcon': 'My ModelCheckpoint callback:\\nckpt_callback = pl.callbacks.ModelCheckpoint(\\n    filename=\"T5-{epoch}-{step}-{val_loss:.2f}-{val_ppl:.2f}\",\\n    monitor=\"val_loss\",\\n    save_top_k=-1,\\n    every_n_train_steps=100\\n)\\nQuestions:\\n\\nI got the checkpoints starting with 199 step:\\n\\nWhy does NOT pl save the checkpoint in 99 step?\\nWhy is step not an integer multiple of 100? Because counting starts at 0? It is kind of weird.',\n",
              " 'D_kwDOCqWgoM4AO1KVcon': 'BUG\\nI finished installing pytorch_lightnin by pip but can not import pytorch_lightning.\\nEnvironment\\n\\n\\nCUDA: 11.2\\n\\n\\nGPU: v100 * 8\\n\\n\\nPackges:\\n-OpenCC==1.1.0\\n-pytorch-lightning==1.1.2\\n-six==1.14.0\\n-tensorboard==2.4.0\\n-tensorboard-plugin-wit==1.7.0\\n-threadpoolctl==2.1.0\\n-tokenizers==0.9.4\\n-torch==1.10.2+cu113\\n-transformers==4.1.1\\n-yacs\\n-lxml\\n\\n\\nError message',\n",
              " 'D_kwDOCqWgoM4AO1fMcon': 'I implemented pytorch lightning-based learning as follows.\\n\\ndm = build_datamodule(config)\\nmodel = build_model(config)\\ntrainer = Trainer(\\n...\\naccelerator=\"ddp\",\\n...\\n)\\ntrainer.fit(model, dm)\\n\\nIn this situation, in order to set different model parameters for each gpu process, distributed.get_rank() must be called at the stage of model building.\\nHowever, the trainer.fit function doesn\\'t seem to be able to implement this because it requires an already built model.\\nI wonder if there is any other way to do this.',\n",
              " 'D_kwDOCqWgoM4AO1rwcon': \"With a basic Datamodule like:\\nclass MyDM(pl.lightningDataModule):\\n    def __init__(self,):\\n        <init some stuff>\\n\\n    def setup(self, stage:typing.Optional[str] = None):\\n        .... <sort out dataset etc>\\n\\n   def train_dataloader(self):\\n         ....\\n   etc etc\\n\\nmodel = MyModel()\\ndata = MyDM()\\ntrainer pl.Trainer(reload_dataloaders_every_n_epochs=5)\\ntrainer.fit(model, data)\\n\\nDoes the flag reload_dataloaders_every_n_epochs=N cause data.setup() to be called every reload.\\nI expect my dataset to be constantly changing and am currently assuming that I can define anything I don't expect to be changing in init and anything I will need to change every N epochs in setup. A quick look at the pl.trainer source code (specifically the reset_train_dataloader method) doesn't immediately elucidate this for me.\",\n",
              " 'D_kwDOCqWgoM4AO2uCcon': 'How to load a model saved in PyTorch Lightning in Vanilla PyTorch?',\n",
              " 'D_kwDOCqWgoM4AO2wIcon': '6 t = pl.Trainer(gpus=[0])\\n\\n----> 7 lr_finder = t.lr_find(module)\\nWhen I try to train a model, I got an error like;\\n\"AttributeError: \\'Trainer\\' object has no attribute \\'lr_find\\' \"\\nHow can I fix this?\\nThanks,',\n",
              " 'D_kwDOCqWgoM4AO3Etcon': \"Hi, I was recently reading this example from NVIDIA DALI:\\nhttps://github.com/NVIDIA/DALI/blob/629c57592b9b4e91b8213e6c77c1af179f7dd079/docs/examples/frameworks/pytorch/pytorch-lightning.ipynb\\nI wanted to split the model and datamodule apart. In that case, how can I get local_rank, global_rank and world_size for datamodule's setup?\",\n",
              " 'D_kwDOCqWgoM4AO3U-con': 'Hey,\\nI set up an AutoEncoder for image reconstruction using the LightningModule (i.e. my_model). At inference, I currently run pl.Trainer.test() to obtain my test metrics, pl.Trainer.predict() to obtain my image predictions, and then in a 3rd loop save each prediction obtained from the pl.Trainer.predict()-step. This is awfully complicated and ideally I would like to directly save predictions in test_step(). I have the following test_step().\\ndef test_step(self, batch, batch_idx):\\n    batch_hat = self(batch)\\n    loss_test = nn.functional.mse_loss(batch_hat, batch)\\n    self.log(\"loss_test\", loss_test, on_step=True, on_epoch=True, sync_dist=True)\\n    \\n    # I want to add something like this (NOT FUNCTIONAL)\\n    filepath = trainer.datamodule.dataset_test.my_file_names[batch_idx]\\n    my_save_function(batch_hat, filepath)\\n\\n    return loss_test\\nand I call test_step() with the conventional\\ntrainer.test(my_model, my_datamodule)\\n, where my_datamodule is LightningDataModule and trainer is a pl.Trainer().\\nQuestion: How can I access the name of my samples in batch within test_step() that are stored in my_datamodule?\\nGoal: I want to re-use the file names assigned to my input images for the predictions in test_step().\\nBest,\\ndsethz',\n",
              " 'D_kwDOCqWgoM4AO4EPcon': 'I am getting the below error when running trainer.fit:\\nAttributeError: \\'Trainer\\' object has no attribute \\'run_evaluation\\'\\nFull traceback:\\nTraceback (most recent call last):\\n  File \"sdr_main.py\", line 81, in <module>\\n    main()\\n  File \"sdr_main.py\", line 28, in main\\n    main_train(model_class_pointer, hyperparams,parser)\\n  File \"sdr_main.py\", line 73, in main_train\\n    trainer.fit(model)\\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 741, in fit\\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\\n    return trainer_fn(*args, **kwargs)\\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 777, in _fit_impl\\n    self._run(model, ckpt_path=ckpt_path)\\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1199, in _run\\n    self._dispatch()\\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1279, in _dispatch\\n    self.training_type_plugin.start_training(self)\\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\\n    self._results = trainer.run_stage()\\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1289, in run_stage\\n    return self._run_train()\\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1319, in _run_train\\n    self.fit_loop.run()\\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 140, in run\\n    self.on_run_start(*args, **kwargs)\\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in on_run_start\\n    self.trainer.call_hook(\"on_train_start\")\\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1495, in call_hook\\n    callback_fx(*args, **kwargs)\\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py\", line 138, in on_train_start\\n    callback.on_train_start(self, self.lightning_module)\\n  File \"/content/SDR/utils/pytorch_lightning_utils/callbacks.py\", line 10, in on_train_start\\n    return trainer.run_evaluation()\\nAttributeError: \\'Trainer\\' object has no attribute \\'run_evaluation\\'\\n\\nMy Trainer object:\\ntrainer = pytorch_lightning.Trainer(\\n    num_sanity_val_steps=2,\\n    gradient_clip_val=hparams.max_grad_norm,\\n    callbacks=[RunValidationOnStart()],\\n    checkpoint_callback=ModelCheckpoint(\\n        save_top_k=3,\\n        save_last=True,\\n        mode=\"min\" if \"acc\" not in hparams.metric_to_track else \"max\",\\n        monitor=hparams.metric_to_track,\\n        dirpath=model.hparams.hparams_dir,\\n        filename=\"{epoch}\",\\n        verbose=True,\\n    ),\\n    logger=logger,\\n    max_epochs=hparams.max_epochs,\\n    gpus=hparams.gpus,\\n    strategy=\"dp\",\\n    limit_val_batches=hparams.limit_val_batches,\\n    limit_train_batches=hparams.limit_train_batches,\\n    limit_test_batches=hparams.limit_test_batches,\\n    check_val_every_n_epoch=hparams.check_val_every_n_epoch,\\n    profiler=SimpleProfiler(),\\n    accumulate_grad_batches=hparams.accumulate_grad_batches,\\n    reload_dataloaders_every_epoch=True,\\n    resume_from_checkpoint=hparams.resume_from_checkpoint,\\n)\\n\\nAny idea on how to fix this? My pytorch-lightning version is 1.5.10',\n",
              " 'D_kwDOCqWgoM4AO4uEcon': 'Hey, I was wondering if there was a correct way to evaluate and log multi-class metrics at the end of an epoch, such that the metrics could be evaluated across multiple GPUs using ddp?\\nTypically, if I was looking at a image classification problem I could use the following:\\nself.train_average_accuracy = torchmetrics.Accuracy(num_classes=self.num_classes)\\n\\nx, y = batch\\noutputs = self(x)  # Evaluate logits.\\nsigmoid_outputs = torch.sigmoid(outputs)  # Produce sigmoid outputs for multi-label metrics.\\nloss = self.compute_loss(outputs, y)\\naverage_acc = self.train_average_accuracy(sigmoid_outputs, y)\\n\\nI could log the either with single GPU or multi-GPU accuracy using:\\nself.log(\\n\"train_average_acc\",\\naverage_acc,\\non_step=False,\\non_epoch=True,\\nprog_bar=True,\\n sync_dist=self.multi_gpu)\\n\\nHowever, if I try modifying the accuracy metric to use multi-label accuracy using:\\ntorchmetrics.Accuracy(threshold=0.5, average=None, num_classes=self.num_classes)\\n\\nI find that the metric is logged as nan, which is I\\'m guessing due to the tensor which contains the class wide accuracies not updating between batches. I\\'ve also tried to get around this I ended up passing y and y_pred into the def validation_epoch_end(self, outputs) and evaluating the metric using concatenated batches. However, I still find nan values logged. Is there any other way of logging the multi-label metrics other than, say iterating over the tensor containing the class level metrics?',\n",
              " 'D_kwDOCqWgoM4AO6Tvcon': \"I may be misunderstanding something about the trainer argument fast_dev_run. When I provide fast_dev_run=1 and I add a print statement in my LightningModule's test_step function, the print statement does not appear. In addition, I can see a progress bar for my training set and validation set, but no progress bar appears for the test set.\\nIs fast_dev_run actually running n batches of my training set? I have passed in a DataModule to trainer.fit() that includes a test_dataloader.\",\n",
              " 'D_kwDOCqWgoM4AO7KDcon': \"Hi\\nI first trained the model on kaggle on celeba:\\nHere's the link to the notebook: https://www.kaggle.com/yashrathikaggle/resnet-gender-detection-with-98-16-accuracy/notebook\\nWhile training the trained automatically checkpointed the best val_acc and train_loss. I downloaded the checkpoint on colab and tried to load the model to see the outputs. And it seems the model outputs are random. It doesn't looks like I was able to get the model.\\nLink to colab: https://colab.research.google.com/drive/1E9eg3BkBQCsyPjmy1TZ3oR-kK7Ti4JXt#scrollTo=waqBK-6WaxrX\\nPlease help if I am doing something wrong here.\",\n",
              " 'D_kwDOCqWgoM4AO7Yecon': 'I have an issue with a weighted mse function that I instantiate in the setup, with a buffer as parameter. Something like this:\\n@torch.jit.script\\ndef weighted_mse_func(weights, y, y_hat):\\n    # weighted regression loss\\n    reg_loss = torch.dot(weights,torch.mean(F.mse_loss(y_hat, y, reduction=\\'none\\'), dim=0))\\n    return reg_loss\\n\\ndef weighted_mse(weights):\\n    def func(y, y_hat):\\n        return weighted_mse_func(weights, y, y_hat)\\n    return func\\n\\n\\nclass model(pl.LightningModule):\\n    def __init__(self, weights):\\n        weights = torch.tensor(weights.copy(), dtype=self.dtype, device=self.device)\\n        self.register_buffer(\"weights\", weights)\\n    \\n    def setup(self, stage):\\n        super().setup(stage)\\n        self.loss = weighted_mse(self.weights)\\nWhen initializing training on the GPU I get an error because self.weights is on CPU and not in GPU, if after the error I check the device of the buffer it\\'s on GPU. So if I re-run the trainer, it works fine, also works fine if I call model.cuda() before training. What is going on? Why is the buffer not in GPU on the setup where it fails, but it is afterward? Is something asynchronous going on here?',\n",
              " 'D_kwDOCqWgoM4AO7sjcon': 'Just received qty 2 of A6000 and these are not compatible\\nwith my existing docker file (lack of sm_86 support)\\nFROM pytorch/pytorch:1.6.0-cuda10.1-cudnn7-runtime\\nRUN pip install pytorch-lightning==1.0.7\\n\\nSo upgraded my docker to\\nFROM pytorch/pytorch:1.10.0-cuda11.3-cudnn8-runtime\\nRUN pip install pytorch-lightning==1.5.10\\n\\nI also made changed to my code the for the lightning braking change from\\ntrainer = pl.Trainer( gpus=[0,1],  \\n        distributed_backend=\\'ddp\\', , \\n        ....\\n\\nto\\ntrainer = pl.Trainer( gpus=[0,1],  \\n        strategy=\\'ddp\\', \\n        ....\\n\\nWhen I try to train it just stops.  So set env  NCCL_DEBUG=WARN\\nas per suggestion\\nto get the following output:\\ninitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\\ninitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\\n----------------------------------------------------------------------------------------------------\\ndistributed_backend=nccl\\nAll distributed processes registered. Starting with 2 processes\\n----------------------------------------------------------------------------------------------------\\n60b476048acc:22:22 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\\nNCCL version 2.10.3+cuda11.1\\n60b476048acc:120:120 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\\n\\n\\nSame happens when I try\\nFROM pytorch/pytorch:1.8.1-cuda11.1-cudnn8-runtime \\nFROM pytorch/pytorch:1.9.1-cuda11.1-cudnn8-runtime \\nFROM pytorch/pytorch:1.10.0-cuda11.3-cudnn8-runtime\\n\\nMy old setup was 2xRTX Titan with nvlink while the new setup is 2xA6000 without a nvlink. nvidia doc says that PCI is used but unclear if I need to do something to use this.\\nDistributed communication docs say \"NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA)\" .\\nI suspect I am missing something about the breaking changes from pl 1.0 to 1.5. Would appreciate hints as to what to look for.\\nIs NCCL something used in pl 1.0 or is this new to pl 1.5?\\nDoes NCCL need to be installed?',\n",
              " 'D_kwDOCqWgoM4AO7tDcon': 'I notice that step in PyTorch Lighting can mean batch or optimizer.step().  So what does step mean in max_steps ? @rohitgr7\\nThe following code maybe helpful:\\n\\n  \\n    \\n      pytorch-lightning/pytorch_lightning/loops/epoch/training_epoch_loop.py\\n    \\n    \\n        Lines 258 to 260\\n      in\\n      5da065e\\n    \\n  \\n  \\n    \\n\\n        \\n          \\n           if not self._should_accumulate(): \\n        \\n\\n        \\n          \\n               # progress global step according to grads progress \\n        \\n\\n        \\n          \\n               self.global_step += 1 \\n        \\n    \\n  \\n\\n\\n\\n  \\n    \\n      pytorch-lightning/pytorch_lightning/loops/epoch/training_epoch_loop.py\\n    \\n    \\n        Lines 323 to 331\\n      in\\n      5da065e\\n    \\n  \\n  \\n    \\n\\n        \\n          \\n               def _should_accumulate(self) -> bool: \\n        \\n\\n        \\n          \\n                   \"\"\"Checks if the optimizer step should be performed or gradients should be accumulated for the current \\n        \\n\\n        \\n          \\n                   step.\"\"\" \\n        \\n\\n        \\n          \\n                   accumulation_done = self._accumulated_batches_reached() \\n        \\n\\n        \\n          \\n                   # Lightning steps on the final batch \\n        \\n\\n        \\n          \\n                   is_final_batch = self._num_ready_batches_reached() \\n        \\n\\n        \\n          \\n                   # but the strategy might not \\n        \\n\\n        \\n          \\n                   strategy_accumulates_on_final_batch = self.trainer.strategy.handles_gradient_accumulation or not is_final_batch \\n        \\n\\n        \\n          \\n                   return not accumulation_done and strategy_accumulates_on_final_batch \\n        \\n    \\n  \\n\\n\\n\\n  \\n    \\n      pytorch-lightning/pytorch_lightning/loops/epoch/training_epoch_loop.py\\n    \\n    \\n        Lines 90 to 93\\n      in\\n      5da065e\\n    \\n  \\n  \\n    \\n\\n        \\n          \\n           @property \\n        \\n\\n        \\n          \\n           def _is_training_done(self) -> bool: \\n        \\n\\n        \\n          \\n               max_steps_reached = _is_max_limit_reached(self.global_step, self.max_steps) \\n        \\n\\n        \\n          \\n               return max_steps_reached or self._num_ready_batches_reached()',\n",
              " 'D_kwDOCqWgoM4AO89ycon': 'When I create a Trainer and run Trainer.fit() I am now getting the following error:\\nraise TypeError(\"cannot assign \\'{}\\' as child module \\'{}\\' \"\\nTypeError: cannot assign \\'int\\' as child module \\'precision\\' (torch.nn.Module or None expected)\\n\\nThis is a new error and this code was just working earlier. Do yall know what could be causing this issue?',\n",
              " 'D_kwDOCqWgoM4AO97econ': 'I would appreciate some help understanding the overfit_batches argument for the trainer.\\nWhen overfitting batches, it seems that the entire train dataloader is deepcopied (code in v1.5.10). In my case, this immediately results in my machine running out of RAM because of the size of this dataset. I also have many validation dataloaders, so I believe this compounds the issue.\\nWill this behavior of copying the entire dataloader be removed in a future release? I believe some other mechanism of duplication is necessary to avoid copying the data that is not included in the batches that are being overfitted.\\nI was trying to read #10877 to understand how the behavior would change, but I am unsure.',\n",
              " 'D_kwDOCqWgoM4AO9hZcon': \"My training code (after a lot of setup) looks like this:\\n# Create a trainer\\ntrainer = pl.Trainer.from_argparse_args(argparse.Namespace(**dict_args),\\n                                        callbacks=my_callbacks)\\n# Look for largest batch size and optimal LR\\ntuner = Tuner(trainer)\\ntuner.scale_batch_size(\\n    model,\\n    train_dataloaders=data_module.get_descending_size_train_dataloader(),\\n    init_val=1)\\ntuner.lr_find(model, data_module.train_dataloader())\\n\\n# Train the model\\ntrainer.fit(model, data_module)\\ntrainer.test(model, data_module)\\n\\nI thought making a data module was the best practice. Am I not allowed to pass a dataloader to both trainer.fit and tuner.scale_batch_size? It's not completely clear from the documentation. Is my only option to re-incorporate my data module into my Lightning Module? It'd be great to have both!\",\n",
              " 'D_kwDOCqWgoM4AOAaccon': \"Hi all, do you know how to save the best model? Since pytorchlighting 's earlystop callback will monitor val_loss and if val_loss stop decreasing,\\xa0it will stop training automaticlly. In this case, the checkpoint of the final model would be the final epoch (the val_loss starts to increase). Can I save epoch 5 or 6 (before val_loss increasing) as the best model?\",\n",
              " 'D_kwDOCqWgoM4AOCktcon': \"In TorchGeo, we use PyTorch Lightning to organize reproducible benchmarks for geospatial datasets. Currently, we have a set of LightningDataModules for each dataset and a much smaller number of LightningModules for each task (semantic segmentation, classification, regression, etc.). Each Dataset defines its own plot() method that describes how to plot images and masks.\\nDuring training/validation steps, we would like to plot a few examples to see how training is progressing. However, the LightningModule doesn't seem to know anything about the LightningDataModule/DataLoader/Dataset. Because of this, if we want to perform dataset-specific plotting during training or validation steps, we're forced to create a separate LightningModule for each dataset, increasing code duplication and defeating the whole purpose of PyTorch Lightning (example).\\nIs there an easy way for a LightningModule to tell which DataModule/DataLoader/Dataset is being used and call its dataset.plot() method?\\n@calebrob6 @isaaccorley\\n@tchaton this is slightly related to #10469 but different enough that I wanted to start a separate discussion about it.\",\n",
              " 'D_kwDOCqWgoM4AODJUcon': \"Why do these two test codes result in different test results(both average acc and average loss)?\\n\\n\\n\\ndef test_step(self, batch, batch_idx):\\n    input_ids, labels = batch\\n    outs = self(input_ids)\\n    loss = self.loss_fn(outs, labels)\\n    acc = self.acc_fn(outs, labels)\\n    self.log_dict({'test_loss': loss, 'test_acc': acc}, on_step=False, on_epoch=True, logger=False)\\n    return loss, acc\\n\\n\\n\\ndef test_step(self, batch, batch_idx):\\n    input_ids, labels = batch\\n    outs = self(input_ids)\\n    loss = self.loss_fn(outs, labels)\\n    acc = self.acc_fn(outs, labels)\\n    return loss, acc\\n\\ndef test_epoch_end(self, step_outputs):\\n    avg_loss = torch.stack([x[0] for x in step_outputs]).mean()\\n    avg_acc = torch.stack([x[1] for x in step_outputs]).mean()\\n    self.log_dict({'test_loss': avg_loss, 'test_acc': avg_acc}, logger=False)\\nI only use one GPU for testing.\",\n",
              " 'D_kwDOCqWgoM4AOE7xcon': 'When the train.fit() starts to train my model, it starts validating at 95% of training of current epoch instead of waiting until 100%.\\nWhy does it happen? It makes Val-loss/acc/miou not accurate anymore…',\n",
              " 'D_kwDOCqWgoM4AOGUxcon': \"I wrote following code:\\n    def configure_optimizers(self):\\n        ......\\n        return [\\n            {\\n                'optimizer': optimizer,\\n                'lr_scheduler': {\\n                    'scheduler': scheduler,\\n                    'interval': 'step',\\n                    'frequency': 1\\n                }\\n            }\\nI choose step as the interval. Actually, I don't understand what step means!!!\\nIn my opinion, step may mean a batch? But when I set Trainer parameter:  accumulate_grad_batches=5, will lr_scheduler still execute after one batch or it only execute after every  accumulate_grad_batches batches? If the answer is the later, so the step means the call of optimizer.step()?\\n(I know accumulate_grad_batches can affect optimizer, but I don't know whether it can affect lr_scheduler)\",\n",
              " 'D_kwDOCqWgoM4AOGVEcon': 'When I update the following repository to 1.5.2, I get the following error. Is there an implementation guide somewhere?\\nKeiku/PyTorch-Lightning-CIFAR10: \"Not too complicated\" training code for CIFAR-10 by PyTorch Lightning https://github.com/Keiku/PyTorch-Lightning-CIFAR10\\n(PyTorch-Lightning-CIFAR10) ⋊> /m/n/k/c/PyTorch-Lightning-CIFAR10 on main ⨯\\npipenv run python train.py +experiments=train_exp01 hydra.run.dir=outputs/train_exp01\\n\\nGlobal seed set to 0\\nGPU available: True, used: False\\nTPU available: False, using: 0 TPU cores\\nIPU available: False, using: 0 IPUs\\n/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1579: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\\n  rank_zero_warn(\\nTraceback (most recent call last):\\n  File \"train.py\", line 68, in main\\n    trainer.fit(model,\\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 737, in fit\\n    self._call_and_handle_interrupt(\\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 682, in _call_and_handle_interrupt\\n    return trainer_fn(*args, **kwargs)\\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 772, in _fit_impl\\n    self._run(model, ckpt_path=ckpt_path)\\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1141, in _run\\n    self.accelerator.setup(self)\\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu.py\", line 35, in setup\\n    return super().setup(trainer)\\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 93, in setup\\n    self.setup_optimizers(trainer)\\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 351, in setup_optimizers\\n    optimizers, lr_schedulers, optimizer_frequencies = self.training_type_plugin.init_optimizers(\\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 245, in init_optimizers\\n    return trainer.init_optimizers(model)\\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/optimizers.py\", line 35, in init_optimizers\\n    optim_conf = self.call_hook(\"configure_optimizers\", pl_module=pl_module)\\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1496, in call_hook\\n    output = model_fx(*args, **kwargs)\\n  File \"/mnt/nfs/kuroyanagi/clones/PyTorch-Lightning-CIFAR10/model.py\", line 73, in configure_optimizers\\n    total_steps = cfg.train.num_epochs * len(self.train_dataloader())\\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/core/hooks.py\", line 477, in train_dataloader\\n    raise NotImplementedError(\"`train_dataloader` must be implemented to be used with the Lightning Trainer\")\\nNotImplementedError: `train_dataloader` must be implemented to be used with the Lightning Trainer\\n\\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\\n(PyTorch-Lightning-CIFAR10) ⋊> /m/n/k/c/PyTorch-Lightning-CIFAR10 on main ⨯',\n",
              " 'D_kwDOCqWgoM4AOHL0con': 'These hooks are called when trainer initialization begins and ends, before the model has been set, essentially allowing the user to modify the Trainer constructor.  Should we be giving the user this much control over Trainer constructor? Are there scenarios where this is needed? Or can we deprecate these hooks?\\n\\n  \\n    \\n      pytorch-lightning/pytorch_lightning/trainer/callback_hook.py\\n    \\n    \\n        Lines 55 to 63\\n      in\\n      338f3cf\\n    \\n  \\n  \\n    \\n\\n        \\n          \\n           def on_init_start(self): \\n        \\n\\n        \\n          \\n               \"\"\"Called when the trainer initialization begins, model has not yet been set.\"\"\" \\n        \\n\\n        \\n          \\n               for callback in self.callbacks: \\n        \\n\\n        \\n          \\n                   callback.on_init_start(self) \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n           def on_init_end(self): \\n        \\n\\n        \\n          \\n               \"\"\"Called when the trainer initialization ends, model has not yet been set.\"\"\" \\n        \\n\\n        \\n          \\n               for callback in self.callbacks: \\n        \\n\\n        \\n          \\n                   callback.on_init_end(self) \\n        \\n    \\n  \\n\\n\\ncc @ananthsub',\n",
              " 'D_kwDOCqWgoM4AOH_mcon': 'Is the grad scaler included in the model.state_dict() after steup by model, optimizer = self.setup(model, optimizer)?\\nIf not, how can I save and load the state of the grad scaler for resume?',\n",
              " 'D_kwDOCqWgoM4AOI4hcon': \"It is strange that my val_dataloader's shuffle is set to False, but I still get this warning. Any ideas on how to solve this?\",\n",
              " 'D_kwDOCqWgoM4AOI5pcon': 'Hi,\\nI see the doc describing the functions of LightningDataModule.\\nhttps://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/datamodules.html#Defining-The-MNISTDataModule\\nHere is my thinking. If some variable, e.g., a transform, can be defined in init function, and later shared across different GPUs. Theoretically, if we load data in init, the data should also be able to transfer to different GPUs similarly. In the case of a single machine with multiple GPUs, the data will be copied multiple times in the memory. In the case of multiple machines, the data will broadcast through the network from the main node to other nodes. Broadcasting large data through networks may have efficiency issue, which is why we had better load data in the setup function.\\nPlease let me know whether my analysis is correct or not. Basically, I am not clear about how the variables, e.g., i.e. self.something, defined in init are shared across multiple GPUs/machines. Thanks!',\n",
              " 'D_kwDOCqWgoM4AOIRxcon': 'I am trying to port my very old pytorch code to lightning and in my training loop, I have something as follows:\\nbatch_order = np.arange(data.x_train.shape[0])\\nbatch_index = np.random.choice(batch_order, batch_size, p=seq_sample_probs).tolist()\\nbatch = torch.tensor(data.x_train[batch_index], dtype=torch_dtype, device=torch_device, requires_grad=False)\\n\\n# then call the forward on this batch\\nmodel.encoder.forward(batch)\\n\\nI was wondering how I can incorporate this batch index selection in the lightning code. In my code, I have the usual:\\ndef train_dataloader(self):\\n        return DataLoader(self.train_dataset, batch_size=self.batch_size)\\n\\nBut I do not know where I can inject my sampling code inside all this.',\n",
              " 'D_kwDOCqWgoM4AOIa1con': 'In case of pytorch-lightning 1.5.2, omission of argument of trainer.test() does not work.\\n        trainer.fit(model, datamodule)\\n        trainer.test()\\nIf it is below, it worked fine. Is this the expected behavior?\\n        trainer.fit(model, datamodule)\\n        trainer.test(model, datamodule.test_dataloader())\\nI\\'m currently refactoring the following:\\nKeiku/PyTorch-Lightning-CIFAR10: \"Not too complicated\" training code for CIFAR-10 by PyTorch Lightning https://github.com/Keiku/PyTorch-Lightning-CIFAR10',\n",
              " 'D_kwDOCqWgoM4AOIuBcon': 'I want to test performance of my model on a few different feature sets and visualize losses of different features on the same plot in tensorboard.\\nMy current work flow is,\\nclass LightningWrapper(pl.LightningModule):\\n    def__init__(self, features):\\n        self.features = features \\n\\n    def training_step(self, batch, batch_idx):\\n        ....\\n        .....\\n        self.logger.experiment.add_scalars(\"version_0\", { f\"{self.features}\" : loss})\\n\\ntrainer = pl.Trainer()\\nfor feature_set in potential_feature_sets:\\n    model = LightningWrapper(feature_set)\\n    trainer.fit(model)\\n\\nBut this is creating different versions inside tensorboard logging. Does each call to Lightning module create a new version ? Is this because the loggers are not shared across feature sets in my current design ? How do I share logger across different feature sets or different models ?',\n",
              " 'D_kwDOCqWgoM4AOJ8Dcon': \"I'll explain: Let's say that I have two nn.modules inside my main LightningModule, but one of them is frozen, i.e. doesn't learn during the training but is only used for inferencing during training (requires_grad is False in this module) and I would like to avoid saving the state_dictionray of this static (frozen) module to the checkpoint file.\\nIn plain PyTorch I'd probably filter manually the state_dictionray fields of the frozen module before the saving.\\nIs there a simple way to do that with pytorch-lightning? Or to raise some flag inside the modules which say to the LightningModule not to save all the parameters inside this frozen module?\\n\\nA simple toy example for clarification.\\nIn this example, I'd like to avoid saving the parameters of self.frozen_nn_module.\\nAll parameters in self.frozen_nn_module don't require grads.\\nclass LightMod(LightningModule):\\n    def __init__(self):\\n        super().__init__()\\n\\n        #some non frozen module  \\n        self.non_frozen_nn_module = non_frozen_nn_module\\n        #some frozen(static) nn.Module\\n        self.frozen_nn_module= frozen_nn_module\\n\\n    def forward(self, x):\\n    Some code....\",\n",
              " 'D_kwDOCqWgoM4AOJpqcon': 'On https://pytorch-lightning.readthedocs.io/en/latest/starter/converting.html, it says that \".test() loads the best checkpoint automatically\". Is that also the case for .predict()?',\n",
              " 'D_kwDOCqWgoM4AOKTvcon': \"As shown in the screenshot,RichProgressBar is hard to read in light theme.  I don't have much time to report a bug, so I open a discussion temporary.\",\n",
              " 'D_kwDOCqWgoM4AOKiQcon': 'Hi!\\nI am currently working on a project where I would like to checkpoint my model in separated pieces.\\nMy model has a backbone composed by:\\n\\na backbone, which is also composed by 3 modules\\nseveral heads, each one being a module\\nI would like to save one ckpt with the backbone and one ckpt per head. I understand that I should create a custom callback inheriting from ModelCheckpoint and then modifying on_save_checkpoint, I am not really aware of how to do it.\\non_save_checkpoint is defined as:\\n\\n\\nAnother solution would be to modify my lightning module to load the ckpt when the training ends as a dict and then save each subpart as a ckpt using torch.save(), but I understand that this solution is much less elegant.\\nAny suggestions? Thanks in advance!',\n",
              " 'D_kwDOCqWgoM4AOKtdcon': 'If I select X workers in my dataloader, and I train my model using ddp with Y GPUs, is the effective amount of workers running in the machine X*Y or X?',\n",
              " 'D_kwDOCqWgoM4AOL5Ncon': 'Hi, I\\'m new to PyTorch Lightning, used it for the first time and kind of liked it. However, I am facing this one problem, Implemented a classification task for which I trained the model with Huggingface pretrained model as base and classification head on top. The model is training successfully and giving decent validation losses. The problem is, I\\'m not quite able to figure out the inferencing part.\\ncan anyone please point out what is it that I\\'m doing wrong? It\\'s probably something very basic.\\nI\\'ll add the classes of the lightning modules and the Data Modules below.\\n# |¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n# | Define the Pytorch Lightning Module Classifier Class     |\\n# |__________________________________________________________|\\n\\nclass ABSASentimentClassifier(pl.LightningModule):\\n\\n  def __init__(self, learning_rate = setup[\\'lr\\'], weights=None, **kwargs):\\n    super().__init__()\\n\\n    self.save_hyperparameters(\\'learning_rate\\', \\'max_epochs\\')\\n    self.model = ABSAModel_Bert()\\n    self.weights = weights\\n    self.preds = []\\n  \\n  def training_step(self, batch, batch_nb):\\n\\n    # Forward\\n    y_hat = self.model(batch)\\n\\n    # if self.weights:\\n    #   self.weights = torch.tensor(class_weights,dtype=torch.float) \\n    \\n    # Loss\\n    loss_fct = torch.nn.CrossEntropyLoss()\\n    \\n    loss = loss_fct(y_hat.view(-1, self.model.num_labels), batch[\\'label\\'].view(-1))\\n\\n    # Logs\\n    self.log_dict({\\'training_loss\\':loss}, prog_bar=True)\\n\\n    return loss\\n\\n  \\n  def validation_step(self, batch, batch_nb):\\n    \\n    # Forward\\n    y_hat = self.model(batch)\\n        \\n    # Loss\\n    loss_fct = torch.nn.CrossEntropyLoss()\\n    loss = loss_fct(y_hat.view(-1, self.model.num_labels), batch[\\'label\\'].view(-1))\\n\\n    # Acc\\n    a, y_hat = torch.max(y_hat, dim=1)\\n    val_acc = accuracy_score(y_hat.cpu(), batch[\\'label\\'].cpu())\\n    val_acc = torch.tensor(val_acc)\\n    \\n    # Logs\\n    self.log_dict({\\'val_loss\\':loss,\\'val_acc\\':val_acc}, prog_bar=True)\\n    \\n    return loss\\n\\n  \\n  def test_step(self, batch, batch_nb):\\n    self.model.eval()\\n    \\n    # Forward\\n    yhat = self.model(batch)\\n      \\n    # Loss\\n    # loss_fct = torch.nn.CrossEntropyLoss()\\n    # loss = loss_fct(y_hat.view(-1, self.model.num_labels), batch[\\'label\\'].view(-1))\\n    \\n    # a, y_hat = torch.max(y_hat, dim=1)\\n    # test_acc = accuracy_score(y_hat.cpu(), batch[\\'label\\'].cpu())\\n    \\n    # Logs\\n    # self.log_dict({\\'test_loss\\':loss,\\'test_acc\\':test_acc}, prog_bar=True)\\n    self.preds = self.preds.extend(yhat.cpu().detach().numpy().tolist())\\n    return \\n\\n  \\n  def predict_dataloader(self, batch, batch_idx: int , dataloader_idx: int = None):\\n\\n    return self.model(batch)\\n\\n\\n  \\'\\'\\'\\n  |¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n  | Training Setup  |\\n  |_________________|\\n  \\'\\'\\'\\n  def configure_optimizers(self):\\n    \\'\\'\\'\\n    |¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n    |   REQUIRED                                                            |\\n    |   can return multiple optimizers and learning_rate schedulers         |\\n    |   (LBFGS it is automatically supported, no need for closure function) |\\n    |_______________________________________________________________________|\\n    \\'\\'\\'\\n    optimizer = torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.hparams.learning_rate, eps=1e-08)\\n    scheduler = {   \\n      \\'scheduler\\': torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-5, \\n                                                       steps_per_epoch=len(self.trainer.datamodule.train_dataloader()),\\n                                                       epochs=self.hparams.max_epochs),\\n                 \\n      \\'interval\\': \\'step\\'  # called after each training step\\n    } \\n    \\n    return [optimizer], [scheduler]\\n\\n  @staticmethod\\n  def add_model_specific_args(parent_parser, root_dir):  # pragma: no-cover\\n    \"\"\"\\n    |¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n    | Define parameters that only apply to this model     |\\n    |_____________________________________________________|\\n    \"\"\"\\n    parser = ArgumentParser(parents=[parent_parser])\\n\\n    # data\\n    parser.add_argument(\\'--data_root\\', default=os.path.join(root_dir, \\'train_val_data\\'), type=str)\\n\\n    # training params (opt)\\n    parser.add_argument(\\'--learning_rate\\', default=setup[\\'lr\\'], type=float, help = \"type (default: %(default)f)\")\\n    return parser\\n\\nalso the dataset class is :\\nclass ABSADataset(Dataset):\\n  def __init__(self, df, tokenizer, max_len=setup[\\'max_sen_length\\']):\\n    self.texts = df[\\'text\\']\\n    self.aspects = df[\\'aspect\\']\\n    if \\'label\\' in df.columns:\\n      # print(\\'****Labels Present****\\')\\n      self.targets = df[\\'label\\']\\n\\n    else:\\n      self.targets = None\\n\\n    self.tokenizer = tokenizer\\n    self.max_len = max_len\\n\\n  def __len__(self):\\n    return len(self.aspects)\\n\\n  def __getitem__(self, idx):\\n\\n    # convert indexes, tensor->list\\n    if torch.is_tensor(idx):\\n      idx = idx.tolist()\\n    \\n    # define the aspect and text item\\n    text = (str(self.texts[idx]))\\n    aspect = str(self.aspects[idx])\\n\\n    # define the label\\n    target = self.targets[idx]\\n\\n    # pair the aspect and text for pair-encoding\\n    pairs = [text, aspect]\\n    \\n    \\'\\'\\'\\n    # |¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n    # | For Debugging            |\\n    # |__________________________|\\n    # print(f\\' text: {text}\\')\\n    # print(f\\' aspect: {aspect}\\')\\n    # print(type(text))\\n    # print(type(aspect))\\n    \\'\\'\\'\\n    \\n    # encode the feature pair\\n    encoded = self.tokenizer.encode_plus(pairs,\\n                                    add_special_tokens=True,\\n                                    padding=\\'max_length\\', \\n                                    max_length=setup[\\'max_sen_length\\'], \\n                                    return_attention_mask=True,\\n                                    return_tensors=\\'pt\\',\\n                                    truncation=True)\\n    \\'\\'\\'\\n    # |¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n    # | For Debugging            |\\n    # |__________________________|\\n    # for ids in encoded[\\'input_ids\\']:\\n    #   print(\\'*\\'*20)\\n    #   print(f\\'{self.tokenizer.decode(ids)} of length = {len(self.tokenizer.decode(ids).split(\" \"))}\\')\\n    #   print(f\\'is encoded as : \\\\n{ids} \\\\nwith length = {len(ids)}\\')\\n    #   print(\\'*\\'*20)\\n    \\'\\'\\'\\n    \\n    return {\\n        \\'label\\' : target,\\n        \\'input_ids\\' : encoded[\\'input_ids\\'],\\n        \\'attention_mask\\' : encoded[\\'attention_mask\\'] \\n    }\\n\\nMy goal is to be able to generate predictions for data without any labels present, using the trained model (saved as checkpoint (.ckpt))\\nThis is what I did:\\ntestset = ABSATest_Dataset(test, tokenizer=transformer_tokenizer)\\n\\ntestLoader = DataLoader(testset, batch_size=setup[\\'test_batch_size\\'])\\n\\ntrainer.predict(model_infer, testLoader)\\n\\nWhere model_infer is :\\nmodel_infer = ABSASentimentClassifier.load_from_checkpoint(PATH_TO_CKPT_FILE)\\n\\nand got :\\n---------------------------------------------------------------------------\\nMisconfigurationException                 Traceback (most recent call last)\\n<ipython-input-44-c724efd019b7> in <module>()\\n----> 1 trainer.predict(model_infer, testLoader)\\n\\n5 frames\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in predict(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\\n    987         \"\"\"\\n    988         return self._call_and_handle_interrupt(\\n--> 989             self._predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\\n    990         )\\n    991 \\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in _call_and_handle_interrupt(self, trainer_fn, *args, **kwargs)\\n    680         \"\"\"\\n    681         try:\\n--> 682             return trainer_fn(*args, **kwargs)\\n    683         # TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\\n    684         except KeyboardInterrupt as exception:\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in _predict_impl(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\\n   1030         )\\n   1031 \\n-> 1032         results = self._run(model, ckpt_path=self.predicted_ckpt_path)\\n   1033 \\n   1034         assert self.state.stopped\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in _run(self, model, ckpt_path)\\n   1115             parsing.clean_namespace(model.hparams)\\n   1116 \\n-> 1117         verify_loop_configurations(self, model)\\n   1118 \\n   1119         # attach model log function to callback\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py in verify_loop_configurations(trainer, model)\\n     38         __verify_eval_loop_configuration(trainer, model, \"test\")\\n     39     elif trainer.state.fn == TrainerFn.PREDICTING:\\n---> 40         __verify_eval_loop_configuration(trainer, model, \"predict\")\\n     41 \\n     42     __verify_dp_batch_transfer_support(trainer, model)\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py in __verify_eval_loop_configuration(trainer, model, stage)\\n    187             raise MisconfigurationException(\"`predict_step` cannot be None to run `Trainer.predict`\")\\n    188         elif not has_step and not is_overridden(\"forward\", model):\\n--> 189             raise MisconfigurationException(\"`Trainer.predict` requires `forward` method to run.\")\\n    190     else:\\n    191         # -----------------------------------\\n\\nMisconfigurationException: `Trainer.predict` requires `forward` method to run.\\n\\nALso, I haven\\'t defined a forward function in the lightning module because it is present in the model class:\\nclass ABSAModel_Bert(torch.nn.Module):\\n\\n  def __init__(self, num_labels=setup[\\'num_labels\\'], config = setup, **kwargs):\\n    super(ABSAModel_Bert, self).__init__()\\n    \\n    self.num_labels = num_labels\\n    self.bert = transformers.AutoModel.from_pretrained(config[\\'model_name\\'])\\n    self.bert_config = transformers.AutoConfig.from_pretrained(config[\\'model_name\\'])\\n\\n    self.pre_classifier = torch.nn.Linear(self.bert_config.hidden_size, self.bert_config.hidden_size)\\n\\n    self.classifier = torch.nn.Linear(self.bert_config.hidden_size, self.num_labels)\\n\\n    self.dropout = torch.nn.Dropout(self.bert_config.hidden_dropout_prob)\\n    # print(f\\'Using Dropout = {self.bert.config.seq_classif_dropout}\\')\\n\\n    self.relu = torch.nn.ReLU()\\n\\n    \\'\\'\\'\\n    |¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n    | freeze the layers of Bert for training if needed so that |   \\n    | the embeddings of all layers of Bert are not changed     |\\n    |__________________________________________________________|\\n    \\'\\'\\'\\n    # for param in self.bert.parameters():\\n    #   param.requires_grad = False\\n\\n  \\n  def forward(self, batch):\\n\\n  #   print((batch[\\'input_ids\\'].squeeze(1)).shape)\\n  #   print(\"*\"*10)\\n  #   print(batch[\\'input_ids\\'])\\n  #   print(\"*\"*10)\\n    \\n    outputs = self.bert(input_ids=batch[\\'input_ids\\'].squeeze(1), \\n                        attention_mask=batch[\\'attention_mask\\'])\\n    \\n    # output from last hidden layer\\n    hidden_state = outputs[0]  # (batch_size, seq_len, dim)\\n\\n    \\'\\'\\'\\n    |¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n    | *output of [CLS] token                                   |\\n    |                                                          |\\n    | [CLS] token contains the pooled embeddings of the entire | \\n    | Sequence, these are used for the classification.         |\\n    |__________________________________________________________|\\n    \\'\\'\\'\\n    pooled_output = hidden_state[:, 0] # (batch_size, dim)\\n\\n    \\'\\'\\'\\n    |¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n    | sending the [CLS] token embeddings through Linear, ReLU  |\\n    | and Dropout layers                                       |\\n    |__________________________________________________________|\\n    \\'\\'\\'\\n    pooled_output = self.pre_classifier(pooled_output)  # (batch_size, dim)\\n    pooled_output = self.relu(pooled_output)  # (batch_size, dim)\\n    pooled_output = self.dropout(pooled_output)  # (batch_size, dim)\\n    logits = self.classifier(pooled_output)  # (batch_size, num_labels)\\n\\n    return logits\\n\\n  def get_outputs(self, input_ids, attention_mask):\\n    outputs = self.bert(input_ids=input_ids, \\\\\\n                        attention_mask=attention_mask)',\n",
              " 'D_kwDOCqWgoM4AOL9Icon': 'I am trying to get multi-gpu training working, on single gpu it is al working fine. However, when I increase the number of GPUs I get a pickling error, and I don\\'t know what to do about it. For the dataloader I am using the patch-based approach from TorchIO, which creates a Queue, maybe that is the cause? Does anyone has experience with TorchIO Queue and Lightning multi-gpu maybe? Or is something else going on?\\nThe error i am getting is as follows:\\nGPU available: True, used: True\\nTPU available: False, using: 0 TPU cores\\nIPU available: False, using: 0 IPUs\\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3,4]\\n\\nTraceback (most recent call last):\\n  File \"/filepath/SRGAN-patch_tio.py\", line 94, in <module>\\n    main()\\n  File \"/filepath/SRGAN-patch_tio.py\", line 91, in main\\n    trainer.fit(model, train_dataloaders=training_loader)\\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 737, in fit\\n    self._call_and_handle_interrupt(\\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 682, in _call_and_handle_interrupt\\n    return trainer_fn(*args, **kwargs)\\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 772, in _fit_impl\\n    self._run(model, ckpt_path=ckpt_path)\\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1195, in _run\\n    self._dispatch()\\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1274, in _dispatch\\n    self.training_type_plugin.start_training(self)\\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 173, in start_training\\n    self.spawn(self.new_process, trainer, self.mp_queue, return_result=False)\\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 201, in spawn\\n    mp.spawn(self._wrapped_function, args=(function, args, kwargs, return_queue), nprocs=self.num_processes)\\n  File \"/mnt/beta/name/miniconda/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\\n    return start_processes(fn, args, nprocs, join, daemon, start_method=\\'spawn\\')\\n  File \"/mnt/beta/name/miniconda/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 179, in start_processes\\n    process.start()\\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/process.py\", line 121, in start\\n    self._popen = self._Popen(self)\\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\\n    return Popen(process_obj)\\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\\n    super().__init__(process_obj)\\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\\n    self._launch(process_obj)\\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\\n    reduction.dump(process_obj, fp)\\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/reduction.py\", line 60, in dump\\n    ForkingPickler(file, protocol).dump(obj)\\n  File \"/mnt/beta/name/miniconda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 547, in __getstate__\\n    raise NotImplementedError(\"{} cannot be pickled\", self.__class__.__name__)\\nNotImplementedError: (\\'{} cannot be pickled\\', \\'_SingleProcessDataLoaderIter\\')',\n",
              " 'D_kwDOCqWgoM4AOLA6con': 'Hi!\\nI have been working with PyTorch lightning for a year or so, but I am still confused (probably because I am not a software developer and my development skills are not really sharp).\\nDo any of you know of any guide or reference on how callbacks and hooks interact with each other? Some of my particular questions are:\\n\\nWhile training, where (or in which order) each callback (train_step, train_epoch_start...) is called?\\nHow exactly do the hooks work?\\nI am aware that maybe this is a really basic question, but any advice on how to better understand the workflow would be great!\\nThanks in advance, you rock guys!',\n",
              " 'D_kwDOCqWgoM4AOLTwcon': 'I am learning deep RL, and as an exercise I looked through and tried to implement this example of DQN in pytorch lightning:\\nDQN example\\nI believe that I have found a bug and I am not sure how to flag it or how to upload a fix myself. This is my first time trying to contribute to an open source project and any advice will be greatly appreciated.\\nOne line 106 of the section for the DQN Lightning Module, the expression for epsilon (which decays over the first eps_last_frame steps) is incorrect. The code is currently:\\n epsilon = max( self.hparams.eps_end, self.hparams.eps_start - self.global_step + 1 / self.hparams.eps_last_frame, )\\nI believe this is incorrect for the following reason: 0 <= epsilon <= 1, so self.global_step immediately overcomes the other two terms. There is no decay, only a single timestep at eps_start + 1/eps_last_frame, and then timesteps at eps_end. This also seems like an error because with the default values,  eps_start + 1/eps_last_frame > 1.\\nThe intended behavior is for the epsilon to decay linearly from eps_start to eps_end over the first eps_last_frame frames. I believe the second argument to max() should be:\\nself.hparams.eps_start - (self.global_step / self.hparams.eps_last_frame) * (self.hparams.eps_start - self.hparams.eps_end)\\nWho do I contact, or is there a way I can upload this fix myself?',\n",
              " 'D_kwDOCqWgoM4AOM_Jcon': 'Hi\\nI am running pyTorch 1.10 and pytorch-lightning 1.5.4. I want to downgrade pytorch-lightning to 0.7.1 because the code I am testing uses this version and It looks to me a lot of breaking changes have happened since then.\\nHow can I do that please? Would a pip uninstall pytorch-lightning and then pip install pytorch-lightning==0.7.1 suffice or there is something else I need to take care of?\\nDoes anyone know also if 0.7.1 is going to be compatible with  pyTorch 1.10 ?',\n",
              " 'D_kwDOCqWgoM4AOMizcon': \"I was wondering if there's a way to use apex.parallel.DistributedDataParallel instead of pytorch native DistributedDataParallel. (I am trying to reproduce a paper that used Apex DDP and apex mixed precision and i am getting lower results using pytorch native one)\",\n",
              " 'D_kwDOCqWgoM4AOMkCcon': 'I am trying to get my losses in Tensorboard, but I am quite confused.\\nI simply return a dict after training_step and validation_step, containing loss and log. However, the only thing that is showing up in Tensorboard of that run is the \\'hp_metric\\' thing... Nothing on the scalars of the losses... Both during and after training.\\nI got it working with manual logging (self.log(...)), however, that should not be necessary right? And is more complicated when I want for example training and validation loss in one plot. I am working on a Super Resolution GAN. This is my trainer:\\nclass LitTrainer(pl.LightningModule):\\n    def __init__(self,\\n                 netG,\\n                 netD,\\n                 lr: float = 0.0002,\\n                 b1: float = 0.5,\\n                 b2: float = 0.999,\\n                 **kwargs\\n                 ):\\n        super().__init__()\\n        self.save_hyperparameters(ignore=[\"netG\", \"netD\"])\\n\\n        self.netG = netG\\n        self.netD = netD\\n\\n        self.criterion_GAN = GANLoss(\"vanilla\")\\n        self.criterion_edge = edge_loss1\\n        self.criterion_pixel = torch.nn.L1Loss()\\n\\n    def forward(self, inputs):\\n        return self.netG(inputs)\\n\\n    def prepare_batch(self, batch):\\n        return batch[\"LR\"][tio.DATA].squeeze(4), batch[\"HR\"][tio.DATA].squeeze(4)\\n\\n    def training_step(self, batch, batch_idx, optimizer_idx):\\n        imgs_lr, imgs_hr = self.prepare_batch(batch)\\n\\n        # train generator\\n        if optimizer_idx == 0:\\n            self.gen_hr = self(imgs_lr)\\n\\n            loss_adv = self.criterion_GAN(self.netD(self.gen_hr), True)\\n            loss_edge = self.criterion_edge(self.gen_hr, imgs_hr)\\n            loss_pixel = self.criterion_pixel(self.gen_hr, imgs_hr)\\n            g_loss = loss_adv + loss_edge + loss_pixel\\n\\n            # self.log(\"loss/G train\", g_loss, on_step=True, on_epoch=True)\\n            tensorboard_logs = {\"loss_g\": {\"train\": g_loss}}\\n            return {\"loss\": g_loss, \"log\": tensorboard_logs}\\n\\n        # train discriminator\\n        if optimizer_idx == 1:\\n\\n            # for real image\\n            pred_real = self.netD(imgs_hr)\\n            real_loss = self.criterion_GAN(pred_real, True)\\n            # for fake image\\n            pred_fake = self.netD(self.gen_hr.detach())\\n            fake_loss = self.criterion_GAN(pred_fake, False)\\n\\n            d_loss = (real_loss + fake_loss) / 2\\n            tensorboard_logs = {\"loss_d\": {\"train\": d_loss}}\\n\\n            # self.log(\"loss/D train\", d_loss, on_step=True)\\n            return {\"loss\": d_loss, \"log\": tensorboard_logs}\\n\\n    def validation_step(self, batch, batch_idx):\\n        with torch.no_grad():\\n            imgs_lr, imgs_hr = self.prepare_batch(batch)\\n            gen_hr = self(imgs_lr)\\n            loss_adv = self.criterion_GAN(self.netD(gen_hr), True)\\n            loss_edge = self.criterion_edge(gen_hr, imgs_hr)\\n            loss_pixel = self.criterion_pixel(gen_hr, imgs_hr)\\n            g_loss = loss_adv + loss_edge + loss_pixel\\n\\n            # for real image\\n            pred_real = self.netD(imgs_hr)\\n            real_loss = self.criterion_GAN(pred_real, True)\\n            # for fake image\\n            pred_fake = self.netD(self.gen_hr.detach())\\n            fake_loss = self.criterion_GAN(pred_fake, False)\\n\\n            d_loss = (real_loss + fake_loss) / 2\\n            tensorboard_logs = {\"loss_g\": {\"val\": g_loss},\\n                                \"loss_d\": {\"val\": d_loss},\\n                                }\\n\\n            # self.log(\"loss/G val\", g_loss, on_step=True)\\n            # self.log(\"loss/D val\", d_loss, on_step=True)\\n\\n            return {\"log\": tensorboard_logs}\\n\\n    def configure_optimizers(self):\\n        lr = self.hparams.lr\\n        b1 = self.hparams.b1\\n        b2 = self.hparams.b2\\n        opt_g = torch.optim.Adam(self.netG.parameters(), lr=lr, betas=(b1, b2))\\n        opt_d = torch.optim.Adam(self.netD.parameters(), lr=lr, betas=(b1, b2))\\n        return [opt_g, opt_d], []\\n\\nAnd this is how I start training:\\nlogger = TensorBoardLogger(\"log\", name=\"test\")\\n\\nmodel = LitTrainer(netG=generator, netD=discriminator)\\ntrainer = pl.Trainer(gpus=1, max_epochs=1, logger=logger, log_every_n_steps=10)\\ntrainer.fit(model, train_dataloaders=training_loader, val_dataloaders=val_loader)',\n",
              " 'D_kwDOCqWgoM4AONiMcon': 'Hey,\\nI\\'ve been using both, PyTorch Lightning/CLI and jsonargparse for quite a while. Yet, I haven\\'t found a simple method to instantiate a specific DataModule whose parameters are set in a config.yaml that was used during training. I have 2 workarounds which are both unsatisfying:\\nWorkaround 1 - Reuse CLI\\nDefine an instantiate-only CLI and use .datamodule\\nclass InstantiateOnlyLightningCLI(LightningCLI): # probably unnecessary in current RC: run=False\\n    def fit(self) -> None:\\n        return None\\n\\ncli = InstantiateOnlyLightningCLI(\\n        wave.WaveNet,\\n        pl.LightningDataModule,\\n        subclass_mode_model=False,\\n        subclass_mode_data=True,\\n    )\\ncli.datamodule\\n\\nDownsides\\n\\ntrainer and model are instantiated although not used\\n--help is populated with many unneeded parameters\\n\\nWorkaround 2 - Load Yaml directly\\nWhen the actual datamodule is known then\\nwith open(config, \"r\") as f:\\n  plcfg = yaml.safe_load(f.read())\\n  datamodule = SpecificDataModule(**plcfg[\"data\"][\"init_args\"])\\n\\nDownsides\\n\\nNo parameter validation\\nNo class arguments (i.e transformation classes) supported\\n\\nDoes anyone have a better solution?',\n",
              " 'D_kwDOCqWgoM4AONtAcon': 'hi all,\\nMy model validation code (see below) appears to leak memory which leads to a rapid increase in GPU memory usage and, eventually, to an OOM error right before the validation loop is about to complete (about 90% done or so). CUDA memory usage hovers around 8-9GB during training, then increases rapidly to ca. 15+GB during validation, hitting the memory limit of my GPU card.\\nWhat am I doing wrong here?\\nclass Lightning_WGAN_GP(pl.LightningModule):\\n    \"\"\"Conditional Wasserstein GAN with gradient penalty.\"\"\"\\n \\n    # (...)\\n\\n    def _get_noise(self, X: torch.Tensor) -> torch.Tensor:\\n        bs, _, h, w = X.shape\\n        return torch.randn(bs, 1, h, w).type_as(X)\\n\\n    def validation_step(self, batch: Tuple[Dict, ...], batch_idx: int) -> Dict:\\n        del batch_idx  # not used\\n        X, X_hr, real = batch[0][\"X_lr\"], batch[0][\"X_hr\"], batch[1][\"y\"]\\n\\n        with torch.no_grad():\\n            noise = self._get_noise(X)\\n            fake = self.gen(noise, X, X_hr)  # calling the generator\\n            loss_gen_val = F.l1_loss(fake, real)   # generator loss\\n            disc_real = self.disc(X, real, X_hr).reshape(-1)  # calling the discriminator\\n            disc_fake = self.disc(X, fake, X_hr).reshape(-1)\\n            loss_disc_val = -torch.mean(disc_real) + torch.mean(disc_fake)  # discriminator loss\\n            \\n        self.log(\"gen_val_loss\", loss_gen_val, on_epoch=True, on_step=False, prog_bar=True, logger=True)\\n        self.log(\"disc_val_loss\", loss_disc_val, on_epoch=True, on_step=False, prog_bar=True, logger=True)\\n\\n        return {\"gen_val\": loss_gen_val, \"disc_val\": loss_disc_val, \"batch\": batch}\\n\\n\\nRuntimeError: CUDA out of memory. Tried to allocate 266.00 MiB (GPU 0; 16.00 GiB total capacity; 12.84 GiB already allocated; 96.55 MiB free; 13.52 GiB reserved in total by PyTorch)\\n\\nDecreasing (or increasing) the validation batch size doesn\\'t make the problem go away. Any thoughts?\\n$ conda list | grep pytorch\\npytorch                   1.9.1           cuda102py38ha031fbe_3    conda-forge\\npytorch-gpu               1.9.1           cuda102py38hf05f184_3    conda-forge\\npytorch-lightning         1.5.3              pyhd8ed1ab_0    conda-forge\\n\\nLater edit: Skipping the validation loop, i.e.,\\ngan_trainer.fit(gan_model, train_dataloaders=dl_train)\\n\\ngets rid of the OOM error (the trainer makes it past the 1st epoch).\\nAlso, I am running in mixed precision (although i suspect precision doesn\\'t have much to do with this issue?)\\nThank you!',\n",
              " 'D_kwDOCqWgoM4AOO-Ycon': \"AttributeError: module 'pytorch_lightning' has no attribute 'metrics'\\nwhat is this...? T.T\",\n",
              " 'D_kwDOCqWgoM4AOOukcon': 'Hallo,\\nI am trying to train a BERT model with the squad dataset.\\nSince the model has a maximum input length, examples with longer contexts must be split into several samples.\\nIn the validation_epoch_end() and test_epoch_end() methods I want to compute the squad metric (https://github.com/PyTorchLightning/metrics/blob/master/torchmetrics/text/squad.py) that needs the predicted answers in text format.\\nBut for each sample the model predicts only the start and end token.\\nTo generate the answers in text format I need the dataset from the val/test_dataloader as well as the original val/test_dataset (with text instead of input_ids, offset_mappings etc.)\\nSo I was wondering how to access the original dataset in those methods.\\nOf course I can use self.trainer.datamodule.original_val_dataset but then I need to ensure that the trainer uses a datamodule and not only dataloaders. What is the best practice for this case?\\nAnd how do I use this model in production? Can I compute the text answers in the LightningModule and return them when predict() is called ?\\nOr should predict() only return the start and end tokens and the text answer is computed outside of the LightningModule?\\nThank you in advance!',\n",
              " 'D_kwDOCqWgoM4AOOv9con': 'I\\'ve stumbled upon the problem of not being able to use accumulate_grad_batches argument in the Trainer as I was doing manual optimization in my LightningModule to use adversarial loss functions.\\nHowever, I think it would be possible to implement something that would \"store\" calls to the step method for the module\\'s optimizers and actually apply them once every accumulate_grad_batches iterations. I\\'ve seen several related issues about similar behavours when overriding optimizer_step or close to my use case (#5054, #5108). The proposed fixes always leave some manual get-arounds in the final code.\\nMy question: is there a reason for such incompatibility of accumulate_grad_batches with manual optimization ?\\nOne reason might be the need to step different optimizers at different paces (one every batch, another every n batches ...) but this seems to be an extreme use case.',\n",
              " 'D_kwDOCqWgoM4AOP1kcon': 'I would like to save the top-10 checkpionts along training. By checking documentations, setting save_top_k, monitor and mode options in ModelCheckpoint jointly seem to do the job.\\nBut I am not sure what are the parameters available for the this callback to monitor. Are they logged values saved during training_step() or validation_step() through self.log(\"loss\", XYZ)?\\nThank you in advance!',\n",
              " 'D_kwDOCqWgoM4AOPZacon': \"I would like to use Adafactor as my optimizer with LightningCLI. I've tried the method described in the documentation for custom optimizers but it didn't work. Can anybody tell me how they would train a model with this optimizer using LightningCLI?\",\n",
              " 'D_kwDOCqWgoM4AOPhEcon': 'Can I define multi training dataloaders in LightningDataModule?\\n    def train_dataloader(self):\\n        lab_loader = torch.utils.data.DataLoader(\\n                         self.train_subset_lab_train,\\n                         batch_size=self.batch_size,\\n                         shuffle=True,\\n                         num_workers=self.num_workers,\\n                         pin_memory=True,\\n                         drop_last=True,\\n                    )\\n        unlab_loader = torch.utils.data.DataLoader(\\n                         self.train_subset_unlab_train,\\n                         batch_size=self.batch_size*self.uratio,\\n                         shuffle=True,\\n                         num_workers=self.num_workers,\\n                         pin_memory=True,\\n                         drop_last=True,\\n                    )\\n\\n        return [lab_loader, unlab_loader]',\n",
              " 'D_kwDOCqWgoM4AORWwcon': 'I call trainer.fit(model=model_lit, datamodule=datamodule)\\nI have set parameters:\\nlimit_val_batches: 0\\nlimit_test_batches: 0\\n\\ndatamodule.val_dataloader() returns None\\nBut it is still trying to perform validation...\\nIs it a problem of lightning, or am I doing something wrong? :)',\n",
              " 'D_kwDOCqWgoM4AOSKCcon': 'I\\'m training a model across two GPUs on patient data (id). In my test steps, I output dictionaries, which contain the id, as well as all the metrics. I store these (a list with a dict per id) at the end of the test epoch, so I can later on statistically evaluate model performances.\\nI\\'m experiencing a problem with the test step, however.\\n# Test step\\ndef test_step(self, batch, batch_idx):\\n\\n    # Get new input and predict, then calculate loss\\n    x, y, id = batch[\"input\"], batch[\"target\"], batch[\"id\"]\\n\\n    # Infer and time inference\\n    start = time()\\n    y_hat = self.test_inference(x, self, **self.test_inference_params)\\n    end = time()\\n\\n    # Calculate metrics\\n    id = id[0] if len(id) == 1 else tuple(id)\\n\\n    # Output dict with duration of inference\\n    output = {\"id\": id, \"time\": end - start}\\n\\n    # Add other metrics to output dict\\n    for m, pars in zip(self.metrics, self.metrics_params):\\n\\n        metric_value = m(y_hat, y, **pars)\\n\\n        if hasattr(metric_value, \"item\"):\\n            metric_value = metric_value.item()\\n\\n        output[f\"test_{m.__name__}\"] = metric_value\\n\\n    return output\\n\\n# Test epoch end (= test end)\\ndef test_epoch_end(self, outputs):\\n\\n    # Go over outputs and gather\\n    self.test_results = outputs     #self.all_gather(outputs)\\n\\nI hadn\\'t considered this before (as I\\'m used to training on a single GPU), but the test_results attribute now only contains half of the outputs (one half per process). So when my main script reaches this section, only half the output is effectively stored:\\nlog(\"Evaluating model.\")\\ntrainer.test(model=model,\\n             dataloaders=brats.val_dataloader())\\nresults = model.test_results\\n\\n# Save test results\\nlog(\"Saving results.\")\\nnp.save(file=join(result_dir, f\\'{model_name}_v{version}_fold{fold_index}.npy\\'), arr=results)\\n\\nI have read about the self.all_gather method, but I\\'m not sure it suits my needs. I want to merge the lists, not reduce anything. Also, they\\'re not Tensors, but dicts. How can I store all dicts across both DDP processes?',\n",
              " 'D_kwDOCqWgoM4AOSKgcon': 'Hi there,\\nI am using the ModelCheckpoint callback to save my model every n epochs but I cannot find a way to prevent PL from overwriting/deleting the previous checkpoint.\\nIdeally, I would like to keep the default naming convention {epoch}-{step} but without losing previous checkpoints.\\nThanks',\n",
              " 'D_kwDOCqWgoM4AOT-Tcon': \"Hello everyone, I'm currently implementing a Wasserstain type of GAN using Gradient Penalty. I want to save the checkpoints monitoring the negative critic loss, which starts from low values, increases to higher values in the first epochs and then decreases reaching almost 0. A plot of this loss can be seen in the paper: https://arxiv.org/pdf/1704.00028.pdf\\nThe problem is that if I use ModelCheckpoint and set the monitor parameter to negative critic_loss and mode = 'min', it basically saves the first epoch only. However I don't want to consider the training start epochs, when the negative loss increase, but only the epochs when the loss decrease.\\nI'm currently using multi-gpu training\\nHow can I implement this? Should I override the function on_train_epoch_end and save there the checkpoints, after checking the above criteria? Or should I use a lightning Callback? If so how can I acces to the monitored values?\\nThanks in advance\",\n",
              " 'D_kwDOCqWgoM4AOUM0con': 'Hello, I\\'m trying to validate my model on multiple subsets of the initial validation set to compare performance. Reading this page I got the idea that returning a list contaning the multiple Dataloaders would be enough. My val_dataloader method became the following:\\n\\nBut this isn\\'t working properly. I get the following error: \"TypeError: validation_step() takes 3 positional arguments but 4 were given\"\\n(it worked properly when I only used 1 validation Dataloader).\\nWhat am I doing wrong? Can someone help me with this? Or just point me to some more documentation on this.\\nThanks in advance!',\n",
              " 'D_kwDOCqWgoM4AOUfJcon': 'Hello, is there a way to call validation for every N global steps? For example, we could have vall_check_interval greater than the number of batches in the training dataset.',\n",
              " 'D_kwDOCqWgoM4AOVh6con': \"Hi I'm newbie for pytorch-lightning.\\nPlease teach me about this topic.\\nI want to process 100,000 records. So I set max_step = 100,000.\\nAnd to speed up learning, I also set strategy = ddp and use 4 GPUs with single node.\\nBut when I observe behavior of pytorch-lightning, it seems process 400,000 records.\\nbut step number is still 100,000 steps.\\nIs there any recognition that the number of steps specified when using multiple GPUs needs to be divided by the number of GPUs with the expected number of steps?\\n(on above example, should I set max_step to 25,000? )\\n(my pytorch-lightinng version is 1.5.4)\",\n",
              " 'D_kwDOCqWgoM4AOW_ycon': 'I usually set seed_everything() at the beginning of my script but this does not always solves the problem.\\nWhen I train models on cpu, it is determinstic; but when I switch to gpu, it becomes non-deterministic.\\nWhen I am training simple model, like one-layer LSTM, it is deterministic both on cpu and gpu.\\nBut when I train a more completed model like LSTM-FCN, it is deterministic on cpu but not on gpu\\nCan I get any help on debugging?\\nI got my LSTM-FCN model from here (https://github.com/timeseriesAI/tsai/blob/main/tsai/models/RNN_FCN.py) and the LSTM model I tested was simply a nn.LSTM',\n",
              " 'D_kwDOCqWgoM4AOXglcon': 'LightningModule can be coupled with various callbacks? I wonder if it is possible LightningLite can also reuse those callbacks, such as wandb, checkpoint?',\n",
              " 'D_kwDOCqWgoM4AOXqQcon': 'My question is like title. Thank you!',\n",
              " 'D_kwDOCqWgoM4AOY0ycon': \"class Mynet(pl.LightningModule)\\n    ... ...\\n    def training_step(self, batch, batch_idx):\\n        x, y = batch\\n        logits = self(x)\\n        loss = F.nll_loss(logits, y)\\n        accu = ((torch.argmax(logits, dim=1) == y).sum()/x.shape[0]).item()   \\n        return {'loss':loss, 'accuracy':accu}\\ni define training_step like this, the progress bar show loss,  but not show accu.\\ni want to know why?\\npytorch_lightning.version = 1.5.7\",\n",
              " 'D_kwDOCqWgoM4AOYR-con': \"Hi, I am trying to make my code invariant to the choice of strategies by being able to compute the global batch size which depends on the strategy. For example, for DDP it is N * batch_size with N being the number of processes.\\nThe use case I can think of is using the global batch size to initialize the optimizer.\\ntrainer(num_nodes=1, gpus=2, strategy='ddp') # pass the strategy ddp for example\\n\\nclass MyLightningModule(pl.LightningModule):\\n    @property\\n    def global_batch_size(self) -> int:\\n        if self.trainer.strategy is None:\\n            return self.trainer.datamodule.train.loader.batch_size\\n        elif self.trainer.strategy is DDPStrategy:\\n            return self.trainer.num_nodes * self.trainer.gpus *\\\\       # There might be a better way to compute the\\n                      self.trainer.datamodule.train.loader.batch_size  # number of processes using the strategy\\n        ...\\n\\n    def configure_optimizers(self) -> Dict[Any, Any]:\\n        optimizer, scheduler = hydra.utils.instantiate(\\n            self.hparams.optimizer, model=self, batch_size=self.global_batch_size, _recursive_=False)\\n\\n        return {\\n            'optimizer': optimizer,\\n            'lr_scheduler': scheduler\\n        }\\nTo do so, I would like to retrieve inside my Lightning module the strategy used by my trainer. I tried to find in the trainer code how to access the strategy and I found the property:\\n# in pytorch_lightning.trainer.trainer.py\\nclass Trainer(...):\\n    ...\\n    @property\\n    def strategy(self) -> Strategy:\\n        return self._accelerator_connector.strategy\\nHowever self.trainer.strategy in configure_optimizers raises AttributeError: 'Trainer' object has no attribute 'strategy'.\\nWeirdly, self.trainer._accelerator_connector.strategy works and returns the passed strategy in the trainer. Yet, if I understood correctly the _accelerator_connector should resolve the strategy 'ddp' to DDPStrategy in its initialization but it returns 'ddp':\\n# in pytorch_lightning.trainer.connectors.accelerator_connector.py\\n\\nclass AcceleratorConnector(...):\\n    def __init__(...):\\n        ...\\n        self.strategy = self.final_strategy()\\n     ...\\nIs it possible to access the strategy used for training?\",\n",
              " 'D_kwDOCqWgoM4AOYvdcon': \"I'm trying to incorporate the pytorch_ema library into the PL training loop. I found one topic relating to using pytorch_ema in lightning in this discussion thread, but how would this work if i want to save a model checkpoint based on the EMA weights? for example if i want to save the model weights using just pytorch, i could do something like\\n# using accuracy as an example\\nif current_val_acc >= best_val_acc:\\n    with ema.average_parameters():\\n        torch.save(model.state_dict(), saved_model_pth)\\n\\nso that i save the smoothed weights, but restore the original weights to the model so it doesn't affect training\\none workaround i can think of is to create my own model saving logic in the validation_epoch_end instead of relying on the ModelCheckpoint callback, but that seems to be a bit hacky. are there any potentially better solutions?\",\n",
              " 'D_kwDOCqWgoM4AOZEQcon': 'Hi,\\nI\\'ve currently refactored a part of code to use Pytorch Lightning instead of a regular pytorch script. I find a 35% speed up on a toy dataset, where the model had 20.9 K parameters.\\nHowever, when trying a bigger version of the model (9.1 M params), the initialization of the model takes a lot more time (even before lightning prints the number of params). I\\'ve managed to nail it down to the self.save_hyperparameters() function in init, since the initialization is instantaneous without calling self.save_hyperparameters(), but takes more than a full minute when calling it.\\nRemoving self.save_hyperparameters() causes an issue in my code since I am calling  model.load_from_checkpoint() afterward. Would you have any thoughts on how I can speed up the code? The \"regular\" pytorch model manages to initialize, train and save the model faster than the Lightning one due to this slow down ...\\nThanks!',\n",
              " 'D_kwDOCqWgoM4AOZMwcon': \"i use self.training param to judge what data return.\\nreturn x if self.training else (torch.cat(z, 1), x)\\nbut when i load my model, i use debug mode find that the self.training is save True.\\nself.model = CustomModel.load_from_checkpoint(model_path)\\nself.model.training = False\\ni use above code change model.training status, but its not work\\nthis is my inference full code:\\nclass CustomModelInference:\\n    def __init__(\\n            self,\\n            model_path: str,\\n            conf_thres: float = 0.25,\\n            iou_thres: float = 0.45,\\n            max_det: int = 1000,\\n            device: str = 'cuda:0',\\n            need_classes: list | None = None\\n    ):\\n        self.conf_thres = conf_thres\\n        self.iou_thres = iou_thres\\n        self.max_det = max_det\\n        self.device = device\\n        self.need_classes = need_classes\\n\\n        self.model = CustomModel.load_from_checkpoint(model_path)\\n        self.model.training = False\\n        self.model.to(device)\\n        self.stride = int(self.model.stride.max())\\n        self.names = self.model.names\\n        self.imgsz = self.model.imgsz\\n\\n    @torch.no_grad()\\n    def infer(self, img: np.ndarray):\\n        imgsz = check_img_size(self.imgsz, s=self.stride)\\n        cudnn.benchmark = True\\n        img = letterbox(img, imgsz, stride=self.stride, auto=True)[0]\\n        # img = np.stack(img, 0)\\n        if len(img.shape) == 3:\\n            img = img[None]\\n        img = img[..., ::-1].transpose((0, 3, 1, 2))\\n        img = np.ascontiguousarray(img)\\n        img = torch.from_numpy(img).to(self.device)\\n        img = img.float()\\n        img = img / 255.0\\n\\n        out, train_out = self.model(img)\",\n",
              " 'D_kwDOCqWgoM4AOa1kcon': 'According to the manual_backward() documentation, it takes care of scaling when using mixed precision. In that case, is it correct to assume one can simply and safely use loss.backward() during manual optimization if not using mixed precision?',\n",
              " 'D_kwDOCqWgoM4AOaI0con': 'Hi!\\nI’m working on a LSTM to predict price changes. The data has to be transformed (standardized) when training/validering and later inverse-transformed when predicting in production.\\nI’m using the LightningModule as well as the LightingDataModule, but I’m not sure where to apply the StandardScaler’s transform and more specifically; where to save the scaler-parameters and where to apply the inverse-transform on the predictions. And ideeas?\\n// R',\n",
              " 'D_kwDOCqWgoM4AObNjcon': \"The documentation about the on_before_optimizer_step hook mentions that:\\n\\nThe hook is only called if gradients do not need to be accumulated.\\n\\nHowever, in theory, combining both seems possible to me.\\nI took a look at PR #8048 which is the implementation of this hook but it didn't seem clear there.\",\n",
              " 'D_kwDOCqWgoM4AOb_Zcon': 'Hello,\\nI had an error where the training_step() was not run properly. I just found out the cause was because of the optimizer_step(). My training step immediately runs after I commented out optimizer_step().\\nSome other users also experienced the same error as described here: https://stackoverflow.com/questions/66756245/training-step-not-executing-in-pytorch-lightning\\nMy question is: Now that training_step() is running, but my train_loss is explosive because of the lack of a learning rate scheduler, henceforth, what can I implement to re-enable back my learning rate scheduler?\\nHere\\'s my chunk of code:\\n    def configure_optimizers(self):\\n        \\n        \"\"\"\\n        AdamW Optimizer lr=0.0006\\n\\n        \"\"\"        \\n        optimizer = optim.AdamW(self.parameters(),\\n                               lr=self.lr,\\n                               weight_decay=0.01 # Default\\n                               )\\n        \\n        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\\n            optimizer,\\n            mode=\\'min\\',\\n            factor=0.1,\\n            patience=2,\\n            min_lr=1e-6,\\n            verbose=True\\n        )\\n\\n        return optimizer\\n    \\n\\n        \\n    #def optimizer_step(self, epoch=None,\\n    #                   batch_idx=None,\\n    #                   optimizer=None,\\n    #                   optimizer_idx=None,\\n    #                   optimizer_closure=None,\\n    #                   on_tpu=None,\\n    #                   using_native_amp=None,\\n    #                   using_lbfgs=None,\\n    #                   second_order_closure=None):              \\n    #    \\n    #    if batch_idx == 0: # to call the scheduler after each validation\\n    #        \\n    #        self.scheduler.step(self.epoch_val_loss)\\n    #        \\n    #        print(f\\'metric: {self.epoch_val_loss}, \\\\\\n    #              best: {self.scheduler.best}, \\\\\\n    #                  num_bad_epochs: {self.scheduler.num_bad_epochs}\\') # for debugging\\n    #    \\n    #    optimizer.step()\\n    #    \\n    #    optimizer.zero_grad()\\n\\nThank you!',\n",
              " 'D_kwDOCqWgoM4AOd26con': 'Hi.\\nI\\'m trying to come up with ways to get my validation loss shown in the progress bar. My model is defined like this:\\nclass DummyNet(pl.LightningModule):\\n    def __init__(self, batch_size):\\n        super().__init__()\\n        self.batch_size = batch_size\\n        self.fc = nn.Sequential(\\n            nn.Dropout(0.5),\\n            nn.Linear(512, 2)\\n        )\\n        # loss\\n        self.loss_fn = nn.CrossEntropyLoss()\\n        # metrics\\n        metrics = torchmetrics.MetricCollection(\\n            {\\n                \"accuracy\": torchmetrics.Accuracy(),\\n                \"precision\": torchmetrics.Precision(),\\n                \"recall\": torchmetrics.Recall(),\\n                \"auc\": torchmetrics.AUC(reorder=True),\\n            }, \\n        )\\n        self.f1 = nn.ModuleDict({\\n            \"train_f1\": torchmetrics.F1(),\\n            \"val_f1\": torchmetrics.F1(),\\n            \"test_f1\": torchmetrics.F1(),\\n        })\\n        self.metrics = nn.ModuleDict({\\n            f\"{k}_metrics\": metrics.clone(prefix=k) for k in \"train val test\".split()\\n        })\\n    def forward(self, x):\\n        x = self.fc(x)\\n        return x\\n    \\n    def loop_step(self, batch, stage):\\n        x, targets = batch[\"windows\"], batch[\"diagnosis\"]\\n        logits = self(x)\\n        loss = self.loss_fn(logits, targets)\\n        preds = logits.argmax(-1)\\n        # computing metrics\\n        f1_str = f\"{stage}_f1\"\\n        metric_str = f\"{stage}_metrics\"\\n        self.f1[f1_str](preds, targets)\\n        self.metrics[metric_str](preds, targets)\\n        # logging metrics\\n        on_step = False if stage != \"train\" else True\\n        self.log(f1_str, self.f1[f1_str], on_step=on_step, on_epoch=True)\\n        self.log_dict(self.metrics[metric_str], on_step=False, on_epoch=True)   \\n        self.log(f\"{stage}_loss\", loss, on_step=on_step, on_epoch=True)\\n        return loss\\n    \\n    def training_step(self, batch, batch_idx):\\n        return self.loop_step(batch, \"train\")\\n    \\n    def validation_step(self, batch, batch_idx):\\n        return self.loop_step(batch, \"val\")\\n\\n    def testing_step(self, batch, batch_idx):\\n        return self.loop_step(batch, \"test\")\\n\\n    def configure_optimizers(self):\\n        return torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=0.01)\\n\\nBut as of now none of my metrics nor my validation loss comes up in the progress bar. Is it because I\\'m returning loss in the dictionary and not \"{stage}_loss\"?\\nThank you.',\n",
              " 'D_kwDOCqWgoM4AOd6Scon': \"Greetings. I am getting NaN val loss Cannot log infinite or NaN value to attribute training/val_loss/ with cnnlstm network.I am thinking to use gradient clipping.\\nBut the doc say gradient clipping should not be used with mixed precision.\\nIf using mixed precision, the gradient_clip_val does not need to be changed as the gradients are unscaled before applying the clipping function.\\n\\nFurther, i am doing regression, i  dont know what value of gradient clipping should i use?\\nFurther i checked trainer doc and find that\\ntrack_grad_norm\\n(Union[int, float, str]) – -1 no tracking. Otherwise tracks that p-norm. May be set to ‘inf’ infinity-norm. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them.\\n\\nCan you explain what this mean 'If using Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them'\",\n",
              " 'D_kwDOCqWgoM4AOdFscon': \"I'm confused about two API:\\n\\nModule.load_from_checkpoint\\ntrainer.resume_from_checkpoint\",\n",
              " 'D_kwDOCqWgoM4AOdZ5con': 'I am working with pytorchvideo with pytorch lightning but it say UserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.\\nfrom pytorchvideo.models.resnet import create_resnet\\nclass OurModel(LightningModule):\\n    def __init__(self):\\n        super(OurModel,self).__init__()\\n        self.model =  create_resnet(\\n                      input_channel=3, # RGB input from Kinetics\\n                      model_depth=50, # For the tutorial let\\'s just use a 50 layer network\\n                      model_num_class=1, # Kinetics has 400 classes so we need out final head to align\\n                      norm=nn.BatchNorm3d,\\n                      activation=nn.ReLU)\\n\\n    def forward(self,x):\\n        return self.model(x)\\n\\n  \\n    def val_dataloader(self):\\n        val_dataset=LabeledVideoDataset(labeled_video_paths=\\\\\\n                    list(zip(val_df.vidpath,val_df.pulse)),\\n                   clip_sampler=make_clip_sampler(\"uniform\", 2),\\\\\\n                    transform=train_transform,  decode_audio=False)\\n        \\n        val_loader=DataLoader(val_dataset,shuffle=False,\\n                   batch_size=self.batch_size,\\n                   num_workers=self.numworker,\\n                   pin_memory=True)\\n        return val_loader\\n    \\n    def validation_step(self,batch,batch_idx):\\n        out = self(batch[\"video\"]).flatten()\\n        loss=self.criterion(out,batch[\"label\"].float())\\n        mae=self.metric(out,batch[\"label\"].float())\\n        return {\\'loss\\':loss,\\'mae\\':mae.detach()}\\n\\n\\nA you can see, shuffle is False. but it keep me giving warning that\\nUserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.\\n\\nSorry, i am not sure whether i had to ask it at pytorchvideo or here',\n",
              " 'D_kwDOCqWgoM4AOeB9con': 'I have created an issue on this. Moderators, please delete this discussion',\n",
              " 'D_kwDOCqWgoM4AOemVcon': \"I have used the below code\\n\\n\\nDataclass below\\n\\n\\n\\nModel Class below\\n\\n\\n\\nTrainnig below\\n\\n\\n\\nI have trained the model for 20 epochs, After that trainer. test( ) didn't run. I Got the Following error lines :\\n\\n\\nI have also tried the way suggested in Doc here But still, the error exists. This happening mostly in Google collab and in AwsInstance. In Local, It works sometimes and sometimes not.\\nPlease let me know what am I missing here.\",\n",
              " 'D_kwDOCqWgoM4AOgOwcon': 'I would like to change the names of the logging directories from the default \"version_{n}\" to something of my own choosing. How can I do this using command-line arguments to LightningCLI?\\nI know I can set the logger using trainer.logger but setting logger args e.g. trainer.logger.version does not work (unrecognized argument). So how can I pass args to the logger?',\n",
              " 'D_kwDOCqWgoM4AOgPGcon': \"Hello,\\nI implemented MoCo in Pytorch lightning. I was surprised to see that my lightning version was slower than Pytorch's and I ran the profiler to check which function is slow. I can't share all my code but here are the relevant parts:\\nclass MoCoModel(LightningModule):\\n    def __init__(\\n        ...\\n    ) -> None:\\n        ...\\n\\n        self.register_buffer('queue', torch.randn(queue.feature_dim, queue.size))\\n        self.queue = nn.functional.normalize(self.queue, dim=0)\\n        self.register_buffer('queue_ptr', torch.zeros(1, dtype=torch.long))\\n    \\n    @torch.no_grad()\\n    def _update_queue(self, x: Tensor) -> None:\\n        x = self.concat_all_gather_without_backprop(x)\\n\\n        #batch_size = x.shape[0]\\n        batch_size = self._get_batch_size(x)\\n\\n        # for simplicity\\n        ptr = self._get_ptr()\\n        #ptr = int(self.queue_ptr)\\n\\n        self._assert(batch_size)\\n        #assert self.queue_size % batch_size == 0\\n\\n        # replace the keys at ptr (dequeue and enqueue)\\n        x = self._transpose(x)\\n        self._assign_in_queue(x, ptr, batch_size)\\n        #self.queue[:, ptr: ptr + batch_size] = x.T\\n\\n        # move pointer\\n        ptr = self._compute_ptr(ptr, batch_size)\\n        self._assign_ptr(ptr)\\n        #ptr =  (ptr + batch_size) % self.queue_size\\n    \\n    def _get_batch_size(self, x):\\n        return x.shape[0]\\n\\n    def _get_ptr(self):\\n        return int(self.queue_ptr)\\n\\n    def _assert(self, batch_size):\\n        assert self.queue_size % batch_size == 0\\n    \\n    def _assign_ptr(self, ptr):\\n        self.queue_ptr[0] = ptr\\n    \\n    def _compute_ptr(self, batch_size, ptr):\\n        return (ptr + batch_size) % self.queue_size\\n\\n    def _transpose(self, x):\\n        return x.T\\n    \\n    def _assign_in_queue(self, x, ptr, batch_size):\\n        self.queue[:, ptr: ptr + batch_size] = x\\n\\n    def training_step(self, batch):\\n        ...\\n        self._update_queue(k)\\nHere is the output of running simple profiler:\\nAction                             \\t|  Mean duration (s)\\t|Num calls      \\t|  Total time (s) \\t|  Percentage %   \\t|\\n--------------------------------------------------------------------------------------------------------------------------------------\\nTotal                              \\t|  -              \\t|_              \\t|  53.595         \\t|  100 %          \\t|\\n--------------------------------------------------------------------------------------------------------------------------------------\\nrun_training_epoch                 \\t|  45.224         \\t|1              \\t|  45.224         \\t|  84.381         \\t|\\nrun_training_batch                 \\t|  0.21673        \\t|195            \\t|  42.262         \\t|  78.854         \\t|\\noptimizer_step_with_closure_0      \\t|  0.20378        \\t|195            \\t|  39.738         \\t|  74.145         \\t|\\ntraining_step_and_backward         \\t|  0.19978        \\t|195            \\t|  38.957         \\t|  72.688         \\t|\\nmodel_forward                      \\t|  0.1909         \\t|195            \\t|  37.225         \\t|  69.457         \\t|\\ntraining_step                      \\t|  0.19077        \\t|195            \\t|  37.201         \\t|  69.411         \\t|\\nbackward                           \\t|  0.0083673      \\t|195            \\t|  1.6316         \\t|  3.0443         \\t|\\non_train_batch_end                 \\t|  0.0077772      \\t|195            \\t|  1.5166         \\t|  2.8296         \\t|\\nget_train_batch                    \\t|  0.0034326      \\t|196            \\t|  0.6728         \\t|  1.2553         \\t|\\nfetch_next_train_batch             \\t|  0.0034203      \\t|196            \\t|  0.67037        \\t|  1.2508         \\t|\\nzero_grad                          \\t|  0.00049274     \\t|195            \\t|  0.096084       \\t|  0.17928        \\t|\\nconfigure_optimizers               \\t|  0.093719       \\t|1              \\t|  0.093719       \\t|  0.17486        \\t|\\ntraining_batch_to_device           \\t|  0.00028381     \\t|195            \\t|  0.055342       \\t|  0.10326        \\t|\\non_train_batch_start               \\t|  0.00018134     \\t|195            \\t|  0.03536        \\t|  0.065977       \\t|\\non_train_start                     \\t|  0.033906       \\t|1              \\t|  0.033906       \\t|  0.063264       \\t|\\non_pretrain_routine_start          \\t|  0.006531       \\t|1              \\t|  0.006531       \\t|  0.012186       \\t|\\non_batch_start                     \\t|  3.062e-05      \\t|195            \\t|  0.0059708      \\t|  0.011141       \\t|\\non_after_backward                  \\t|  3.0163e-05     \\t|195            \\t|  0.0058817      \\t|  0.010974       \\t|\\non_before_optimizer_step           \\t|  2.989e-05      \\t|195            \\t|  0.0058285      \\t|  0.010875       \\t|\\non_batch_end                       \\t|  2.9087e-05     \\t|195            \\t|  0.005672       \\t|  0.010583       \\t|\\non_before_zero_grad                \\t|  2.8804e-05     \\t|195            \\t|  0.0056167      \\t|  0.01048        \\t|\\non_before_backward                 \\t|  2.6982e-05     \\t|195            \\t|  0.0052616      \\t|  0.0098172      \\t|\\non_train_epoch_end                 \\t|  0.0014064      \\t|1              \\t|  0.0014064      \\t|  0.0026241      \\t|\\ntraining_step_end                  \\t|  4.9198e-06     \\t|195            \\t|  0.00095937     \\t|  0.00179        \\t|\\non_train_epoch_start               \\t|  0.00025167     \\t|1              \\t|  0.00025167     \\t|  0.00046957     \\t|\\non_train_end                       \\t|  0.00017067     \\t|1              \\t|  0.00017067     \\t|  0.00031844     \\t|\\non_before_accelerator_backend_setup\\t|  6.968e-05      \\t|1              \\t|  6.968e-05      \\t|  0.00013001     \\t|\\nsetup                              \\t|  5.0209e-05     \\t|1              \\t|  5.0209e-05     \\t|  9.3682e-05     \\t|\\nprepare_data                       \\t|  4.4779e-05     \\t|1              \\t|  4.4779e-05     \\t|  8.355e-05      \\t|\\non_fit_end                         \\t|  3.892e-05      \\t|1              \\t|  3.892e-05      \\t|  7.2618e-05     \\t|\\non_epoch_start                     \\t|  3.332e-05      \\t|1              \\t|  3.332e-05      \\t|  6.2169e-05     \\t|\\non_pretrain_routine_end            \\t|  3.009e-05      \\t|1              \\t|  3.009e-05      \\t|  5.6143e-05     \\t|\\non_epoch_end                       \\t|  2.741e-05      \\t|1              \\t|  2.741e-05      \\t|  5.1142e-05     \\t|\\non_configure_sharded_model         \\t|  2.556e-05      \\t|1              \\t|  2.556e-05      \\t|  4.7691e-05     \\t|\\non_fit_start                       \\t|  2.0869e-05     \\t|1              \\t|  2.0869e-05     \\t|  3.8938e-05     \\t|\\nteardown                           \\t|  1.9379e-05     \\t|1              \\t|  1.9379e-05     \\t|  3.6158e-05     \\t|\\nconfigure_sharded_model            \\t|  6.5197e-06     \\t|1              \\t|  6.5197e-06     \\t|  1.2165e-05     \\t|\\nconfigure_callbacks                \\t|  5.16e-06       \\t|1              \\t|  5.16e-06       \\t|  9.6277e-06     \\t|\\non_train_dataloader                \\t|  4.2003e-06     \\t|1              \\t|  4.2003e-06     \\t|  7.837e-06      \\t|\\n\\nAs we can see a large time is spent in training_step and here is the output of advanced profiler for this function:\\nProfile stats for: training_step rank: 0\\n         1065072 function calls (862519 primitive calls) in 37.086 seconds\\n\\n   Ordered by: cumulative time\\n\\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n      195    0.001    0.000   37.082    0.190 accelerator.py:210(training_step)\\n      195    0.000    0.000   37.079    0.190 ddp.py:438(training_step)\\n31980/195    0.053    0.000   37.079    0.190 module.py:1096(_call_impl)\\n      195    0.015    0.000   37.078    0.190 distributed.py:852(forward)\\n      195    0.002    0.000   36.629    0.188 base.py:76(forward)\\n      195    0.009    0.000   36.624    0.188 moco.py:201(training_step)\\n 1170/390    0.006    0.000   34.429    0.088 grad_mode.py:25(decorate_context)\\n      195    0.002    0.000   32.216    0.165 moco.py:83(_update_queue)\\n      195   32.171    0.165   32.171    0.165 moco.py:109(_get_ptr)\\n      390    0.000    0.000    3.942    0.010 resnet.py:268(forward)\\n     ...\\n      195    0.008    0.000    0.008    0.000 moco.py:124(_assign_in_queue)\\n      195    0.005    0.000    0.006    0.000 moco.py:115(_assign_ptr)\\n      195    0.002    0.000    0.002    0.000 moco.py:121(_transpose)\\n      195    0.001    0.000    0.001    0.000 gather.py:44(concat_all_gather_without_backprop)\\n      195    0.000    0.000    0.000    0.000 moco.py:106(_get_batch_size)\\n     ...\\n\\nThe function _update_queue is very long and the function taking the most time is _get_ptr which should be really fast in comparison with forwards or computation of MoCo loss. I watched lightning bolts implementation that uses the same kind of operations so I don't really understand why it is this slow.\\nI tested with DDP and SingleDevice strategy that resulted in the same kind of slow down on a SLURM cluster environment.\",\n",
              " 'D_kwDOCqWgoM4AOgZDcon': 'Hi,\\nWhen training a WGAN we update the discriminator several times for each update of the generator (a typical choice is 5 to 1).\\nIn PL we control this by setting the \"frequency\" parameter within the configure_optimizers function:\\nreturn ( {\\'optimizer\\': dis_opt, \\'frequency\\': 5}, {\\'optimizer\\': gen_opt, \\'frequency\\': 1} ) \\nNow, if the number of batches for each epoch is not divisible by the sum of frequencies (6 in this case), the generator will end up being trained less than the discriminator.\\nIf, for example, there are 11 batches in our dataset, it will result in the discriminator being updated 10 times and the generator only 1 for each epoch because the optimizers\\' order is reset at the beginning of each epoch.\\nIs there a workaround for this? The most useful solution would be to be able to save the number of updates across epochs.\\nThanks for any suggestion.',\n",
              " 'D_kwDOCqWgoM4AOkGzcon': 'When I use “resume from checkpoint”,\\nthere is a “CUDA out of memory” problem,\\nwhen using torch.load(), set \"map location\" to \"cpu\" can solve this problem,\\nin \"resume from checkpoint\" scenario, what should I do?',\n",
              " 'D_kwDOCqWgoM4AOoRXcon': 'Hey!\\nI\\'m trying to use LightningArgumentParser.link_arguments to link an argument from the Datamodule to the init_args of the LR scheduler with:\\nclass MyLightningCLI(LightningCLI):\\n    def add_arguments_to_parser(self, parser):\\n        # Set lr_scheduler\\'s num_training_steps from datamodule class\\n        parser.link_arguments(\\n            \"data\",\\n            \"lr_scheduler.init_args.num_training_steps\",\\n            compute_fn=lambda dm: dm.get_num_training_steps(),\\n            apply_on=\"instantiate\",\\n        )\\nand I get the following error:\\nValueError: No action for key \"lr_scheduler.init_args.num_training_steps\".\\nI was wondering if such thing is possible, or is linking to init_args is exclusive to the model and data classes?\\n\\n  Code to reproduce\\ntrainer.py\\nimport pytorch_lightning as pl\\nimport torch.nn\\nfrom pytorch_lightning.utilities.cli import LR_SCHEDULER_REGISTRY\\nfrom pytorch_lightning.utilities.cli import LightningCLI\\nfrom torch.optim import Optimizer\\nfrom torch.optim.lr_scheduler import LambdaLR\\nfrom torch.utils.data import DataLoader, Dataset\\n\\n\\nclass MyLightningCLI(LightningCLI):\\n    def add_arguments_to_parser(self, parser):\\n        # Set lr_scheduler\\'s num_training_steps from datamodule class\\n        parser.link_arguments(\\n            \"data\",\\n            \"lr_scheduler.init_args.num_training_steps\",\\n            compute_fn=lambda dm: dm.get_num_training_steps(),\\n            apply_on=\"instantiate\",\\n        )\\n\\n\\n@LR_SCHEDULER_REGISTRY\\nclass WarmupLR(LambdaLR):\\n    def __init__(\\n        self,\\n        optimizer: Optimizer,\\n        warmup_proportion: float,\\n        num_training_steps: int,\\n        last_epoch=-1,\\n    ) -> None:\\n        self.num_training_steps = num_training_steps\\n        self.num_warmup_steps = round(num_training_steps * warmup_proportion)\\n        super().__init__(optimizer, lr_lambda=self.lr_lambda, last_epoch=last_epoch)\\n\\n    def lr_lambda(self, current_step: int) -> float:\\n        if current_step < self.num_warmup_steps:\\n            return float(current_step) / float(max(1, self.num_warmup_steps))\\n        return max(\\n            0.0,\\n            float(self.num_training_steps - current_step)\\n            / float(max(1, self.num_training_steps - self.num_warmup_steps)),\\n        )\\n\\n\\nclass DataModule(pl.LightningDataModule):\\n    def __init__(self, name):\\n        super().__init__()\\n        self.length = len(name)\\n\\n    def train_dataloader(self):\\n        return DataLoader(Dataset())\\n\\n    def get_num_training_steps(self) -> int:\\n        return self.length\\n\\n\\nclass LitModel(pl.LightningModule):\\n    def __init__(self, num_labels):\\n        super().__init__()\\n        self.num_labels = num_labels\\n        self.nn = torch.nn.Linear(num_labels, num_labels)\\n\\n    def training_step(self, *args, **kwargs):\\n        return\\n\\n\\nif __name__ == \"__main__\":\\n    cli = MyLightningCLI(\\n        model_class=LitModel,\\n        datamodule_class=DataModule,\\n    )\\nconfig.yaml\\ndata:\\n  name: blablabla\\n\\nmodel:\\n  num_labels: 5\\n\\noptimizer:\\n  class_path: torch.optim.Adam\\n  init_args:\\n    lr: 0.01\\n\\nlr_scheduler:\\n  warmup_proportion: 0.1\\n\\ntrainer:\\n  max_epochs: 2',\n",
              " 'D_kwDOCqWgoM4AOp8Rcon': 'I want to implement a custom callback which calculates a custom metric and needs all of the outputs from the complete epoch. Is there any way to pass all the outputs to on_validation_epoch_end hook of the callback ?\\nHere\\'s the pseudo-code of the setup\\nclass FeedBackPrize(pl.LightningModule):\\n    def __init__(\\n        self,\\n        num_train_steps,\\n        steps_per_epoch,\\n        model_name: str = \"allenai/longformer-base-4096\",\\n        lr: float = 1e-5,\\n        num_labels: int = 16,\\n        multi_sample_dropout=True,\\n        step_scheduler_after: str = \"step\",\\n    ):\\n        super().__init__()\\n        self.learning_rate = lr\\n        self.model_name = model_name\\n        self.multi_sample_dropout = multi_sample_dropout\\n        self.num_train_steps = num_train_steps\\n        self.num_labels = num_labels\\n        self.steps_per_epoch = steps_per_epoch\\n        self.step_scheduler_after = step_scheduler_after\\n\\n        hidden_dropout_prob: float = 0.1\\n        layer_norm_eps: float = 1e-7\\n\\n        config = AutoConfig.from_pretrained(model_name)\\n\\n        config.update(\\n            {\\n                \"output_hidden_states\": True,\\n                \"hidden_dropout_prob\": hidden_dropout_prob,\\n                \"layer_norm_eps\": layer_norm_eps,\\n                \"add_pooling_layer\": False,\\n                \"num_labels\": self.num_labels,\\n            }\\n        )\\n\\n        self.transformer = AutoModel.from_pretrained(model_name, config=config)\\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\\n        self.dropout1 = nn.Dropout(0.1)\\n        self.dropout2 = nn.Dropout(0.2)\\n        self.dropout3 = nn.Dropout(0.3)\\n        self.dropout4 = nn.Dropout(0.4)\\n        self.dropout5 = nn.Dropout(0.5)\\n\\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\\n\\n    def forward(self, ids, mask, token_type_ids=None):\\n        transformer_out = self.transformer(ids, mask)\\n        sequence_output = transformer_out.last_hidden_state\\n        sequence_output = self.dropout(sequence_output)\\n\\n        if self.multi_sample_dropout:\\n            logits1 = self.output(self.dropout1(sequence_output))\\n            logits2 = self.output(self.dropout2(sequence_output))\\n            logits3 = self.output(self.dropout3(sequence_output))\\n            logits4 = self.output(self.dropout4(sequence_output))\\n            logits5 = self.output(self.dropout5(sequence_output))\\n\\n            logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\\n            logits = torch.softmax(logits, dim=-1)\\n            return logits\\n        else:\\n            return sequence_output\\n\\n    def configure_optimizers(self):\\n        param_optimizer = list(self.named_parameters())\\n        no_decay = [\"bias\", \"LayerNorm.bias\"]\\n        optimizer_parameters = [\\n            {\\n                \"params\": [\\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\\n                ],\\n                \"weight_decay\": 0.01,\\n            },\\n            {\\n                \"params\": [\\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\\n                ],\\n                \"weight_decay\": 0.0,\\n            },\\n        ]\\n        optimizer = AdamW(optimizer_parameters, lr=self.learning_rate)\\n\\n        scheduler = get_cosine_schedule_with_warmup(\\n            optimizer,\\n            num_warmup_steps=int(0.1 * self.num_train_steps),\\n            num_training_steps=self.num_train_steps,\\n            num_cycles=1,\\n            last_epoch=-1,\\n        )\\n        scheduler = {\\n            \"scheduler\": scheduler,\\n            \"interval\": self.step_scheduler_after,\\n            \"frequency\": 1,\\n        }\\n\\n        return [optimizer], [scheduler]\\n\\n    def _calculate_loss(self, outputs, targets, attention_mask):\\n        loss_fct = nn.CrossEntropyLoss()\\n\\n        active_loss = attention_mask.view(-1) == 1\\n        active_logits = outputs.view(-1, self.num_labels)\\n        true_labels = targets.view(-1)\\n        outputs = active_logits.argmax(dim=-1)\\n        idxs = np.where(active_loss.cpu().numpy() == 1)[0]\\n        active_logits = active_logits[idxs]\\n        true_labels = true_labels[idxs].to(torch.long)\\n\\n        loss = loss_fct(active_logits, true_labels)\\n\\n        return loss\\n\\n    def training_step(self, batch, batch_idx):\\n        ids, mask, targets = batch[\\'ids\\'], batch[\\'mask\\'], batch[\\'targets\\']\\n        outputs = self(ids, mask)\\n        loss = self._calculate_loss(outputs, targets, mask)\\n        return loss\\n    \\n    def validation_step(self, batch, batch_idx):\\n        ids, mask, targets = batch[\\'ids\\'], batch[\\'mask\\'], batch[\\'targets\\']\\n        outputs = self(ids, mask)\\n        loss = self._calculate_loss(outputs, targets, mask)\\n\\n        return {\\n            \"loss\": loss,\\n            \"preds\": outputs,\\n            \"targets\": targets\\n        }\\n    \\n    def validation_epoch_end(self, validation_step_outputs):\\n        preds = []\\n        targets = []\\n\\n        for output in validation_step_outputs:\\n            preds += output[\\'preds\\']\\n            targets += output[\\'targets\\']\\n        \\n        targets = torch.stack(targets) #torch.Size([2, 1536])\\n        preds = torch.stack(preds) # torch.Size([2, 1536, 15])\\n\\n        return {\\n            \"targets\": targets,\\n            \"preds\": preds\\n        }\\nCustom callback\\nclass CompMetricEvaluator(Callback):\\n    def __init__(self):\\n        pass\\n    def on_validation_epoch_end(self, trainer, pl_module):\\n\\n        print(\"After validation epoch [custom metric evaluation]\")\\n        # calculate custom metric here....',\n",
              " 'D_kwDOCqWgoM4AOqUScon': \"When trying to disable find_unused_parameters in the trainer by doing the following,\\nstrategy=DDPStrategy(find_unused_parameters=False)\\nAm being thrown an import error for from pytorch_lightning.strategies import DDPStrategy\\nError:\\nNo module named 'pytorch_lightning.strategies'\",\n",
              " 'D_kwDOCqWgoM4AOqojcon': 'Hello,\\nI´m running my model in a cluster with multiples GPUs (2). My problem is that I would like to access all the datapoints in the batch (predictions and labels). Because I´m using more than 2 GPUs, my batch in divided between those two devices for parallelisation purposes, which means than when I access the data in the batch in eval/training, I´m getting just half the batch.\\nHow could I obtain the complete batch and the predictions of the model that are divided among different devices/GPUs? @rohitgr7 suggested using self.all_gather, but after trying it on my LightningModule’s forward method, I get just half the batch, that is, just the data stored in one of the two GPUs being used.\\nThanks!\\nPD:  may it be possible to access this info through \"validation_epoch_end\", \"test_epoch_end\", etc?',\n",
              " 'D_kwDOCqWgoM4AOqpicon': \"Hello, I have some question about the self.log function and batch_size during the trainer.\\nIf I have two GPUs, and I want to train my model with batch_size 16 per GPU and I use DDP, so what's the number of batch_size in Datamodule and what's the number of batch_size in self.log, If I want to calculate my metrics correctly?\",\n",
              " 'D_kwDOCqWgoM4AOrGlcon': 'How does gradient accumulation interact with DeepSpeed learning rate scheduling (e.g. the per-step warm-up scheduler)? Is the learning rate updated after every iteration, or only after the model weights are ultimately updated?',\n",
              " 'D_kwDOCqWgoM4AOsH4con': \"I would like to create a hook that automatically uploads checkpoints to the cloud (e.g., AWS, Azure) when they're created. I tried using on_save_checkpoint roughly like this:\\ndef on_save_checkpoint(self, trainer: pl.Trainer, pl_module: pl.LightningModule, checkpoint: Dict[str, Any]) -> dict:\\n    checkpoint_bytes = io.BytesIO()\\n    torch.save(checkpoint, checkpoint_bytes)\\n    # Upload the BytesIO somewhere...\\n\\nHowever, states for optimizers, schedulers, AMP, etc. are added after on_save_checkpoint hooks are called. Is there an elegant way to create a hook that receives the fully formed checkpoint state?\",\n",
              " 'D_kwDOCqWgoM4AOsH5con': 'I want to create a hook that uploads checkpoints to cloud storage (e.g. AWS, Azure). I tried using the on_save_checkpoint hook as follows:\\ndef on_save_checkpoint(self, trainer: pl.Trainer, pl_module: pl.LightningModule, checkpoint: Dict[str, Any]) -> dict:\\n    checkpoint_bytes = io.BytesIO()\\n    torch.save(checkpoint, checkpoint_bytes)\\n    # Upload the BytesIO...\\n\\nHowever, states for optimizers, learning rate schedulers, etc. are added to the checkpoint dict after on_save_checkpoint is called. Is there an elegant way to create a hook that receives fully formed checkpoints?\\nedit: sorry, this is a duplicate -- GitHub was giving me errors when I posted',\n",
              " 'D_kwDOCqWgoM4AOsPRcon': \"I want to enable dropout during .predict and tried implementing the following:\\nmodel.eval() \\nfor m in model.modules():\\n    if m.__class__.__name__.startswith('Dropout'):\\n        m.train()\\n                \\n...\\n\\ntrainer.predict(\\n    model, dataloaders=data_loader, return_predictions=True\\n)\\n\\nIt seems like .predict is overriding this because I get identical predictions with different seeds.\\nCan someone explain how to accomplish this, or point me to the relevant docs? (Couldn't find them & tried looking for while)\\nThank you!\",\n",
              " 'D_kwDOCqWgoM4AOsmIcon': 'Hi everyone, I am new to PyTorch lightening and I am currently trying to implement a continual learning model in PyTorch lightening.\\nI have multiple data loaders for different tasks and I want to train on all of these data loaders. After training on task1 with dataloader1 I want to update the parameters of the model which are going to be trained for task two. To do this, I have an attribute named current_task in my dataloader which decides the dataset from which the samples are generated for the current task. My datamodule looks something like this.\\n\\nclass RandSplitCIFAR100DataModule(LightningDataModule):\\n    def __init__(self):\\n        .....\\n\\n    def setup(self, stage: Optional[str] = None):\\n    \\n        # load datasets only if they\\'re not loaded already\\n        if not self.data_train and not self.data_val and not self.data_test:\\n            self.data_train = datasets.CIFAR100(self.hparams.data_dir, train=True, transform=self.train_transforms)\\n            self.data_val = datasets.CIFAR100(self.hparams.data_dir, train=False, transform=self.val_transforms)\\n        \\n        np.random.seed(self.hparams.seed)\\n        perm = np.random.permutation(self.num_classes)\\n        print(perm)\\n\\n        splits = [\\n            (self.partition_datasetv4(self.data_train, perm[5 * i:5 * (i+1)]),\\n            self.partition_datasetv4(self.data_val, perm[5 * i:5 * (i+1)]),)\\n            for i in range(self.hparams.num_tasks)\\n        ]\\n\\n        kwargs = {\"num_workers\": self.hparams.workers, \"pin_memory\": self.hparams.pin_memory}\\n        self.loaders = [\\n            (DataLoader(x[0], batch_size=self.hparams.batch_size, shuffle=True, **kwargs),\\n            DataLoader(x[1], batch_size=self.hparams.test_batch_size, shuffle=False, **kwargs),)\\n            for x in splits\\n        ]\\n\\n    def update_task(self, i):\\n        self.current_task = i\\n        \\n    def train_dataloader(self):\\n        return self.loader[self.current_task][0]\\n\\n    def val_dataloader(self):\\n        return self.loader[self.current_task][1]\\n\\nNow I want to have a training loop that does something like this.\\n\\nfor task in range(num_tasks):\\n    self.dataloder.update_task(task)\\n\\n    for n, p in model.named_parameters():\\n        # change parameters to update\\n    for epoch in range(max_epochs):\\n        for batch in dataloader:\\n            ....\\n\\n\\nI am currently not able to figure out how to go about this, I feel confident that lightening should be able to handle such cases but I am just not sure how to go about this.\\nAny help is greatly appreciated!\\nPrateek',\n",
              " 'D_kwDOCqWgoM4AOtq7con': \"We use CacheDataset MONAI CacheDataset to speed up data loading. However, when combining the lightning module's standard training code with DDP strategy and multi-GPU environment, the cached dataset is not working as expected:\\nIf provided with a full length of data in the CacheDataset, the initial epoch takes forever to load because each GPU will try to read in and cache ALL data, which is unnecessary because in DDP each GPU will only use a portion of the data.\\nA workaround is mentioned in here MONAI issue, which mentioning to partition data before feeding into the CacheDataset:\\nMONAI Tutorial\\nHowever, if I make the partitioning in the setup() function, the trainer will train for total_data_length // num_gpus samples each epoch instead of total_ data_length.\\nAnd if I put the CacheDataset with full data length in the prepare_data function, the subprocess's object can't access the dataset instance (saved in self.x, which is not recommended).\\nSo what's the best practical way to handle this? My gut feeling is that I should use the partitioned dataset on each GPU, and let the loader use the full length of dataset instead of part of it. Any suggestions?\",\n",
              " 'D_kwDOCqWgoM4AOtvtcon': 'In the docstring it says the save_dir is None, but then why does it return a path? Should we change either the docstring, or the implementation here?\\n\\n  \\n    \\n      pytorch-lightning/pytorch_lightning/loggers/neptune.py\\n    \\n    \\n        Lines 516 to 524\\n      in\\n      9d8faec\\n    \\n  \\n  \\n    \\n\\n        \\n          \\n               @property \\n        \\n\\n        \\n          \\n               def save_dir(self) -> Optional[str]: \\n        \\n\\n        \\n          \\n                   \"\"\"Gets the save directory of the experiment which in this case is ``None`` because Neptune does not save \\n        \\n\\n        \\n          \\n                   locally. \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n                   Returns: \\n        \\n\\n        \\n          \\n                       the root directory where experiment logs get saved \\n        \\n\\n        \\n          \\n                   \"\"\" \\n        \\n\\n        \\n          \\n                   return os.path.join(os.getcwd(), \".neptune\")',\n",
              " 'D_kwDOCqWgoM4AOuBRcon': 'The documentation advises the usage of \\'type_as\\' when initializing new tensors in multi-gpu settings:\\n\\nhttps://pytorch-lightning.readthedocs.io/en/stable/advanced/multi_gpu.html#init-tensors-using-type-as-and-register-buffer\\nWhen you need to create a new tensor, use type_as. This will make your code scale to any arbitrary number of GPUs or TPUs with Lightning.\\n\\nThe example shows a case where a new tensor is initialized inside a LightningModule forward function:\\n\\ndef forward(self, x):\\nz = torch.Tensor(2, 3)\\nz = z.type_as(x)\\n\\nPresumably x is a tensor that has already been initialized on the target gpu.\\nMy question is what to do in the case where we want to initialize a new tensor on the target gpu, and we do not have access to a tensor that has already been initialized on the target gpu?\\nFor example, how does one properly initialize a new tensor when it is created inside a Dataset constructor that is instantiated during LightningDataModule setup()?\\n\\nclass SomeDataModule(LightningDataModule):\\n...\\ndef setup(self, stage: Optional[str] = None):\\nif stage in (None, \"fit\"):\\ndataset = SomeDataset()\\n...\\n\\nwhere:\\n\\nclass SomeDataset(Dataset):\\ndef init(self):\\nself.some_tensor = torch.Tensor(2,3)\\n\\nWill using type_as on the new tensor initialize the data on the target gpu?\\n\\nself.some_tensor = self.some_tensor.type_as(self.some_tensor)\\n\\nOr is a different approach necessary? (e.g. register_buffer())',\n",
              " 'D_kwDOCqWgoM4AOuUNcon': 'Thanks to all the contributors of PyTorch Lightning for a fantastic product!\\nI want to save a checkpoint, hparams & tfevents after training finishes. I have written this callback:\\nclass AfterTrainCheckpoint(pl.Callback):\\n    \"\"\"\\n    Callback for saving the checkpoint weights, hparams and tf.events after training finishes\\n    \"\"\"\\n    def __init__(self):\\n        super().__init__()\\n\\n    def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\\n        print(f\"Saving final checkpoint...\")\\n        # As we advance one step at end of training, we use `global_step - 1`\\n        final_checkpoint_name = f\"final_models/final_step_{trainer.global_step - 1}.ckpt\"\\n        final_hparams_name = f\"final_models/final_step_{trainer.global_step - 1}.yaml\"\\n\\n        trainer.save_checkpoint(final_checkpoint_name)\\n        save_hparams_to_yaml(config_yaml=final_hparams_name, hparams=trainer.model.hparams)\\n\\nIs this the best \\'Lightning\\' way to achieve this?\\nHow can I save the final events.out.tfevents file to a new directory?\\nShould I be setting save_last=True? seen in ModelCheckpoint in the docs. I am slightly confused about \"monitor metrics logged during training/validation steps or end of epochs are not guaranteed to be available at this stage.\"',\n",
              " 'D_kwDOCqWgoM4AOuWgcon': \"Hi all, is it okay to feed the optimizer that's been initialized outside this code to pl.LightningModule?\\ndef Model(pl.LightningModule):\\n    def __init__(optimizer):\\n        self.optimizer = optimizer\\n    def configure_optimizers(self) -> Any:                                                                                      \\n        optimizer = self.optimizer          # like this                                                                                    \\n        scheduler = {                                                                                                               \\n            'scheduler': LambdaLR(optimizer, self.lr_lambda),                                                                       \\n            'interval': 'step',                                                                                                     \\n            'frequency': 1,   \\n        }                                                                                                                                                                                                                   \\n    return [optimizer], [scheduler]\\n\\nThanks!\",\n",
              " 'D_kwDOCqWgoM4AOwH-con': \"Hello,\\nI am trying to remove some layers from DistributedDataParallel to prevent them being synchronized between devices.\\nI spent last 6 hours googling, and I have found out, that there's a attribute _ddp_params_and_buffers_to_ignore which can be set to module that is passed to DistributedDataParallel constructor. I've implemented custom strategy plugin to Trainer, I have checked that the parameters are then passed to a parameters_to_ignore attribute of the DistributedDataParallel but somehow if I check gradients, of the layer, they are always the same.\\nIs there some simpler way to remove some layer / module from being synchronized between more devices in DDP strategy?\\nThank you in advance for any help!\",\n",
              " 'D_kwDOCqWgoM4AOwQYcon': 'Hi all, I tried to train a model with pl. And I just ran the trainer.fit() as below:\\ntrainer.fit(model, train_dataloaders=model.train_dataloader(),\\n                val_dataloaders=model.val_dataloader())\\nAnd I found that model.training == False, when it gets into forward()...\\nIs there any solution or does anybody know the potential reason for this?\\nOr does anyone know where can I find the source code for training_step() so that I can check and debug forward() in fit()?\\nThank you very much.',\n",
              " 'D_kwDOCqWgoM4AOwcvcon': 'Hello,\\nI am trying to build a model which uses multiple optimisers. When I try to train the model I get the error validation_step() missing 1 required positional argument: \\'optimizer_idx\\'. I have reproduced this error on the BoringModel used for bug reports:\\n#!/usr/bin/env python3\\n# -*- coding: utf-8 -*-\\n\\nimport os\\n\\nimport torch\\nfrom torch.utils.data import DataLoader, Dataset\\n\\nfrom pytorch_lightning import LightningModule, Trainer\\n\\n\\nclass RandomDataset(Dataset):\\n    def __init__(self, size, length):\\n        self.len = length\\n        self.data = torch.randn(length, size)\\n\\n    def __getitem__(self, index):\\n        return self.data[index]\\n\\n    def __len__(self):\\n        return self.len\\n\\n\\nclass BoringModel(LightningModule):\\n    def __init__(self):\\n        super().__init__()\\n        self.layer = torch.nn.Linear(32, 2)\\n\\n    def forward(self, x):\\n        return self.layer(x)\\n\\n    def training_step(self, batch, batch_idx, optimizer_idx):\\n        if optimizer_idx == 0:\\n            loss = self(batch).sum()\\n            self.log(\"train_loss\", loss)\\n        if optimizer_idx == 1:\\n            loss = self(batch).sum()\\n            self.log(\"train_loss\", loss)\\n        return {\"loss\": loss}\\n\\n    def validation_step(self, batch, batch_idx, optimizer_idx):\\n        if optimizer_idx == 0:\\n            loss = self(batch).sum()\\n            self.log(\"valid_loss\", loss)\\n        if optimizer_idx == 1:\\n            loss = self(batch).sum()\\n            self.log(\"valid_loss\", loss)\\n        return {\"loss\": loss}\\n\\n    def test_step(self, batch, batch_idx, optimizer_idx):\\n        if optimizer_idx == 0:\\n            loss = self(batch).sum()\\n            self.log(\"test_loss\", loss)\\n        if optimizer_idx == 1:\\n            loss = self(batch).sum()\\n            self.log(\"test_loss\", loss)\\n        return {\"loss\": loss}\\n\\n    def configure_optimizers(self):\\n        opt_a = torch.optim.SGD(self.layer.parameters(), lr=0.1)\\n        opt_b = torch.optim.SGD(self.layer.parameters(), lr=0.1)\\n        return [opt_a, opt_b], []\\n\\n\\ndef run():\\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\\n\\n    model = BoringModel()\\n    trainer = Trainer(\\n        default_root_dir=os.getcwd(),\\n        limit_train_batches=1,\\n        limit_val_batches=1,\\n        limit_test_batches=1,\\n        num_sanity_val_steps=0,\\n        max_epochs=1,\\n        enable_model_summary=False,\\n    )\\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\\n    trainer.test(model, dataloaders=test_data)\\n\\n\\nif __name__ == \"__main__\":\\n    run()\\n\\nWhat am I missing?\\nThanks for the help!',\n",
              " 'D_kwDOCqWgoM4AOwkLcon': \"My code is not printing print('train acc loss',acc,loss) in trained_epoch_end but its printing print('val acc loss',acc,loss) in validation_epoch_end\\nclass Model(LightningModule):\\n  def __init__(self):\\n    super(Model,self).__init__()\\n    self.model=ResneT(21)\\n    self.lr=1e-3\\n    self.bs=128\\n    self.worker=6\\n    self.acc=torchmetrics.Accuracy()\\n    self.creterion=nn.BCEWithLogitsLoss()\\n    self.scheduler='lambda'\\n  def forward(self,x):\\n    x=self.model(x)\\n    return x\\n\\n  def configure_optimizers(self):\\n    opt=torch.optim.AdamW(params=self.parameters(),lr=self.lr )\\n    return opt\\n\\n  def train_dataloader(self):\\n    dataset=DataReader(train_df)\\n    dataloader=DataLoader(dataset,batch_size=self.bs,num_workers=self.worker,shuffle=True,\\n                         pin_memory=True,collate_fn=collate_fn)\\n    return dataloader\\n\\n  def training_step(self,batch,batch_idx):\\n    signal,label=batch\\n    out=self(signal.float())\\n    loss=self.creterion(out.flatten(),label.float().flatten())\\n    acc=self.acc(out.flatten(),label.long().flatten())\\n    return {'loss':loss,'acc':acc}\\n\\n  def trained_epoch_end(self,outputs):\\n    acc=torch.stack([x['acc'] for x in outputs]).mean().detach().cpu().numpy().round(2)\\n    loss=torch.stack([x['loss'] for x in outputs]).mean().detach().cpu().numpy().round(2)\\n    self.log('train acc',acc)\\n    self.log('train loss',loss)\\n    print('train acc loss',acc,loss)\\n\\n  def val_dataloader(self):\\n    dataset=DataReader(val_df)\\n    dataloader=DataLoader(dataset,batch_size=self.bs,num_workers=self.worker,shuffle=False,\\n                          pin_memory=True,\\n                         collate_fn=collate_fn)\\n    return dataloader\\n\\n  def validation_step(self,batch,batch_idx):\\n    signal,label=batch\\n    out=self(signal.float())\\n    loss=self.creterion(out.flatten(),label.float().flatten())\\n    acc=self.acc(out.flatten(),label.long().flatten())\\n    return {'loss':loss,'acc':acc}\\n\\n  def validation_epoch_end(self,outputs):\\n    acc=torch.stack([x['acc'] for x in outputs]).mean().detach().cpu().numpy().round(2)\\n    loss=torch.stack([x['loss'] for x in outputs]).mean().detach().cpu().numpy().round(2)\\n    print('val acc loss',self.current_epoch,acc,loss)\\n    self.log('val acc',acc)\\n    self.log('val loss',loss)\",\n",
              " 'D_kwDOCqWgoM4AOxVNcon': \"Am using the following callback,\\n checkpoint_callback = ModelCheckpoint(monitor='val/loss',\\n                                        mode='min',\\n                                        save_last=True,\\n                                        filename=cfg.CALLBACKS.FILENAME,\\n                                        auto_insert_metric_name=cfg.CALLBACKS.AUTO_INSERT_METRIC_NAME,\\n                                        dirpath=LOGGER_DIR)\\n\\nI am not sure what is going wrong. Am using F.cross_entropy for loss.\",\n",
              " 'D_kwDOCqWgoM4AOxzHcon': 'I was wondering if anyone has observed odd performance when training multi-GPU models? I’ve developed a script which trains a toy dataset (in this case the cats and dogs model), using a ResNet or EfficientNet. The script works fine locally on the GPU. However, when I move the script to the cloud and train using multiple GPUs strange things start to happen. The script trains fine on the cloud using 1 GPU, albeit slow as I was testing using a M60. However, if I run the same script on 4x K80 with ddp I find that the training process is around ~15% slower (which I’m guessing is the difference between K80 and M60).\\n\\nI checked GPU usage and the GPUs are all being used. However, model performance seems slower/worse than using just one GPU. Any ideas why this could be?',\n",
              " 'D_kwDOCqWgoM4AOy9zcon': 'Hi,\\nI am trying to optimise the hyperparameters of my network using raytune. My implementation is pretty much based on this:\\nhttps://docs.ray.io/en/master/tune/tutorials/tune-pytorch-lightning.html#selecting-a-scheduler\\nWhen I train my network using pre-set hyperparameters, it works smoothly. The problems come from the callback, so when I add the following line:\\nTuneReportCallback({\"loss\":\"val_loss\"}, on=\"validation_end\")\\nI get the following error:\\n\\nAnyone knows how to solve this??\\nI don\\'t think the problem is with my code as I haven\\'t done anything different compared to the tutorial!\\nMy code can be found here:\\nhttps://github.com/annalauralerede/anomaly-detection/blob/main/lstmae_pl_opt.py',\n",
              " 'D_kwDOCqWgoM4AOyUncon': \"Hi,\\nAs part of my validation_step in a AE I am trying to return an object containing input and reconstructed output.\\nvalidation_epoch_end does not get the accumulated outputs.\\nHere's the code:\\ndef validation_step(self, batch, batch_idx):\\n        x, x_recon, _, _, _ = self.forward(batch)\\n        outputs = {\\n            'x': x,\\n            'x_recon': x_recon\\n        }\\n        return outputs\\n\\nthese are the shapes:\\nx.shape:  torch.Size([2, 1, 257, 63])\\nx_recon.shape:  torch.Size([2, 1, 257, 63])\\nand this is what validation_step_outputs looks like:\\n[{'x': tensor(0.0033, device='cuda:0'), 'x_recon': tensor(-0.0102, device='cuda:0')}]\\nAny idea why?\\nThanks\",\n",
              " 'D_kwDOCqWgoM4AOzN_con': 'def configure_optimizers(self):\\n        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\\n        print(self.trainer.max_steps)\\n        lr_scheduler = {\\n            \\'scheduler\\': torch.optim.lr_scheduler.OneCycleLR(optimizer,\\n                                                             max_lr=self.lr,\\n                                                             total_steps=self.trainer.max_steps,\\n                                                             anneal_strategy=\\'linear\\',\\n                                                             cycle_momentum=False,\\n                                                             pct_start=0.1),\\n            \\'interval\\': \\'step\\',\\n            \\'frequency\\': 1\\n        }\\n        return {\\'optimizer\\': optimizer, \\'lr_scheduler\\': lr_scheduler, \"monitor\": \\'val_acc\\'}\\nraise ValueError(\"Expected positive integer total_steps, but got {}\".format(total_steps))\\nValueError: Expected positive integer total_steps, but got -1',\n",
              " 'D_kwDOCqWgoM4AOzRXcon': 'Hi everyone.\\nI am trying to use 4 gpus in a single node to train my model with DDP strategy. But everytime I run trainer.fit, the whole bunch of codes are executed 4 times repeatedly, and it requires 4 times of CPU memory compared to a single GPU case.\\nI am not sure whether it is intended behavior or not. I ran the following sample code. It trains MNIST data on 4 gpus.\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")\\n\\nimport os\\nimport torch\\nfrom pytorch_lightning import LightningModule, Trainer\\nfrom torch import nn\\nfrom torch.nn import functional as F\\nfrom torch.utils.data import DataLoader#, random_split\\nfrom torchvision import transforms\\nfrom torchvision.datasets import MNIST\\n\\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\\n\\nclass MNISTModel(LightningModule):\\n    def __init__(self):\\n        super().__init__()\\n        self.l1 = torch.nn.Linear(28 * 28, 10)\\n\\n    def forward(self, x):\\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\\n\\n    def training_step(self, batch, batch_nb):\\n        x, y = batch\\n        loss = F.cross_entropy(self(x), y)\\n        return loss\\n\\n    def configure_optimizers(self):\\n        return torch.optim.Adam(self.parameters(), lr=0.02)\\n\\n\\nif __name__ == \\'__main__\\':\\n    print(\\'Hello world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\')\\n    mnist_model = MNISTModel()\\n\\n    train_ds = MNIST(PATH_DATASETS, train=True, download=True, transform=transforms.ToTensor())\\n    train_loader = DataLoader(train_ds, batch_size=256)\\n\\n    trainer = Trainer(gpus=4, strategy=\\'ddp\\', max_epochs=1, replace_sampler_ddp=True, num_nodes=1)\\n    trainer.fit(mnist_model, train_loader)\\n\\nAnd I got the following output:\\nHello world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\nGPU available: True, used: True\\nTPU available: False, using: 0 TPU cores\\nIPU available: False, using: 0 IPUs\\nHello world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\ninitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\\nHello world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\nHello world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\ninitializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\\ninitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\\ninitializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\\n----------------------------------------------------------------------------------------------------\\ndistributed_backend=nccl\\nAll distributed processes registered. Starting with 4 processes\\n----------------------------------------------------------------------------------------------------\\n\\nThe training is done well, but the thing is that \\'Hello world!\\' is printed four times. My problem here is that train data is loaded four times also and it takes four times of CPU memory. I am not sure whether it is the intended behavior or am I doing something wrong?\\nHow do you deal with DDP if train data is too large to be copied by multiple (=gpu num) times?',\n",
              " 'D_kwDOCqWgoM4APAR6con': 'This is the code\\n    def test_step(self,batch,batch_idx):\\n        image,label=batch\\n        pred = self(image)\\n        loss=self.criterion(pred.flatten(),label.float()) #calculate loss\\n        acc=self.metrics(pred.flatten(),label)#calculate accuracy\\n        pred=torch.sigmoid(pred)\\n        return {\\'loss\\':loss,\\'acc\\':acc,\\'label\\':label,\\'pred\\':pred}\\n\\n    def test_epoch_end(self, outputs):\\n        loss=torch.stack([x[\"loss\"] for x in outputs]).mean().detach().cpu().numpy().round(2)\\n        acc=torch.stack([x[\"acc\"] for x in outputs]).mean().detach().cpu().numpy().round(2)\\n        label=torch.cat([x[\"label\"] for x in outputs]).detach().cpu().numpy().ravel()\\n        pred=torch.cat([x[\"pred\"] for x in outputs]).detach().cpu().numpy().ravel()\\n        pred=pred.astype(int)\\n        print(\\'torch acc\\',acc)\\n        print(classification_report(label,pred))\\n        print(\\'sklearn\\',accuracy_score(label,pred))\\nThere is difference of 10-15% between accuracies obtained by torchmetrics and sklearn',\n",
              " 'D_kwDOCqWgoM4APBbicon': \"Problem:\\nI have a problem. when i train the model, the process would be blocking in validation step(or validation sanity check) but in the training step, it can work. And I debug in the pytorch-lightning , i found when loading data from validation dataloader it would be blocking. I am not sure what problem.\\nEnvironment:\\ndocker; the lastest pytorch-lighting;gpu a100\\nlog:\\nINFO Using validation DataLoader3DOffset with {}\\nINFO Building Sampling Cache for Dataloder\\nSampling Cache: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 1445.07it/s]\\nINFO Using 5 num_processes and 2 num_cached_per_queue for augmentation.                                                                                                           | 0/2 [00:00<?, ?it/s]\\nINFO VALIDATION KEYS:\\nodict_keys(['case_0', 'case_7'])\\nusing pin_memory on device 0\\n\\nI test the validation step and it world jam\\ncan you help me, thank you !!!!!\",\n",
              " 'D_kwDOCqWgoM4APCnscon': 'What is the best practice for performing evaluation on the dev and/or test set/s every X steps and not every epoch end?\\nIn the documentation I saw the options of overriding validation_epoch_end and test_epoch_end, but could not find how to evaluate \"more frequently\".\\nThanks!',\n",
              " 'D_kwDOCqWgoM4APFBWcon': \"I'm trying to gain some confidence in a model that seems to be training fine.\\nAs a simple sanity check I'm trying to make sure I can load then test a checkpoint with the same input, expecting to be able to produce the same output each and every time (I'm using the same input and checkpoint each time so I expect the output to be the same).\\nUnfortunately, I'm observing different output each time I reload the checkpoint.\\nHere is the essence of what I'm doing:\\nfor n in range(2):\\n        my_module = MyLightningModule.load_from_checkpoint(ckpt_path)\\n\\n        my_dataset = MyDataset()\\n        batch = my_dataset.get_sanity_test_batch()  # confirmed to be the same batch every time\\n\\n        # this output is different every time (???)\\n        output = my_module.model.generate(batch, max_length=some_length)\\n\\nIt is also probably worth noting that the model trained/loaded by my_module is a hugginface T5 transformer (T5ForConditionalGeneration )\\nPlease help me figure out how to ensure output is consistent after loading a trained checkpoint.\",\n",
              " 'D_kwDOCqWgoM4APFIRcon': 'Hi everyone. I was recently running a lightning model and saved a checkpoint to store the intermediate results. When I try to open the checkpoint, I get an error that positional arguments (used to initialize the lightning module) are not present. This wouldn\\'t be a big deal but one of the positional arguments is the encoder (used for BarlowTwins training). I was worried if I loaded the model checkpoint with an encoder initialized with starting weights, this would overwrite the weight parameters stored in the checkpoint. See the error log and a block of code below. Any suggestions on how I can appropriately load this stored model to resume training?\\n  model_ckpt = BarlowTwins.load_from_checkpoint(\\'/wynton/protected/home/ichs/dmandair/BRCAness/datasets/train/pcam/epoch=199-step=25599.ckpt\\')\\n\\n      Traceback (most recent call last):\\n      File \"/wynton/protected/home/ichs/dmandair/BRCA/barlow.py\", line 435, in <module>\\n        main(default_config)\\n      File \"/wynton/protected/home/ichs/dmandair/BRCA/barlow.py\", line 427, in main\\n        model_ckpt = BarlowTwins.load_from_checkpoint(\\'/wynton/protected/home/ichs/dmandair/BRCAness/datasets/train/pcam/epoch=199-step=25599.ckpt\\')\\n      File \"/wynton/protected/home/ichs/dmandair/anaconda3/envs/BRCA/lib/python3.9/site-packages/pytorch_lightning/core/saving.py\", line 156, in load_from_checkpoint\\n        model = cls._load_model_state(checkpoint, strict=strict, **kwargs)\\n      File \"/wynton/protected/home/ichs/dmandair/anaconda3/envs/BRCA/lib/python3.9/site-packages/pytorch_lightning/core/saving.py\", line 198, in _load_model_state\\n        model = cls(**_cls_kwargs)\\n    TypeError: __init__() missing 5 required positional arguments: \\'encoder\\', \\'encoder_out_dim\\', \\'num_training_samples\\', \\'batch_size\\', and \\'weight_decay\\'\\n\\noriginal model loaded with:\\n    encoder = resnet18(zero_init_residual=True)\\n\\n    model = BarlowTwins(\\n        encoder=encoder,\\n        encoder_out_dim=encoder_out_dim,\\n        learning_rate = default_config[\\'LR\\'],\\n        weight_decay = default_config[\\'WD\\'],\\n        num_training_samples=262144,\\n        batch_size=BATCH_SIZE,\\n        z_dim=default_config[\\'Z_DIM\\'],\\n        lambda_coeff = default_config[\\'LAMBDA\\'],\\n        max_epochs=MAX_EPOCHS\\n    )',\n",
              " 'D_kwDOCqWgoM4APFNCcon': \"Hi,\\nI have a machine learning architecture project that requires modifying the network structure multiple times. I used PytorchLigtning codes to implement it. The overall structure is as followed.\\nThe model definition, I ignore the training_step, 'validation_step' for clearly demonstration.\\ndef ToyModel(pl.LightningModule):\\n  def __init__(self):\\n    super(ToyModel, self).__init__()\\n    self.list = nn.ModuleList()\\n  def forward(self, x):\\n    for op in self.list:\\n      x = op(x)\\n    return x\\n  def add(self):\\n    self.list.append(nn.Layer(...))\\nThe following main script shows that I want to update the network structure and retrain the model in 10 iterations.\\nmodel = ToyModel()\\nfor iter in range(10):\\n  model.add()\\n  trainer = Trainer(model, strategy='ddp', gpus=-1)\\n  trainer.fit(model)\\nWhen iter == 1, the model has been propagated into different GPU, and the model.add() results in different models. So I add a flag to make sure the modification is happened in the main process by\\nmodel = ToyModel()\\nfor iter in range(10):\\n  model.add()\\n  trainer = Trainer(model, strategy='ddp', gpus=-1)\\n  trainer.fit(model)\\n  if not trainer.is_global_zero:\\n    return # kill other processes\\nBut this time, the program get stuck when iter == 1. My questions are:\\n\\nI have a feeling that native Pytorch using spawn can do that, do I need to switch back to PyTorch?\\nIs there any decent way to do that in PyTorch? Maybe ddp_spawn?\\n\\nThanks for your time. Any comments or suggestions are welcome.\",\n",
              " 'D_kwDOCqWgoM4APFqdcon': 'Hi everyone,\\nIn my current setup, I would like to change the dataloader during a training epoch:\\nThis is what I would like to achieve:\\nstep 1.Train on dataset 1 for n batches\\nstep 2.Train on dataset 2 for n batches\\nstep 3.Go to step 1\\nI found this solution on the old forum but this only switches the dataset after each epoch.\\nHere is my current attempt at switching it every n batches:\\nclass SimpleModule(pl.LightningModule):\\n    def __init__(self):\\n        super().__init__()\\n        self.model = ...\\n        self.batch_size = ...\\n        self.change_every_n_batch = 20\\n    \\n    def train_dataloader(self):\\n        self.current_dataset = (self.global_step // self.change_every_n_batch) % 2\\n        if self.current_dataset == 0:\\n            dataset = Dataset1()\\n        elif self.current_dataset == 1:\\n            dataset = Dataset2()\\n\\n        dataloader = DataLoader(dataset, batch_size=self.batch_size)\\n        return dataloader\\n    \\n    def on_train_batch_end(self, outputs, batch, batch_idx):\\n        new_dataset = (self.global_step // self.change_every_n_batch) % 2\\n        if new_dataset != self.current_dataset:\\n            self.trainer.reset_train_dataloader(self)\\ntrain_dataloader() is called as expected every 20 batches by on_train_batch_end() but the returned dataloader does not seem to be used during the training loop.\\nAny idea what could be going wrong? Or do you have a solution for what I want to achieve?\\nThanks!',\n",
              " 'D_kwDOCqWgoM4APGS0con': 'Hi,\\nI want to implement a BYOL-like model using Lightning CLI, and one of the key aspects is to use specific data augmentations. However, I don\\'t manage to indicate to the cli that I want to use these specific data augmentations. Here is a snippet of my code:\\nmain.py\\nimport torch\\nimport torch.nn as nn\\n\\nimport pytorch_lightning as pl\\nfrom pytorch_lightning.utilities.cli import LightningCLI, MODEL_REGISTRY\\nfrom pl_bolts.datamodules import CIFAR10DataModule\\nfrom pl_bolts.models.self_supervised.simclr import SimCLRTrainDataTransform, SimCLREvalDataTransform\\n\\n\\nclass DummyModel(pl.LightningModule):\\n    def __init__(self):\\n        super().__init__()\\n        self.fc = nn.Linear(32*32*3, 10)\\n        self.loss_fn = nn.MSELoss()\\n\\n    def shared_step(self, batch, batch_idx):\\n        x, y = batch[0][:2]\\n        z1 = self.fc(x.reshape(x.size(0), -1))\\n        z2 = self.fc(y.reshape(y.size(0), -1))\\n        return self.loss_fn(z1, z2)\\n\\n    def training_step(self, batch, batch_idx):\\n        return self.shared_step(batch, batch_idx)\\n\\n    def validation_step(self, batch, batch_idx):\\n        return self.shared_step(batch, batch_idx)\\n\\n    def configure_optimizers(self):\\n        return torch.optim.Adam(self.parameters())\\n\\n\\n# THE COMMENTED LINES BELOW RUN PERFECTLY\\n# trainer = pl.Trainer()\\n# model = DummyModel()\\n# dm = CIFAR10DataModule()\\n# dm.train_transforms = SimCLRTrainDataTransform(32)\\n# dm.val_transforms = SimCLREvalDataTransform(32)\\n# trainer.fit(model, dm)\\n\\ncli = LightningCLI(DummyModel, CIFAR10DataModule, run=False)\\n\\n# not instantiated!\\nprint(cli.config_init.data.train_transforms)\\nand here is my config.yaml:\\ndata:\\n  train_transforms:\\n    class_path: pl_bolts.models.self_supervised.simclr.SimCLRTrainDataTransform\\n    init_args:\\n      input_height: 32\\n  val_transforms:\\n    class_path: pl_bolts.models.self_supervised.simclr.SimCLREvalDataTransform\\n    init_args:\\n      input_height: 32\\nTo run the code, I run the following command:\\npython main.py --config config.yaml\\n\\nand here is the error I get:\\nTraceback (most recent call last):\\n  File \"/home/alain/code/misc/examples/main.py\", line 42, in <module>\\n    cli.trainer.fit(cli.model, cli.datamodule)\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 740, in fit\\n    self._call_and_handle_interrupt(\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\\n    return trainer_fn(*args, **kwargs)\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 777, in _fit_impl\\n    self._run(model, ckpt_path=ckpt_path)\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1199, in _run\\n    self._dispatch()\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1279, in _dispatch\\n    self.training_type_plugin.start_training(self)\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\\n    self._results = trainer.run_stage()\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1289, in run_stage\\n    return self._run_train()\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1311, in _run_train\\n    self._run_sanity_check(self.lightning_module)\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1375, in _run_sanity_check\\n    self._evaluation_loop.run()\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/loops/base.py\", line 145, in run\\n    self.advance(*args, **kwargs)\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\\n    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/loops/base.py\", line 140, in run\\n    self.on_run_start(*args, **kwargs)\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 86, in on_run_start\\n    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py\", line 121, in _update_dataloader_iter\\n    dataloader_iter = enumerate(data_fetcher, batch_idx)\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py\", line 199, in __iter__\\n    self.prefetching(self.prefetch_batches)\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py\", line 258, in prefetching\\n    self._fetch_next_batch()\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py\", line 300, in _fetch_next_batch\\n    batch = next(self.dataloader_iter)\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\\n    data = self._next_data()\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\\n    data = [self.dataset[idx] for idx in possibly_batched_index]\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\\n    data = [self.dataset[idx] for idx in possibly_batched_index]\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torch/utils/data/dataset.py\", line 363, in __getitem__\\n    return self.dataset[self.indices[idx]]\\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torchvision/datasets/cifar.py\", line 121, in __getitem__\\n    img = self.transform(img)\\nTypeError: \\'dict\\' object is not callable\\n\\nWhat I understand from the error is that LightningCLI doesn\\'t manage to interprete and instantiate the data augmentation model and understands it only as a dictionary with keys class_path and init_args. Does someone encounter a similar issue and/or knows how to solve it?',\n",
              " 'D_kwDOCqWgoM4APGyZcon': 'Hi!\\nI need to create a callback that once every N training steps performs the forward pass over the PL module and do some calculations.\\nMy first approach has been to simply create a callback and then define a new training_step() within that callback that does my needed calculations. The problem is that this is calculations are not being executed.\\nUsing the debugger I see that the callback is correctly initialized and correctly passed to the trainer, but it is not entering in this newly defined training step. Here is a minimal example of what I need to do\\n\\nDo you have some insights on what I am doing wrong?',\n",
              " 'D_kwDOCqWgoM4APH-Lcon': 'My deepspeed_zero2_config.json:\\n{\\n    \"zero_optimization\": {\\n        \"stage\": 2,\\n        \"offload_optimizer\": {\\n            \"device\": \"cpu\",\\n            \"pin_memory\": true\\n        },\\n        \"allgather_partitions\": true,\\n        \"allgather_bucket_size\": 2e8,\\n        \"overlap_comm\": true,\\n        \"reduce_scatter\": true,\\n        \"reduce_bucket_size\": 2e8,\\n        \"contiguous_gradients\": true\\n    },\\n    \"gradient_accumulation_steps\": \"auto\",\\n    \"gradient_clipping\": \"auto\",\\n    \"steps_per_print\": 2000,\\n    \"train_batch_size\": \"auto\",\\n    \"train_micro_batch_size_per_gpu\": \"auto\",\\n    \"wall_clock_breakdown\": false\\n}\\nI have some questions about how to configure DeepSpeed in Pytorch-Lightning:\\n\\nI see that the custom deepspeed config includes optimizer and scheduler. Should I add them in my config even I have configured in Model.configure_optimizers?\\nYou have not specified an optimizer or scheduler within the DeepSpeed config. Using `configure_optimizers` to define optimizer and scheduler.\\n\\n\\nShould I add fp16 config into deepspeed config json even I have passed precision=\"bf16\" in pl.Trainer?\\nShould I pass logging_batch_size_per_gpu to pl.plugins.DeepSpeedPlugin even I have configured batch_size in data loader?\\n[2022-03-24 12:42:11,529] [WARNING] [deepspeed.py:630:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedPlugin(logging_batch_size_per_gpu=batch_size))`.\\n\\n\\nIt appears in log before training every time. Is that okey?\\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\\nninja: no work to do.\\n\\n\\n\\nThanks a lot!😊',\n",
              " 'D_kwDOCqWgoM4APJ2ucon': 'Hi,\\nCould anyone advice me how to set up PyTorch lightning trainer to learn based on iterations instead of epochs?\\nThank you!',\n",
              " 'D_kwDOCqWgoM4APM3ccon': \"Hi, is there a way in trainer to disable tf32 for ampere architecture? It's motivated by this discussion:https://discuss.pytorch.org/t/numerical-error-on-a100-gpus/148032/2\\ncc @justusschock @kaushikb11 @awaelchli @Borda @rohitgr7\",\n",
              " 'D_kwDOCqWgoM4APNwRcon': \"Hi,\\nI want to use a pretrained ResNet or DenseNet with adjusted fc layer for image regression.\\nAfter training, however, when I want to load the finetuned DenseNet for prediction I get a RuntimeError: Error(s) in loading state_dict.\\nThis error does not occur for the ResNet.\\nMy best guess is that this is somehow linked to checkpointing and that the value for self.model_arch might not be properly stored in the state_dict?\\nBut I am not even sure if I need to tell the LightningModule what to save in the state_dict.\\nAny ideas what might go wrong?\\nBig Thanks already!\\nLightningModule constructor:\\nclass ImageRegressor(LightningModule):\\n    def __init__(self, optimizer:str = 'adam', k:int = 0, lr:float = 1e-3, batch_size:int = 16, pretrained:bool = True, tune_fc_only:bool = False, model: str = 'resnet50'):\\n        super().__init__()\\n        self.lr = lr\\n        self.batch_size = batch_size\\n        self.k = k\\n        \\n        optimizers = {'adam': Adam, 'sgd': SGD}\\n        self.optimizer = optimizers[optimizer]\\n        self.criterion = nn.MSELoss(reduction='mean')\\n        self.model_arch = model\\n        num_target_classes = 1\\n\\n        if self.model_arch == 'resnet50':\\n            # init a pretrained resnet\\n            self.model = models.resnet50(pretrained=pretrained)\\n            num_filters = self.model.fc.in_features\\n            self.model.fc = nn.Sequential(\\n                nn.ReLU(),\\n                nn.Linear(num_filters, num_target_classes))\\n        elif self.model_arch == 'densenet':\\n            print('setting up densenet')\\n            self.model = models.densenet121(pretrained=pretrained)\\n            num_filters = self.model.classifier.in_features\\n            self.model.classifier = nn.Sequential(\\n                nn.ReLU(),\\n                nn.Linear(num_filters, num_target_classes))\\n        if pretrained:\\n            if tune_fc_only: # option to only tune the fully-connected layers\\n                for child in list(self.model.children())[:-1]:\\n                    for param in child.parameters():\\n                        param.requires_grad = False\\n\\nSaving is done automatically using this checkpoint:\\nModelCheckpoint(dirpath=this_output_dir,\\n                                     filename='model_'+str(k)+'_{epoch}.pt',\\n                                     monitor='val_loss')\\n\\nloading checkpoint:\\nmodel = ImageRegressor(pretrained=True, tune_fc_only=True, model='densenet')\\ntype(model).load_from_checkpoint(path)\",\n",
              " 'D_kwDOCqWgoM4APO41con': 'When I trained on multi-GPU, the model weight file is corrupted. I guess the reason is that multi-gpus are saving the model weight at the same time. So how can I call the CheckpointCallback on a specific GPU?',\n",
              " 'D_kwDOCqWgoM4APOIScon': \"I have a metric from torchmetric as follows:\\nAccuracy(\\n    num_classes=self.model.out_channels,\\n     average='none',\\n     ignore_index=self.ignore_index\\n)\\nObviously I can not log this, however I don't want to set average to any aggregation. I want to log its mean in training_step but want to preserve the class wise metric to till end of epoch where I display it to terminal. I want the metric to reset at epoch end only, so can't call compute() in training step.\\nHow to solve this?\",\n",
              " 'D_kwDOCqWgoM4APOMHcon': 'I call save_hyperparameters() in __init__(), and all hyper parameters sent to PL model are saved to checkpoint file. However, when i resume training from a checkpoint(call trainer.fit(..., ckpt_path=checkpoint_file_path)), the hyper parameters are not restored from checkpoint file and all of them keep initial values.',\n",
              " 'D_kwDOCqWgoM4APPdAcon': 'Hi,\\nit would be great if you can help me unravel, what is a mystery to me.\\nBackground\\nI have adapted a pretrained model for image regression.\\nIssue :\\nIf I finetune the model using the lightning trainer, the training loss stagnates at a value of ~10. However, in my pytorch training implementation training and validation loss become much less.\\nCan you help me understand where my mistake is? Did I implement .train_step and .forward correctly?\\nPLModule:\\nclass RGBYieldRegressor(LightningModule):\\n    def __init__(self, optimizer:str = \\'sgd\\', k:int = 0, lr:float = 0.001, momentum:float = 0.8, wd:float = 0.01, batch_size:int = 16, pretrained:bool = True):\\n        super().__init__()\\n\\n        self.lr = lr\\n        self.momentum = momentum\\n        self.wd = wd\\n        self.batch_size = batch_size\\n        self.k = k\\n\\n        optimizers = {\\'adam\\': Adam, \\'sgd\\': SGD}\\n        self.optimizer = optimizers[optimizer]\\n\\n        self.criterion = nn.MSELoss(reduction=\\'mean\\')\\n\\n        self.model_arch = model\\n\\n        num_target_classes = 1\\n\\n        self.model = models.resnet50(pretrained=pretrained)\\n        num_filters = self.model.fc.in_features\\n        self.model.fc = nn.Sequential(\\n            nn.ReLU(),\\n            nn.Linear(num_filters, num_target_classes))\\n\\n    def forward(self, x):\\n        return torch.flatten(self.model(x))\\n\\n    def training_step(self, batch, batch_idx): # torch.autograd?\\n        x, y = batch\\n        y_hat = torch.flatten(self.model(x))\\n        loss = self.criterion(y, y_hat)\\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\\n        return loss\\n\\n    def validation_step(self, batch, batch_idx):\\n        x, y = batch\\n        y_hat = torch.flatten(self.model(x))\\n        loss = self.criterion(y, y_hat)\\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\\n\\n    def test_step(self, batch, batch_idx):\\n        x, y = batch\\n        y_hat = torch.flatten(self.model(x))\\n        loss = self.criterion(y, y_hat)\\n        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\\n\\n    def predicts_step(self, batch, batch_idx, dataloader_idx=0):\\n        return self.model(batch).squeeze()\\n\\n    def configure_optimizers(self):\\n        return self.optimizer(self.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.wd)\\n\\n\\nTrainer:\\ntrainer = Trainer(\\n            max_epochs=50,  # general\\n            num_sanity_val_steps=0,\\n            devices=1,\\n            accelerator=\"auto\",\\n            callbacks=callbacks,\\n            default_root_dir=this_output_dir,\\n            weights_save_path=this_output_dir,\\n            logger=logger,\\n            num_processes=1,  \\n        )\\n        trainer.fit(lightningmodule, train_dataloaders=datamodule.train_dataloader(), val_dataloaders=datamodule.val_dataloader())\\n\\nvs. pytorch training:\\nfor phase in [\\'train\\', \\'val\\']:\\n    if phase == \\'train\\':\\n        model.train()  # Set model to training mode\\n    else:\\n        model.eval()   # Set model to evaluate mode\\n    running_loss = 0.0\\n\\n    # Iterate over data.\\n    for inputs, labels in dataloaders[phase]:\\n        inputs = inputs.to(device)\\n        labels = labels.to(device)\\n\\n        # zero the parameter gradients\\n        optimizer.zero_grad()\\n\\n        # forward\\n        # track history if only in train\\n        with torch.set_grad_enabled(phase == \\'train\\'):\\n            outputs = model(inputs)\\n            loss = criterion(torch.flatten(outputs), labels.data)\\n            if phase == \\'train\\':\\n                loss.backward()\\n                optimizer.step()\\n        # statistics\\n        running_loss += loss.item() * inputs.size(0)\\n\\n    epoch_loss = running_loss / len(dataloaders[phase].dataset)\\n    print(\\'{} Loss: {:.4f}\\'.format(phase, epoch_loss))',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI0MA==con': 'I am training a  model with lightning where I am attempting to use all the GPUs on my system (4 in total).\\nMy trainer is run as:\\nmodel = MyModel(hparams)\\nif torch.cuda.is_available():\\n    trainer = Trainer(gpus=-1)\\nelse:\\n    trainer = Trainer()\\ntrainer.fit(model)\\n\\nMy model is defined as follows:\\nclass SiameseNet(pl.LightningModule):\\n    \"\"\"\\n    Implement a siamese network as a feature extractor withh Lightning module\\n    \"\"\"\\n    def __init__(self,\\n                 hparams):\\n        \"\"\"\\n        Build the network\\n        \"\"\"\\n        super(SiameseNet, self).__init__()\\n        self.net = self._build_net()\\n        self.hparams = hparams\\n        self.train_data_path = hparams.get(\\'train_data_path\\', None)\\n        self.test_data_path = hparams.get(\\'test_data_path\\', None)\\n        self.val_data_path = hparams.get(\\'val_data_path\\', None)\\n        self.train_dataset = None\\n        self.val_dataset = None\\n        self.test_dataset = None\\n\\n        self.lossfn = TripletLoss(margin=1.0)\\n\\n    def forward_once(self, x):\\n        output = self.net(x)\\n        output = torch.squeeze(output)\\n        return output\\n\\n    def forward(self, input1, input2, input3=None):\\n        output1 = self.forward_once(input1)\\n        output2 = self.forward_once(input2)\\n\\n        if input3 is not None:\\n            output3 = self.forward_once(input3)\\n            return output1, output2, output3\\n\\n        return output1, output2\\n\\n    @staticmethod\\n    def _build_net():\\n        net = nn.Sequential(\\n            nn.Conv2d(3, 32,kernel_size=3,stride=2),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(32),\\n\\n            nn.Conv2d(32, 64, kernel_size=3, stride=2),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(64),\\n\\n            nn.Conv2d(64, 128, kernel_size=3, stride=2),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(128),\\n\\n            nn.Conv2d(128, 256, kernel_size=1, stride=2),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(256),\\n\\n            nn.Conv2d(256, 256, kernel_size=1, stride=2),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(256),\\n\\n            nn.Conv2d(256, 512, kernel_size=3, stride=2),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(512),\\n\\n            nn.Conv2d(512, 1024, kernel_size=1, stride=1),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(1024))\\n\\n        return net\\n\\n    def prepare_data(self):\\n        transform = torchvision.transforms.Compose([\\n            torchvision.transforms.Resize((128, 128)),\\n            torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\\n            torchvision.transforms.RandomHorizontalFlip(),\\n            torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\\n            torchvision.transforms.ToTensor()\\n        ])\\n\\n        if self.train_data_path:\\n            train_folder_dataset = dset.ImageFolder(root=self.train_data_path)\\n            self.train_dataset = SiameseTriplet(image_folder_dataset=train_folder_dataset,\\n                                                transform=transform)\\n        if self.val_data_path:\\n            val_folder_dataset = dset.ImageFolder(root=self.val_data_path)\\n            self.val_dataset = SiameseTriplet(image_folder_dataset=val_folder_dataset)\\n\\n        if self.test_data_path:\\n            test_folder_dataset = dset.ImageFolder(root=self.test_data_path)\\n            self.test_dataset = SiameseTriplet(image_folder_dataset=test_folder_dataset)\\n\\n    def training_step(self, batch, batch_idx):\\n        anchor, positive, negative = batch\\n        anchor_out, positive_out, negative_out = self.forward(anchor, positive, negative)\\n        loss_val = self.lossfn(anchor_out, positive_out, negative_out)\\n        return {\\'loss\\': loss_val}\\n\\n    def configure_optimizers(self):\\n        optimizer = optim.Adam(self.parameters(), lr=self.hparams.get(\\'learning_rate\\', 0.001))\\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\\n        return [optimizer], [scheduler]\\n\\n    @pl.data_loader\\n    def train_dataloader(self):\\n        if self.train_dataset:\\n            return DataLoader(self.train_dataset,\\n                              self.hparams.get(\\'batch_size\\', 64),\\n                              num_workers=12)\\n        return None\\n\\nWhen I try and run it, it seems the beginning of the epoch hangs for like 10 minutes to get data into the model and after that the progress is  very sluggish.\\nI also get these messages in  the beginning. Not sure if it is of concern\\nGPU available: True, used: True\\nNo environment variable for node rank defined. Set as 0.\\nCUDA_VISIBLE_DEVICES: [0,1,2,3]\\nMASTER_ADDR environment variable is not defined. Set as localhost\\ninitializing proc_rank 0 world 4\\nMASTER_ADDR environment variable is not defined. Set as localhost\\ninitializing proc_rank 1 world 4\\nMASTER_ADDR environment variable is not defined. Set as localhost\\ninitializing proc_rank 2 world 4\\nMASTER_ADDR environment variable is not defined. Set as localhost\\ninitializing proc_rank 3 world 4\\n\\nIt basically hangs with this:\\nEpoch 1:   0%|                                                                                          | 0/172 [00:00<?, ?it/s]\\n\\nDuring this time, looking at GPU  utilisation it seems:\\n+-----------------------------------------------------------------------------+\\n| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\\n|-------------------------------+----------------------+----------------------+\\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\\n|===============================+======================+======================|\\n|   0  GeForce GTX 108...  Off  | 00000000:05:00.0 Off |                  N/A |\\n| 48%   79C    P2    90W / 250W |   4527MiB / 11176MiB |    100%      Default |\\n+-------------------------------+----------------------+----------------------+\\n|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |\\n| 45%   76C    P2    85W / 250W |   1636MiB / 11178MiB |    100%      Default |\\n+-------------------------------+----------------------+----------------------+\\n|   2  GeForce GTX 108...  Off  | 00000000:09:00.0 Off |                  N/A |\\n| 45%   76C    P2    79W / 250W |   1626MiB / 11178MiB |    100%      Default |\\n+-------------------------------+----------------------+----------------------+\\n|   3  GeForce GTX 108...  Off  | 00000000:0A:00.0 Off |                  N/A |\\n| 32%   65C    P2    79W / 250W |   2689MiB / 11178MiB |    100%      Default |\\n+-------------------------------+----------------------+----------------------+\\n\\n+-----------------------------------------------------------------------------+\\n| Processes:                                                       GPU Memory |\\n|  GPU       PID   Type   Process name                             Usage      |\\n|=============================================================================|\\n|    0      1584      C   /home/pd/.conda/envs/alchera37/bin/python    801MiB |\\n|    0     10714      C   /home/pd/.conda/envs/alchera37/bin/python    543MiB |\\n|    0     28957      C   /home/pd/.conda/envs/alchera37/bin/python   1047MiB |\\n|    0     30880      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\\n|    0     32266      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\\n|    1     10733      C   /home/pd/.conda/envs/alchera37/bin/python    543MiB |\\n|    1     28972      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\\n|    2     10789      C   /home/pd/.conda/envs/alchera37/bin/python    543MiB |\\n|    2     32297      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\\n|    3     10807      C   /home/pd/.conda/envs/alchera37/bin/python    543MiB |\\n|    3     29006      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\\n|    3     30967      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\\n+-----------------------------------------------------------------------------+\\n\\nSo, it seems that getting the data into the GPU is quite slow even though everything looks maxed out.\\nAnd when it does eventually start the epoch after ~30 minutes, it seems to give similar performance as my CPU on MacBook Pro. I am really not sure if I am doing somethingvery  wrong here in how I am using  PL.',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI0MQ==con': '❓ Questions and Help\\nWhat is your question?\\nI want to test summarization model from huggingface summarization example on multiple GPUs . My problem is how could I collect test results  on different GPUs , since test_epoch_end only processes epoch for a single GPU.\\nFor more information, the model is trained with ddp backend.\\nCode\\n def test_epoch_end(self, outputs):\\n        output_test_predictions_file = os.path.join(self.hparams.output_dir, \"test_predictions.txt\")\\n        output_test_targets_file = os.path.join(self.hparams.output_dir, \"test_targets.txt\")\\n        # write predictions and targets for later rouge evaluation.\\n        with open(output_test_predictions_file, \"w+\") as p_writer, open(output_test_targets_file, \"w+\") as t_writer:\\n            for output_batch in outputs:\\n                p_writer.writelines(s + \"\\\\n\" for s in output_batch[\"preds\"])\\n                t_writer.writelines(s + \"\\\\n\" for s in output_batch[\"target\"])\\n            p_writer.close()\\n            t_writer.close()\\n\\n        return self.test_end(outputs)\\n\\nWhat have you tried?\\nFor now, I can only use single GPU to get result of whole dataset.\\nWhat\\'s your environment?\\n\\nOS: Unbuntu 18.04\\nPackaging pip\\nVersion: 0.7.6',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI0Ng==con': '❓ Questions and Help\\nBefore asking:\\n\\nsearch the issues.\\nsearch the docs.\\n\\n\\nWhat is your question?\\nI am training MNIST with below code. 1 GPU training is ok.\\nBut it shows slow start of new epoch when num_workers is a large number and the number of gpus > 2.\\nEven dataloading itself is slower than with 1gpu.\\nCode\\nimport torch\\nfrom torch import nn\\nimport pytorch_lightning as pl\\nfrom torchvision import datasets, transforms\\nfrom torch.utils.data import DataLoader, random_split\\nfrom torchvision.datasets import MNIST\\nfrom torch.nn import functional as F\\nimport torch.distributed as dist\\nimport os, sys\\nclass LightningMNISTClassifier(pl.LightningModule):\\ndef init(self):\\nsuper(LightningMNISTClassifier, self).init()\\nself.layer_1 = nn.Linear(28*28, 128)\\nself.layer_2 = nn.Linear(128, 256)\\nself.layer_3 = nn.Linear(256, 10)\\ndef forward(self, x):\\n    batch_size, channels, width, height = x.size()\\n    x = x.view(batch_size, -1)\\n    x = self.layer_1(x)\\n    x = torch.relu(x)\\n    x = self.layer_2(x)\\n    x = torch.relu(x)\\n    x = self.layer_3(x)\\n    x = torch.log_softmax(x, dim=-1)\\n    return x\\n\\ndef prepare_data(self):\\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307, ), (0.3081,))])\\n    mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transform)\\n    self.mnist_test = MNIST(os.getcwd(), train=False, download=True, transform=transform)\\n    self.mnist_train, self.mnist_val = random_split(mnist_train, [55000,5000])\\n\\ndef train_dataloader(self):\\n    data_loader2 = DataLoader(self.mnist_train, batch_size=64, num_workers=7, shuffle=True) \\n    # data_loader2 = DataLoader(self.mnist_train, batch_size=64, shuffle=True) \\n    return data_loader2\\ndef val_dataloader(self):\\n    return DataLoader(self.mnist_val, batch_size=64)\\n\\n# def test_dataloader(self):\\n#     return DataLoader(self.mnist_test, batch_size=64)\\n\\ndef configure_optimizers(self):\\n    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\\n    return optimizer\\n\\ndef cross_entropy_loss(self, logits, labels):\\n    return F.nll_loss(logits, labels)\\n\\ndef training_step(self, batch, batch_idx):\\n    x, y = batch\\n    logits = self.forward(x)\\n    loss = self.cross_entropy_loss(logits, y)\\n    logs = {\\'train_loss\\': loss}\\n    return {\"loss\": loss, \"log\": logs}\\ndef validation_step(self, batch, batch_idx):\\n    x, y = batch\\n    logits = self.forward(x)\\n    loss = self.cross_entropy_loss(logits, y)\\n    return {\"val_loss\": loss}\\n\\ndef validation_epoch_end(self, outputs):\\n    avg_loss = torch.stack([x[\\'val_loss\\'] for x in outputs]).mean()\\n    tensorboard_logs = {\"val_loss\": avg_loss}\\n    return {\"avg_val_loss\": avg_loss, \\'log\\':tensorboard_logs}\\n\\nif name == \\'main\\':\\nmodel = LightningMNISTClassifier()\\ntrainer = pl.Trainer(gpus=4, distributed_backend=\\'ddp\\')\\n\\ntrainer.fit(model)\\n\\nWhat have you tried?\\nHorovod backend does not show slow start of new epoch.\\nWhat\\'s your environment?\\n\\nOS: ubuntu 18.04\\nPackaging pip\\nVersion pytorch 1.5.0, 0.7.6',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI1MQ==con': \"In my experience, it never works. I looked to the trainer code and saw that the code managing this works only on DDP.\\ndef configure_slurm_ddp(self, num_gpu_nodes):\\n    self.is_slurm_managing_tasks = False\\n\\n    ### !!HERE!!\\n    if self.use_ddp:\\n        self.num_requested_gpus = self.num_gpus * num_gpu_nodes\\n        self.num_slurm_tasks = 0\\n        try:\\n            self.num_slurm_tasks = int(os.environ['SLURM_NTASKS'])\\n            self.is_slurm_managing_tasks = self.num_slurm_tasks == self.num_requested_gpus\\n\\n            # in interactive mode we don't manage tasks\\n            job_name = os.environ['SLURM_JOB_NAME']\\n            if job_name == 'bash':\\n                self.is_slurm_managing_tasks = False\\n\\n        except Exception:\\n            # likely not on slurm, so set the slurm managed flag to false\\n            self.is_slurm_managing_tasks = False\\nHowever, sometimes we are not using the distributed computing on slurm (only DP). It would be nice to have the auto resubmit feature still working in this situation.\\n\\nOS: Linux\\nPackaging conda\\nVersion 16\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjI1Mg==con': \"❓How to use pytorch-lightning distributed training without SLURM?\\nCouldn't find anywhere a single note or tutorial on this.\\nFor example I have just 2 node with 4 GPUs on each.\\nOn each node environment variables required for Pytorch distributed communication are configured (see pytorch documentation).\\nIs this possible to train pytorch-lightning script in this setup and if so how?\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjI1NQ==con': \"Background\\nHi, I try to track the prediction of each individual sample during training/validation-step. The main purpose is to do online hard-example mining/examining.\\nI found out a way of doing this is to make the input variable of the functions training/validation_step  carrying the sample-id information, for example, the file-name. So I made the input to be a dictionary.\\nExample Code\\nclass LightningModule():\\n    def validation_step(self, batch, batch_idx):\\n        y = batch['target'].float()\\n        y_hat = self.forward(batch) \\n        loss = self.get_loss(y_hat, y) \\n\\n        # append the individual result \\n        for i in range(len(batch['sample_id'])):\\n            self.validation_result['prediction_result'].append(y_hat[i])\\n            self.validation_result['sample_id'].append(batch['sample_id'][i])\\n            self.validation_result['target'].append(batch['target'][i])\\n        return {'val_loss': loss}\\n\\n    def forward(self, batch):\\n        x = batch['x']\\n        y_hat = self.model( x)\\n        return y_hat\\n\\nInput-Dict works in Single GPU but fail under multi-GPUs-dp\\ninput_batch = {  \\n    'x' : Tensor (1st dimension as batch), \\n    'target':  Tensor (1st dimension as batch), \\n    'sample-id': [a, b, c] (list-object) \\n}\\n\\nAND It takes me some time to realize that all value-objects inside the input-dictionary should be torch.Tensor, not list contains strings, otherwise while training under Multi-GPU ='dp' mode, the list-obj won't be separated properly.\\nInput-Dict works in both Single/multi-GPUs-dp\\ninput_batch = {  \\n    'x' : Tensor (1st dimension as batch), \\n    'target':  Tensor (1st dimension as batch), \\n    'sample-id': 1D-Tensor for sample-id  ex Tensor([1 , 3, 5]) \\n}\\n\\nCurrently, I still have some doubts on this approach...\\nDoes anyone try to implement similar functions, online hard-example mining, with different approaches?\\nTks : )\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjI1Nw==con': \"❓ Questions and Help\\nWhat is your question?\\nI have been googling around but can't seem to find if there is a multiprocessing module available in Pytorch-Lightning, just like how Pytorch has a torch.multiprocessing module.\\nDoes anyone know if Pytorch-Lightning has this (or a Joblib similar) module? I am looking for a Pytorch-Lightning module which allows me to parallelize over multiple GPUs\\nMany thanks in advance.\\nPs. Sorry if this this the wrong place to post this question. I have posted the same question in Stackoverflow, but haven't received a reply.\\nEdit: To be more specific, I am looking for a multiprocessing module in Pytorch-Lightning which allows me to parallelize over multiple GPUs on non-neural network computations, such as:\\nimport numpy as np\\nimport torch\\nfrom torch.multiprocessing import Pool\\n\\nX = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]])\\nX = torch.DoubleTensor(X)\\n\\ndef X_power_func(j):\\n    X_power = X.cuda()**j\\n    return X_power\\n\\nif __name__ == '__main__':\\n  with Pool(processes = 2) as p:   # Parallelizing over 2 GPUs\\n    results = p.map(X_power_func, range(4))\\n\\nresults\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjI2MA==con': \"❓ Multi-GPU Training GPU Usage\\nBefore asking:\\n\\nsearch the issues.\\nsearch the docs.\\n\\n\\nHi, I'm using lightning and ddp as backend to do multi-gpu training, with Apex amp (amp_level = 'O1'). The gpu number is 8. I noticed that during training, most of time GPU0's utilization is 0%, while others are almost 100%. But their memory usage are the same. Is this normal? I use OpenPAI and have attached their utilization and memeory usage below. Thanks.\\n\\nCode\\n   \\nWhat have you tried?\\nWhat's your environment?\\n\\nOS: [e.g. iOS, Linux, Win]\\nPackaging [e.g. pip, conda]\\nVersion [e.g. 0.5.2.1]\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjI3MQ==con': \"What is your question?\\nFor some learning rate schedulers, there is a required steps_per_epoch parameter. One example is the OneCycleLR scheduler. On a CPU or single GPU, this parameter should be set to the length of the train dataloader. My question is, how should this parameter be set on a multi-GPU machine using DDP. Does this parameter need to be updated to len(self.train_dataloader()) / num_gpus? Or is this done automatically?\\nWhat have you tried?\\nI've tried manually dividing the steps_per_epoch of the OneCycleLR scheduler by the number of GPUs when training on a multi-GPU machine. The LR doesn't seem to be following the expected update pattern and I think the scheduler may be the source of the problem.\\nWhat's your environment?\\n\\nOS: Linux\\nPackaging: conda\\nVersion: 0.7.6\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjI3NA==con': 'Hi! I\\'m currently using Pytorch\\'s weighted random sampler for my multi-class skewed dataset and I\\'ve put \"use_ddp_sampler\" to False.\\nEverything works well with the custom sampler but it\\'s taking a significantly longer time to train my model on multiple GPUs.\\nI notice that with the default sampler my 2000 iterations scale well over 4 GPUs, 500 iterations/gpu but with the custom sampler, it doesn\\'t scale and stays at 2000 iterations/GPU over 4 GPUs - highly likely explaining the longer training times.\\nHow can I speed up my training procedure using a custom sampler?\\nEdit: Trainer settings:\\ntrainer = pl.Trainer(   gpus                = args.gpus, \\n                            num_nodes           = 1,\\n                            distributed_backend = \\'ddp\\', \\n                            max_epochs          = args.epochs,\\n                            weights_save_path   = args.weights_save_path, \\n                            logger              = True,\\n                            replace_sampler_ddp = False,\\n                            checkpoint_callback = checkpoint_callback)',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI3Ng==con': 'When using LARS optimizer, usually the batch size is scale linearly with the learning rate.\\nSuppose I set the base_lr to be 0.1 * batch_size / 256.\\nNow for 1 GPU training with batch size 512, the learning rate should be 0.1 * 2 = 0.2\\nHowever when I use 2 GPUs with DDP backend and batch size of 512 on each GPU. Should my learning rate be:\\n\\n0.1 * 2 = 0.2\\nor 0.1 * 2 *  2 (no. GPUs) = 0.4',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI3OA==con': 'Having refactored my code to avoid iterable datasets I\\'ve now got DDP training working (I also had to set ulimit to prevent another crash). However now it crashes at the test step.\\nThe message implies DDP is not needed for testing - but I don\\'t see any mention in the documentation of how to disable DDP once training has complete (plus I would assume that trainer.test() would do this if it were required).\\nIs there something I should be doing different - this is my train / test code - the test dateloader has batchsize=1\\n    trainer.fit(transformer, datamodule=dm)\\n    transformer.freeze()\\n\\n    # run tests\\n    result = trainer.test(transformer, datamodule=dm)\\n\\n\\n-- Process 0 terminated with the following error:\\nTraceback (most recent call last):\\nFile \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\\nfn(i, *args)\\nFile \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_spawn_backend.py\", line 152, in ddp_train\\nmodel = model.configure_ddp(model, device_ids)\\nFile \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 837, in configure_ddp\\nmodel = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)\\nFile \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 269, in init\\nassert any((p.requires_grad for p in module.parameters())), (\\nAssertionError: DistributedDataParallel is not needed when a module doesn\\'t have any parameter that requires a gradient.',\n",
              " 'MDEwOkRpc2N1c3Npb244MjI4Mg==con': \"I have a need to use a custom [DistributedDataParallel](https://pytorch.org/docs/stable/notes/ddp.html) implementation. I'd like to do this with the existing DDPBackend today.\\nWith Lightning, the API and docs are unclear as to whether I need to extend LightningDistributedDataParallel, or if I can directly extend torch DistributedDataParallel.\\nThe LightningModule API docs suggest configure_ddp should work with torch DistributedDataParallel: https://pytorch-lightning.readthedocs.io/en/latest/lightning-module.html#configure-ddp\\nHowever, there are spots in Lightning which rely on checking isinstance of the custom Lightning overrides: https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/model_connector.py#L31-L34 1\\nThe Lightning DDP also forwards calls to train/val/test step. Is this a requirement for custom DDP implementations when used with Lightning?\\nTLDR: should I subclass LightningDistributedDataParallel or DistributedDataParallel when I implement the model hook for configure_ddp?\\nAlso asked here: https://forums.pytorchlightning.ai/t/expectations-for-custom-data-parallel-implementations/162\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjI4OQ==con': \"❓ Questions and Help\\nBefore asking:\\n\\nsearch the issues.\\nsearch the docs.\\n\\n\\nWhat is your question?\\nI don't know whether this is a bug...\\nAs shown in the code below, I think the behavior of dp mode is unexpected? (The attribute is reset every batch)\\nWhen using ddp mode, everything is fine. (The property will be initialized only once per GPU)\\nCode\\nimport os\\nimport torch\\nfrom torch.nn import functional as F\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.datasets import MNIST\\nfrom torchvision import transforms\\nimport pytorch_lightning as pl\\nfrom pytorch_lightning import Trainer\\n\\nfrom argparse import Namespace\\n\\n\\nclass LitModel(pl.LightningModule):\\n\\n    def __init__(self):\\n        super().__init__()\\n        self.l1 = torch.nn.Linear(28 * 28, 10)\\n        self._dummy_property = None\\n\\n    @property\\n    def dummy_propery(self):\\n        if self._dummy_property is None:\\n            self._dummy_property = '*' * 30\\n            print('print only once per gpu')\\n        return self._dummy_property\\n\\n    def forward(self, x):\\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\\n\\n    def training_step(self, batch, batch_idx):\\n        print(self._dummy_property)\\n        # Access every batch\\n        self.dummy_propery\\n        print(self._dummy_property)\\n\\n        x, y = batch\\n        y_hat = self(x)\\n        loss = F.cross_entropy(y_hat, y)\\n        return pl.TrainResult(loss)\\n\\n    def configure_optimizers(self):\\n        return torch.optim.Adam(self.parameters(), lr=0.02)\\n\\n\\ntrain_loader = DataLoader(\\n    MNIST(\\n        os.getcwd(),\\n        download=True,\\n        transform=transforms.ToTensor()\\n    ),\\n    batch_size=128\\n)\\ntrainer = pl.Trainer(gpus=2,\\n                     distributed_backend='dp',\\n                     max_epochs=2)\\n\\nmodel = LitModel()\\ntrainer.fit(model, train_loader)\\nOutput\\nNone\\nprint only once per gpu\\n******************************\\nNone\\nprint only once per gpu\\n******************************\\nNone\\nprint only once per gpu\\n******************************\\nNone\\nprint only once per gpu\\n******************************\\n...\\n\\n   \\nWhat's your environment?\\n\\nVersion 0.9.0\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjI5Nw==con': \"❓ Questions and Help\\nWhat is your question?\\nThis is about DDP specifics and about the handling of functions in a script that we only want executed once (not for every GPU).\\nI think they are missing from the doc and I couldn't find answers elsewhere. I apologize if they have been covered already.\\nQuestions:\\n\\nDoes the regular torch.save(model.state_dict(), path) call work normally or does DDP complicate things for multiple GPUS?\\nDoes DDP run all the functions of a script for every GPU? For example, in the following script will the delete_and_remake_folder function be executed multiple times (that would result in conflicts)? Is there a way to specify functions to be run only once?\\nAm I correct that sync_batchnorm=True and precision=16 work in DDP?\\nDoes the trainer.test() function automatically aggregate results accross devices or is it required to set self.log(loss,  sync_dist=True) in the model?\\nAm I correct in assuming that, if we set num_workers=X in a Dataloader, the actual CPU core usage will be X*N for N GPUS?\\n\\nQuestions 1-4 are summarized in the following script and whether it works/can work.\\ndef main():\\n   delete_and_remake_folder() # I only want to run once\\n   model = Model()\\n   trainer = Trainer(gpus = 8, backend='ddp, sync_batchnorm=True, precision=16)\\n   trainer.fit()\\n   trainer.test()\\n   torch.save(model.pt_model.state_dict(), save_dir) # I probably only want to run once (?)\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjI5OA==con': \"❓ Questions and Help\\nProblem\\nJupyter terminal freezes, and connection to AWS node closes.\\nThe problem is reproducible with any Lightning example.\\nWhat have you tried?\\npython pl_examples/basic_examples/image_classifier.py --gpus 4 --accelerator ddp\\nWhat's your environment?\\nLinux using docker image\\nLightning 1.0.4\\npytorch 1.6\\ncuda 10.2\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjIwNw==con': \"What is your question?\\nHi, I'm trying to implement my project with your framework, however, I'd like to count the time each part costs to make full use of GPUs, but it's puzzling that the time count by myself is not the same as tqdm does. So could you give me some advice about what happened? From process bar, the time is 1.4s/it while data time is 0.003s, gpu time is 0.5~0.7s.\\nCode\\n            # what I add to the trainer\\n            # code added by me\\n            batch_start_tic = time.time()\\n            for batch_nb, data_batch in enumerate(self.tng_dataloader):\\n                self.batch_nb = batch_nb\\n                self.global_step += 1\\n\\n                model = self.__get_model()\\n                model.global_step = self.global_step\\n\\n                # stop when the flag is changed or we've gone past the amount\\n                #  requested in the batches\\n                self.total_batch_nb += 1\\n                met_batch_limit = batch_nb > self.nb_tng_batches\\n                if met_batch_limit:\\n                    break\\n\\n                # ---------------\\n                # RUN TRAIN STEP\\n                # ---------------\\n                batch_fb = time.time()\\n                batch_result = self.__run_tng_batch(data_batch, batch_nb)\\n                early_stop_epoch = batch_result == -1\\n                # code added by me\\n                batch_fb_end = time.time()\\n                self.__add_tqdm_metrics({'data time': batch_fb-batch_start_tic,'gpu time': batch_fb_end-batch_fb})\\n                batch_start_tic = time.time()\\nBy the way, I find the gpu utils is about 80%, is there any tricks can make it up to 100%?\\nWhat's your environment?\\n\\nPyTorch version   1.1.0\\nLightning version   0.3.6.9\\nTest-tube version  0.6.7.6\\n\\nThanks a lot.\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjIyMA==con': 'In ddp mode, if I use 8 gpus, then it will creates 8 processes for each gpu, if I want to create new tensor at runtime, can I use .cuda(self.trainer.root_gpu)? will it use correct gpu corresponding to its process?',\n",
              " 'MDEwOkRpc2N1c3Npb244MjIyNw==con': \"❓ Questions and Help\\nHaving 2 gpus with DP seems to be slowers than using just 1. Is it normal?\\nMy intuition is that if you are using 2 GPUs and the batch is being splitted into 2 batches, this should be faster. But when I tested the same code using 1 vs >1 my epoch time increased\\nCode\\nMinimalist Implementation of a BERT Sentence Classifier\\nWhat have you tried?\\nI also tried to run ddp but my code seems to break with a TypeError: cannot serialize '_io.TextIOWrapper' object error. I searched online but I couldn't find the reason...\\nWhat's your environment?\\n\\nOS: Linux\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjMwMA==con': \"Question regarding logging from lightning module in DDP model\\nFor example, here is a validation step function that computes accuracy.\\ndef validation_step(self, batch, batch_idx):\\n       x, y = batch\\n       logits = self(x)\\n       acc = acc_fun(logits,y)     \\n       self.log('val_acc', acc)\\nSo what happens in DDP, is the logged value averaged across GPUs? At the end of every epoch?\",\n",
              " 'MDEwOkRpc2N1c3Npb244MjMwMw==con': 'When using pytorch_lightning.tuner.lr_finder.lr_find, ddp have some error. So i change to dp using 4 gpus. Can the learning rate find by dp used by ddp? They have same gpu numbers.',\n",
              " 'MDEwOkRpc2N1c3Npb244MzU0MA==con': \"Problem\\nHi, Everyone. I have some question about ddp. Because I want to write predict result to file.\\nAnd I use trainer.test(model=model, test_dataloaders=test_dataloader) to process it. But I just get 1/ngpus dataset.\\nFor example:\\nGPUS: 2\\ntotal data: 10000\\npredict data: 5000(total data / GPUS)\\n\\nHope someone can help me to solve it. Because I have to write predict result to file.\\nEnvironment\\npytorch: 1.4.0\\npytorch-lightning: 1.1.1\\nGPUS: Tesla P100-PCIE-16GB * 2\\n\\nSample Code\\nclass ExampleModel(pl.LightningModule):\\n   def __init__(self):\\n       self.original_file = open('/path/to/file', 'a')\\n   def train_step(...):\\n       .....\\n   def test_step(self, batch, batch_idx):\\n        init_ids = batch['init_ids']\\n        attention_mask = batch['attention_mask']\\n        token_type_ids = batch['token_type_ids']\\n        predictions = self.model(init_ids, attention_mask, token_type_ids)\\n        ..........\\n        self.original_file.write(convert_ids_to_str(init_ids.cpu().numpy()) + '\\\\t' + str(seg.cpu().numpy()[0]) + '\\\\n')\\n\\n\\ntrainer\\n\\ntrainer = pl.Trainer(max_epochs=EPOCH,\\n                    gpus=[0,1], \\n                    num_nodes=1,\\n                    auto_select_gpus=True,\\n                    num_sanity_val_steps=0,\\n                    accelerator='ddp',\\n                    callbacks=[modelcheckpoint_callback, earlystopping_callback])\\ntrainer.test(model=model, test_dataloaders=test_dataloader)\\nHope someone can help or answer how to do it.\",\n",
              " 'MDEwOkRpc2N1c3Npb24xNjMyMDI5con': \"Hi pytorch-lightning friends! :) I'd like to start a discussion about trainer.check api.\\nBasically, this API should check that all the user-define classes (models, data, callbacks, ...) are programmatically sound. I propose to use inspect to check for function correctness, here's my PR proposal at #3244\",\n",
              " 'MDEwOkRpc2N1c3Npb24xOTI2Mzgzcon': \"Hi,\\nI was wondering if PL can use mkldnn/dnnl/onednn ? I may be mistaken, but it seems pytorch does not default to it and there is no explicit mention of it inside PL's code. If it's not, is there a way to enable it ?\\nAlexandre\",\n",
              " 'MDEwOkRpc2N1c3Npb24yMTc5Njk3con': 'Hi,\\nI\\'m using ModelCheckpoint Callback to save my model checkpoint. pytorch lightning automatically attaches -v0, -v1 to the filename I specified if it finds checkpoint models exist in dirpath.  Instead of saving all the models from different runs, is there a way to make the ModelCheckpoint Callback only save one model in the checkpoint folder and just override the model from previous runs?\\nFor example, my ModelCheckpoint is as follow:\\nModelCheckpoint(monitor=\\'valid_score\\',\\n                              dirpath=\"./checkpoint/\",\\n                              filename=\"model\",\\n                              mode=\\'max\\', save_top_k=1))\\n\\nIf I run the code for 3 three times, my checkpoints folder will have the following:\\n- checkpoint:\\n    - model.ckpt\\n    - model-v0.ckpt\\n    - model-v1.ckpt\\n\\nWould it be possible to just have model.ckpt in my checkpoint folder no matter how many times I run the code?',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkxNTQ4con': 'Weights summary gets printed when Trainer calls fit(), but the output is not persistent as I have everything wrapped up in ray tune, which overwrites the contents of the output cell in jupyter\\nIs there something we can call to manually print the weights summary of a particular model without having to fit the model every time?',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkxODc3con': \"❓ Questions and Help\\nWhat is your question?\\nHow to accumulate metrics for multiple validation dataloaders separately? Currently the metrics are accumulated for all dataloaders simultaneously.\\nCode\\nThe validation step accepts dataset_idx parameter when running validation with multiple dataloaders.\\ndef validation_step(self, batch, batch_idx, dataset_idx: Optional[int] = None):\\nHowever I'm not sure how to update the metrics separately for each dataloader. Would I have to create separate metrics, one for dataset A and second for B? Or maybe my metric could accept the dataset_idx parameter to know for which ds it should log given output.\\nThis however wouldn't work with pl factory metrics like average precision, since they are dataset agnostic?\\ndef update(self, preds: torch.Tensor, target: torch.Tensor):\\nNot sure how to approach this.\",\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkxOTMycon': \"❓ Questions and Help\\nWhat is your question?\\nI'm trying to run the LitAutoEncoder on TPUs, but the code runs for 1 epoch and gets stuck there.\\nCode\\nclass LitAutoEncoder(pl.LightningModule):\\n\\n    def __init__(self, hparams):\\n        super().__init__()\\n        self.hparams = hparams\\n        self.encoder = nn.Sequential(\\n            nn.Linear(28*28, 64),\\n            nn.ReLU(),\\n            nn.Linear(64, 3)\\n        )\\n        self.decoder = nn.Sequential(\\n            nn.Linear(3, 64),\\n            nn.ReLU(),\\n            nn.Linear(64, 28*28)\\n        )\\n\\n    def forward(self, x):\\n        # in lightning, forward defines the prediction/inference actions\\n        embedding = self.encoder(x)\\n        return embedding\\n\\n    def training_step(self, batch, batch_idx):\\n        # training_step defined the train loop.\\n        # It is independent of forward\\n        x, y = batch\\n        x = x.view(x.size(0), -1)\\n        z = self.encoder(x)\\n        x_hat = self.decoder(z)\\n        loss = F.mse_loss(x_hat, x)\\n        # Logging to TensorBoard by default\\n        self.log('train_loss', loss)\\n        return loss\\n\\n    def configure_optimizers(self):\\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\\n        return optimizer\\n\\ndataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\\ntrain_loader = DataLoader(dataset, drop_last=True, batch_size=32)\\n\\nargs_dict = dict(\\n    num_train_epochs=1,\\n    seed=42,\\n)\\n\\nargs = argparse.Namespace(**args_dict)\\nautoencoder = LitAutoEncoder(args)\\n\\ntrain_params = dict(\\n    tpu_cores=8,\\n    progress_bar_refresh_rate=30,\\n)\\n\\ntrainer = pl.Trainer(**train_params)\\ntrainer.fit(autoencoder, train_loader)\\n\\n\\nReproducible Colab Notebook\\nNotebook\\nWhat's your environment?\\n\\nColab\\nPackaging pip\\npytorch-1.7\\npytorch-lightning-1.1.5\",\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkxOTU0con': \"🐛 Bug\\nRuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your \\nmodule has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the \\nkeyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward` \\nfunction outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel \\nmodule wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss \\nfunction and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\\n\\nwhen I use pt 1.0.8, my model is ok, but when I switch to 1.1.4, it throws this error.  It seems 1.0.8 enable unused parameters by default, but 1.1.4 not.  How to solve this problem.\\nI think switch find_unused_parameters=True by default to False is a breaking change, but in docs, it doesn't mention, yet no clear instructions to set to True .\",\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyMjExcon': 'I wanted to ask how pytorch handles accuracy (and maybe even loss) logging when we have something like pl.Trainer(accumulate_grad_batches=ACCUMULATIONS).\\nMy training looks like this:\\n    def training_step(self, batch, batch_idx):\\n        x, y = batch\\n        y_hat = self(x)\\n        loss = F.cross_entropy(y_hat, y, weight=self.weight)\\n        result = pl.TrainResult(loss)\\n        result.log(\"train_loss\", loss, prog_bar=True)\\n        result.log(\"train_accuracy\", self.accuracy(y_hat.argmax(dim=-1), y), prog_bar=True)\\n\\n        return result\\nwhere self.accuracy = pl.metrics.classification.Accuracy(). Is there a way to make sure that the loss and accuracy is averaged across the accumulated batches?\\nIf this is not currently the case, I\\'m happy to do a PR if someone can show me where to look in the source code to make such a change.\\nThanks in advance',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyMjYxcon': 'When Precision and Recall are directly computed, I get the following result:\\nimport torch\\nfrom pytorch_lightning.metrics import Precision\\nfrom pytorch_lightning.metrics import Recall\\n\\ny = torch.tensor([0, 0, 2, 2, 1, 1, 1, 2, 0, 0])\\ny_hat = torch.tensor([1, 1, 2, 1, 1, 1, 1, 1, 2, 1])\\n\\nprecision = Precision(num_classes=3)\\nrecall = Recall(num_classes=3)\\nprecision(y_hat, y)\\n#>>>tensor(0.2917)\\nrecall(y_hat, y)\\n#>>>tensor(0.4444)\\nHowever, when the same metrics are computed over validation_step, I get the following stranger result:\\n    def validation_step(self, batch, batch_idx):\\n        x, y = batch[\"x\"], batch[\"y\"] # y = tensor([0, 0, 2, 2, 1, 1, 1, 2, 0, 0], device=\\'cuda:0\\')\\n        y_hat = self(x) # y_hat  = tensor([1, 1, 2, 1, 1, 1, 1, 1, 2, 1], device=\\'cuda:0\\')\\n\\n        precision = self.precision_score(y_hat, y) # precision =  tensor(0.4000, device=\\'cuda:0\\')\\n        recall = self.recall_score(y_hat, y) # recall = tensor(0.4000, device=\\'cuda:0\\')\\nwhat am I missing?',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyMzU2con': \"Hi, thanks for the nice library. In the readme, the example uses  model.forward(x) not model(x). But wouldn't it usually recommended to use model(x) so that other things (hooks etc) can be, well, hooked as well? What's the best practice?\",\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyMzkwcon': 'What would be the most lightning way to restore the best model? Either directly after training (in the same script) or for later use (in another script)?\\nThanks in advance !',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNDcxcon': \"Hey! I have a question regarding this library. Really, like how it forces me to structure my code better. I encountered one problem I did not know how to solve based on the documentation.\\nLet's say I have two optimizers for two parts of the network, e.g. my configure_optimizers() looks like this:\\n    def configure_optimizers(self):\\n        optimizer_encoder = optim.Adam(self.encoder.parameters(), ...)\\n        optimizer_decoder = optim.Adam(self.decoder.parameters(), ...)\\n        return [optimizer_encoder, optimizer_decoder]\\n\\nnow in the training loop I forward pass the encoder, then the decoder and compute my loss based on the output:\\n    def training_step(self, batch, batch_nb, optimizer_idx):\\n        inp, gt = ...\\n\\n        encoding = self.encoder(inp)\\n        pred = self.decoder(encoding)\\n\\n        loss = F.mse_loss(pred, gt)\\n        return {'loss': loss}\\n\\nSince I have two optimizers I have to respect that this function is called two times with different optimizer_idx however I just have one loss to backprop. How would I go about this?\\nWhat have you tried?\\nI tried something like this\\n    def training_step(self, batch, batch_nb, optimizer_idx):\\n        if optimizer_idx == 1:\\n              return {}\\n        inp, gt = ...\\n\\n        encoding = self.encoder(inp)\\n        pred = self.decoder(encoding)\\n\\n        loss = F.mse_loss(pred, gt)\\n        return {'loss': loss}\\n\\nHowever, this leads to an error since no loss key is present in trainer.py:1392.\",\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNDgycon': \"Freezing layers at the beginning of training works, however unfreezing in on_epoch_start() during training causes the gradient to explode. Without the unfreezing part (or without freezing at all), the model trains fine with no gradient issues.\\nI'm using DDP + Apex O2 and the loss scaling will keep going down to 0 where it would encounter 0 division and crash.\\nIs unfreezing during training not possible in pytorch/lightning? or am I missing snippet?\",\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTAzcon': \"If you still can't find what you need:\\nWhat is your question?\\nI think it's unclear how the training data is split into a training and validation split in the minimal example.\\n(https://williamfalcon.github.io/pytorch-lightning/LightningModule/RequiredTrainerInterface/#minimal-example)\\nDoes this example use all training data for both training and validation? As far as I'm aware, this is bad practice.\\nIs there some magic background process which compares the training and validation data loaders and does splitting? I skimmed through the code and couldn't find anything.\",\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTEycon': 'Hi,\\nI am tring to use BERT for a project. The pretrained BERT model is part of my model. I am wondering how will PL initialize the model weights. Will it overwrite the pretrained BERT weights?\\nThanks.',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTIzcon': \"What is your question?\\nAfter a training, is there an easy way to get the best scores returned by the validation_end function? In order to use a hyperparameters optimizer like Tune.\\nCode example\\nmodel = CoolSystem()\\ntrainer = Trainer()    \\ntrainer.fit(model)   \\n\\nbest_scores = ???\\nprint(best_scores)\\n\\nWhat's your environment?\\n\\nconda 4.7.10\\nPyTorch 1.3.0\\nLightning 0.5.3.1\",\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTM0con': \"What is your question?\\nI want my tqdm logger to show me a history of my training on the terminal. Right now, when a epoch ends, all data for it is scrubbed from the command line and the new epoch data is shown.\\nAlso I want to see the running accuracy of my network and a running average of my loss on the tqdm bar. How should I go on about doing that ?\\nWhat have you tried?\\nI have looked at the docs and logging but am unable to figure out how to modify the tqdm logger, more so maintain a running average\\nWhat's your environment?\\n\\nconda version: latest\\nPyTorch version   : 1.3\\nLightning version : pip install pytorch-lightning at the date of this issue\\nTest-tube version: came boot strapped with lightning.\\n\\nI installed everything on the date of this issue.\",\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTMwcon': 'How do I set the number of epochs to train?\\nWhat have you tried?\\nLooking for documentation.\\nLooking for examples.',\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTQ2con': \"Hi, thanks for the nice product again.\\nFrom #525 and #599, I could guess that hparams is required to load a saved model (which I think should be mentioned somewhere in the doc btw). And from the examples, seems like hparams may be argparse.Namespace. Unfortunately though, it was not so easy to understand the concept.\\nWhat is hparams exactly? What kind of information it should/can/should not include to work properly? Is it recommended to use hyperparameter argument parser? Say, if I'm not into hyperparameter search at the moment and just want to be able to load the checkpoint model, what is the requirement on the hparams?\",\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTU3con': \"Where is EarlyStopping search for metrics?\\nCode\\n    def validation_end(self, outputs):\\n        ...\\n        metrics = {\\n        'val_acc': val_acc,\\n        'val_loss': val_loss\\n        }\\n        ...\\n        output = OrderedDict({\\n            'val_acc':  torch.tensor(metrics['val_acc']),\\n            'val_loss': torch.tensor(metrics['val_loss']),\\n            'progress_bar': metrics,\\n            'log': metrics\\n        })\\n        return output\\n\\nif I attempt to early stop according to val_acc I get the following error:\\nRuntimeWarning: Early stopping conditioned on metric 'val_acc' which is not available. Available metrics are: loss,train_loss\\n\\nThe metrics mentioned (loss,train_loss) are from training_step from what I could find.\\nI guess I'm doing something wrong, could anyone point me in the correct direction?\\n\\nOS: Ubuntu\\nPackaging: pip\\nVersion 0.5.3.2\\n\\n\\nUpdate #1: the same code works with version 0.5.1. Bug in 0.5.3?\\nUpdate #2:\\nI found that this line in trainer/training_loop.py:\\nself.callback_metrics = {k: v for d in all_callback_metrics for k, v in d.items()}\\n\\nFrom what I see, before this line is executed, self.callback_metrics contains val_acc. After this line values that were put in callback_metrics after validation are gone, therefore EarlyStopping can't find them. Can anyone confirm this is an issue?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMTc5NTk3con': 'I am using Pytorch Lightning in an RL setting and want to save a model when it hits a new max average reward. I am using the Tensorboard logger where I return my neural network loss in the training_step() using:\\nlogs = {\"policy_loss\": pred_loss}\\nreturn {\\'loss\\':pred_loss, \\'log\\':logs}\\n\\nAnd then I am saving my RL environment rewards using in on_epoch_end():\\nself.logger.experiment.add_scalar(\"mean_reward\", np.mean(reward_losses), self.global_step)\\nself.logger.experiment.add_scalars(\\'rollout_stats\\', {\"std_reward\":np.std(reward_losses),\\n                \"max_reward\":np.max(reward_losses), \"min_reward\":np.min(reward_losses)}, self.global_step)\\n\\nAnd every 5 epochs I am also writing out another RL reward loss where I use the best actions rather than sampling from them:\\nif self.current_epoch % self.hparams[\\'eval_every\\']==0 and self.logger:\\n            output = self.collect_rollouts(greedy=True, num_episodes=self.hparams[\\'eval_episodes\\'])\\n            reward_losses = output[0]\\n            self.logger.experiment.add_scalar(\"eval_mean\", np.mean(reward_losses), self.global_step)\\n\\nMy question is, how can I set my ModelCheckpoint to monitor eval_mean (which is only written out every 5 epochs, this seems like it would be a problem)? I would also settle for monitoring mean_reward (written out every epoch)? Right now I can only successfully monitor policy_loss which does not always correspond to higher rewards obtained (setting monitor = to anything else throws an error).\\nI know that in the new PL version self.log() should be used but after re-writing my code using this it still didn\\'t solve my issue.\\nI have spent a lot of time looking through the docs and for examples of this but I have found the logging docs on this to be quite sparse and difficult to even get everything to log in the first place.\\nI am using Pytorch Lightning 1.0.5 and Pytorch 1.7.0.\\nThank you for any help/guidance.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMTc5Njcwcon': 'Hi, I don\\'t think this is a bug but I\\'m doing something wrong. I want to use my val_dice as hp_metric tabular AND also see the graph on \"show metric\" (radio button) under the Tensorboard HPARAMS tab:\\n\\nTo achieve this I\\'m logging using self.log(\\'hp_metric\\', mean_dice) (for the graph) and self.logger.log_hyperparams(params=self.hparams, metrics={\\'hp_metric\\': mean_val_dice}) (for the hparams value), both in the function validation_epoch_end\\nHow do I get rid of the initial -1 value? How can I fix my graph so it doesn\\'t draw any points at x=0? (zoomed in version)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjIzNDA4con': '🐛 Bug\\n\\nWhen trainer run_test() called, the results from test cannot properly handle a 1D tensor in the results dictionary.\\nSuch error will happen:\\n/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in run_test(self)\\n708                 for k, v in result.items():\\n709                     if isinstance(v, torch.Tensor):\\n--> 710                         result[k] = v.cpu().item()\\n711\\n712         return eval_loop_results\\nValueError: only one element tensors can be converted to Python scalars\\nPlease reproduce using the BoringModel\\n\\nTo Reproduce\\nTo reproduce with BoringModel, only need to replace the test_epoch_end.\\ndef test_epoch_end(self, outputs) -> None:\\n    torch.stack([x[\"y\"] for x in outputs]).mean()\\n    f1_score = torch.tensor([1,1,1,1])\\n    return {\\'f1_score\\': f1_score}\\n\\nExpected behavior\\ndef run_test(self):\\n\\n        # remove the tensors from the eval results\\n        for i, result in enumerate(eval_loop_results):\\n            if isinstance(result, dict):\\n                for k, v in result.items():\\n                    if isinstance(v, torch.Tensor):\\n                        # should check if you can call .item()\\n                        result[k] = v.cpu().item()\\n\\nEnvironment\\n\\nPyTorch Version (e.g., 1.0): 1.1.8\\n\\nAdditional context',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjMyNTUwcon': \"I have a UNet model. I'm trying for a regression model since, in my output, I have different floating values for each pixel. In order to check the r2score, I tried to put the below code in the 'model class', training_step, validation_step, and test_step.\\nfrom pytorch_lightning.metrics.functional import r2score\\nr2 = r2score(logits, y)\\nself.log('r2:',r2)\\nBut it's giving the following error\\n\\nValueError: Expected both prediction and target to be 1D or 2D tensors, but recevied tensors with dimension torch.Size([50, 1, 32, 32])\\n\\nHow can I check my model fit?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjMyNzk1con': \"Hey,\\nwhat is the canonical way to access/change the optimizer of a pl.LightningModule after model.setup('fit') was called?\\nE.g. if I follow https://pytorch-lightning.readthedocs.io/en/latest/lr_finder.html 4 (the last part where I explicitly plot the found lr and then change it) I don’t see a way how to change the optimizers lr.\\nIf I call model.configure_optimizers() it’ll just return the optimizer and not set them.\\nIs there no official way to access and change them after construction?\\nCheers\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjMzMTgwcon': 'Hi, I’m trying to apply CNN to each image in a video. Currently, my implementation uses a for loop and torch.cat where I take each image and apply the CNN module in the loop. But clearly, this is sequential and I don’t see why it can’t be parallelized in theory since all images are independent from each other.\\nHowever, I’m not sure how this can be accomplished. I couldn’t find any built-in function for PyTorch. Is there a way to do this in parallel in PyTorch Lightning?\\nMy video input shape looks like this: (batch_size, seq_len, channel, height, width) and CNN takes input shape of (batch_size, channel, height, width).\\nThanks in advance for your help!',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQ4NTU2con': 'How do I save checkpoints every, as well as deleting and/or overwriting the previously saved checkpoints?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQ5MTMxcon': 'Because I have a very small network which is quick at training and I want to compare many different runs, I want to disable Lightning\\'s default output to both console and file system. I already tried setting weights_summary=False, checkpoint_callback=False, logger=False and logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR), but I still get the progress bar and the GPU availability report. How can I disable these?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQ5ODE4con': \"I've used seed_everything(7) to initially set the seed then passed deterministic=True, accelerator='ddp' to Trainer to have it run on 4 GPUs.\\nThen I load my map-style dataset using a plain DataLoader with shuffle=True, num_workers=10 .\\nNow what happens is that each of the forked DDP processes spin up N (here 10) worker processes to read the data. So total 4 x 10 DataLoader processes. I have tried setting up a worker_init_fn to see the seeds they each receive, and indeed the worker processes for each GPU get different seeds, but they are the same across worker processes of different GPUs. This causes each data item to be read 4 times (the number of GPU / DDP processes) which I checked in the dataset's __getitem__. So the indexes for example would look like [3,3,3,3,7,7,7,7,2,2,2,2,...].\\nWhat is the way to fix this? Shouldn't the DistributedSampler for DDP automatically get a seed based on the subprocess that it is forked on? (Similar to DataLoader's worker_init_fn)\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQyMjE4con': 'Trying to understand compute() for logging. Could someone please tell if the self.log(train_acc..., on_epoch=True) is same as def training_epoch_end() which uses compute - code below\\ndef __init__():\\n    # ...\\n    self.train_acc = pl.metrics.Accuracy()\\n    self.valid_acc = pl.metrics.Accuracy()\\n\\ndef training_step(self, batch, batch_idx):\\n    # ...\\n    self.train_acc(pred, target)\\n    self.log(\"train_acc\", self.train_acc, on_step=False, on_epoch=True)\\n    self.log(\\'train_loss\\', loss, on_step=True, on_epoch=False)\\n\\n    return loss\\n\\ndef training_epoch_end(self, training_step_outputs):\\n    self.log(\\'train_acc_epoch\\', self.train_acc.compute())\\n    \\n\\ndef validation_step(self, batch, batch_idx):\\n    # ...\\n    self.valid_acc(pred, target)\\n    self.log(\\'val_loss\\', loss, prog_bar=True)\\n    self.log(\\'val_acc\\', self.valid_acc, on_step=False, on_epoch=True, prog_bar=True)\\n\\n    return loss\\n\\n\\ndef validation_epoch_end(self, validation_step_outputs):\\n    self.log(\\'valid_acc_epoch\\', self.valid_acc.compute())',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQyMjUycon': \"Hi, I'm trying to refactor the official NLP (sentiment analysis) tutorial, using Lightning in order to take advantage of things like early stopping etc.\\nI'm moving first steps, and the main hurdle is the creation of a Lightning module, and in particular coding the training_step.\\nWhat I came up so far is\\nclass LitTextClassifier(pl.LightningModule):\\n    def __init__(self, num_class, criterion = CrossEntropyLoss):\\n        super().__init__()\\n        self.embedding = nn.EmbeddingBag(VOCAB_SIZE, EMBED_DIM, sparse=False)\\n        self.fc = nn.Linear(EMBED_DIM, num_class)\\n        self.init_weights()\\n        self.criterion = criterion\\n\\n    def init_weights(self):\\n        initrange = 0.5\\n        self.embedding.weight.data.uniform_(-initrange, initrange)\\n        self.fc.weight.data.uniform_(-initrange, initrange)\\n        self.fc.bias.data.zero_()\\n\\n    def forward(self, text, offsets):\\n        embedded = self.embedding(text, offsets)\\n        return self.fc(embedded)\\n\\n    def configure_optimizers(self):\\n        optimizer = optim.SGD(self.parameters(), lr=4.0)\\n        return optimizer\\n\\n    def training_step(self, batch, batch_idx):\\n        # I am messing up things here\\n        text, offsets, cls = batch\\n        output = self.forward(text, offsets)\\n        loss = self.criterion(output, cls)\\n\\n        return loss\\nI think I am getting the training_step wrong. Can someone provide guidance here?\\nA full gist to reproduce code + errors I get is here: https://gist.github.com/davidefiocco/3b6c6b1e09c4f664b3a73e5bf24d1668/5aa4c224f7772db835bbaa92d559837c7a40f4df\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQyNzg5con': 'Hi,\\nI am trying to incorporate the scheduler provided here.\\nI went through the documentation of PyTorch lightning and it allows only scheduler interval \"epoch\" or \"step\". But in the scheduler attached, it needs to call \"step\" every epoch and  \"batch step\" after each iteration. Could someone help me figure out a way to do this? Any help will be appreciated.\\nThank you,\\nBest,\\nShreyas Kamath',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQzMTM4con': 'Hi,\\nI am getting this weird error; I was able to run my code before and today I got this:\\nimport pytorch_lightning as pl\\n\"~/dir/miniconda3/envs/pytorchenv/lib/python3.7/site-packages/pytorch_lightning/init.py\", line 66, in \\nfrom pytorch_lightning import metrics\\nImportError: cannot import name \\'metrics\\' from \\'pytorch_lightning\\'',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQzNDM5con': 'On a Colab TPU, I get the error:\\nFile \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 42, in overwrite_by_env_vars\\n    return fn(self, **kwargs)\\nTypeError: __init__() got an unexpected keyword argument \\'num_tpu_cores\\'\\n\\nI\\'m running a script that is in my Google Drive. I copy the data up onto Colab. When I do the exact same process for the GPU, it works (with Colab set to GPU, and the appropriate trainer parameter adjustment).\\nThe trainer instantiation is simply\\n    trainer = pl.Trainer(logger=logger,\\n                         num_tpu_cores=8,\\n                         fast_dev_run=False,\\n                         max_epochs=20)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjU0ODExcon': 'What I want it to look like:\\n\\nWhat I got now:\\n\\nI think I do have logged metrics, how to make it printed beautifully ?\\nclass SortNumberModel(pl.LightningModule):\\n    def __init__(self, hf_config, lr):\\n        super().__init__()\\n        self.save_hyperparameters()\\n        self.model = AutoModelForTokenClassification.from_config(hf_config)\\n        self.val_acc = pl.metrics.Accuracy()\\n\\n    def forward(self, batch):\\n        result = self.model(input_ids=batch[\"x\"], labels=batch[\"y\"], return_dict=True)\\n        return result.logits.argmax(dim=-1), result.loss\\n\\n    def training_step(self, batch, batch_idx):\\n        return self.forward(batch)[-1]\\n\\n    def validation_step(self, batch, batch_idx):\\n        preds, loss = self.forward(batch)\\n        self.val_acc(preds.view(-1), batch[\"y\"].view(-1))\\n        self.log(\"valid_acc\", self.val_acc, on_epoch=True)\\n\\n    def configure_optimizers(self):\\n        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, eps=1e-5)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjU1MDkxcon': 'I want to create a custom Callback class where I can access certain attributes from my DataModule and log/save them before the start of the train step.\\nI am little confused on how to do this. Can anyone help me out with a quick snippet? Thanks!',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjU1NDQ5con': 'I can install pl in colab by\\n!pip install pytorch-lightning==1.2.2 --quiet\\nbut I cannot import it by\\nimport pytorch_lightning as pl\\nI am thankful if you help me with this issue.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjU2NDQ1con': 'Changing val_check_interval from 0.01 to 0.005 changed the number of steps in an epoch from 2k to 3k for one of my experiments. Wanted to know if that is expected behavior.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjU4OTQzcon': 'What I did:\\ndef configure_optimizers(self):\\n\\n    optimizer = torch.optim.AdamW(\\n        self.parameters(),\\n        lr=self.hparams.learning_rate,\\n        eps=1e-5,\\n    )\\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\\n        optimizer,\\n        max_lr=self.hparams.learning_rate,\\n        epochs=self.trainer.max_epochs,\\n        steps_per_epoch=len(self.datamodule.train_dataloader()),\\n    )\\n    set_trace()\\n    return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\\nI put an breakpoint on scheduler.step to see when it will be called.\\nWhat I got:\\n\\nAnd I found\\n\\nI ran into breakpoint (scheduler.step is called) only on the end of validation.\\nepochs=self.trainer.max_epochs=1 and steps_per_epoch=len(self.datamodule.train_dataloader())=144, which are correct.\\n\\nCode for your reference if needed\\nimport random\\nfrom IPython.core.debugger import set_trace\\nimport torch\\nimport torch.nn.functional as F\\nfrom transformers import AutoConfig, ElectraForTokenClassification\\nimport pytorch_lightning as pl\\nfrom pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\\nfrom pl_bolts.callbacks import PrintTableMetricsCallback\\nfrom pytorch_lightning.loggers import WandbLogger\\n\\nclass SortNumberDataset(torch.utils.data.Dataset):\\n    def __init__(self, dataset_size, vocab_size, sequence_length):\\n        super().__init__()\\n        self.dataset_size = dataset_size\\n        self.vocab_size = vocab_size\\n        self.sequence_length = sequence_length\\n\\n    def __getitem__(self, i):\\n        x = [\\n            random.randint(0, self.vocab_size - 1) for _ in range(self.sequence_length)\\n        ]\\n        y = sorted(x)\\n        return {\"x\": torch.tensor(x), \"y\": torch.tensor(y)}\\n\\n    def __len__(self):\\n        return self.dataset_size\\n\\n\\nclass NumberSorting(pl.LightningModule):\\n    def __init__(\\n        self,\\n        hf_config,\\n        learning_rate,\\n        trainset_size,\\n        valset_size,\\n        vocab_size,\\n        sequence_length,\\n        batch_size=128,\\n        num_workers=4,\\n    ):\\n        super().__init__()\\n        self.save_hyperparameters()\\n        self.datamodule = pl.LightningDataModule.from_datasets(\\n            SortNumberDataset(trainset_size, vocab_size, sequence_length),\\n            SortNumberDataset(valset_size, vocab_size, sequence_length),\\n            batch_size=batch_size,\\n            num_workers=num_workers,\\n        )\\n        # self.model = ElectraForTokenClassification(hf_config)\\n         # tie input/output embeddings\\n         delattr(self.model, \"classifier\")\\n         self.model.classifier = (\\n             lambda x: x @ self.model.electra.embeddings.word_embeddings.weight.t()\\n        )\\n        self.val_acc = pl.metrics.Accuracy()\\n\\n    def forward(self, batch):\\n        result = self.model(input_ids=batch[\"x\"], labels=batch[\"y\"], return_dict=True)\\n        return result.logits.argmax(dim=-1), result.loss\\n\\n    def training_step(self, batch, batch_idx):\\n        # self.log(\"lr\", self.trainer.optimizers[0].param_groups[0][\"lr\"])\\n        return self(batch)[-1]\\n\\n    def validation_step(self, batch, batch_idx):\\n        preds, loss = self(batch)\\n        self.val_acc(preds.view(-1), batch[\"y\"].view(-1))\\n        self.log(\"valid_acc\", self.val_acc, on_epoch=True)\\n\\n    def configure_optimizers(self):\\n        timizer = torch.optim.AdamW(optimizer_grouped_parameters, eps=1e-5)\\n        optimizer = torch.optim.AdamW(\\n            self.parameters(),\\n            lr=self.hparams.learning_rate,\\n        )\\n        # return optimizer\\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\\n            optimizer,\\n            max_lr=self.hparams.learning_rate,\\n            epochs=self.trainer.max_epochs,\\n            steps_per_epoch=len(self.datamodule.train_dataloader()),\\n        )\\n        set_trace()\\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\\n\\n\\nconfig = AutoConfig.from_pretrained(\\n    \"google/electra-small-generator\",\\n    pad_token_id=-1,\\n    max_position_embeddings=7,\\n    vocab_size=7,\\n    num_labels=7,\\n    embedding_size=10,\\n    hidden_size=10,\\n    intermediate_size=8,\\n    num_hidden_layers=2,\\n    num_attention_heads=2,\\n)\\nplmodule = NumberSorting(\\n    config,\\n    learning_rate=0.05,\\n    trainset_size=18333,\\n    valset_size=20000,\\n    vocab_size=7,\\n    sequence_length=7,\\n)\\n\\n\\ntrainer = pl.Trainer(\\n    max_epochs=1,\\n    gpus=\"0\",\\n    callbacks=[\\n        PrintTableMetricsCallback(),\\n        LearningRateMonitor(logging_interval=\"step\", log_momentum=True),\\n    ],\\n    logger=WandbLogger(),\\n)\\ntrainer.fit(plmodule)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjUwMjU0con': \"I wrote a Python script that loops over sets of hyperparameters and for each set calls trainer.fit(). Subsequently trainer.test() is currently called 2 times, each for the best checkpoint that was logged for two metrics.\\nThis script is executable via commandline and works well on CPU an 1 GPU for any number of hyperparameter sets.\\nI want to run this code on multiple GPUs (2-4) and wonder if I could use 'ddp'?\\nThe latest documentation says that if your script needs to invoke both .fit and .test, or one of them multiple times 'ddp' mode isn't possible.\\nHowever, @awaelchli indicated here, that running trainer.fit() and trainer.test() in 'ddp' mode is now possible but the documentation is outdated.\\nQuestions:\\n\\nIs it now possible to run both  trainer.fit() and subsequently trainer.test() (and eventually trainer.test() mutiple times) in 'ddp' mode?\\nWould I need to reorganize my code so that the loop over hyperparameter sets invokes a subprocess.call to actually instantiate every new trainer for 'ddp' in another process?\\nIf calling .test() after .fit() in 'ddp' is still not possible, would it be a workaround to instantiate trainer again after .fit() is finished and just load the model weights from the best checkpoint for testing (to avoid using the same trainer object)?\\n\\nI would be really happy for some help on that.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjUwMzc1con': 'How would one correctly apply the Precision metric from v1.2.0 on, with the revised metrics api?\\nI am currently doing something like this:\\nimport torch\\nfrom pytorch_lightning import metrics\\n\\n# example data\\npreds = [0] * 200 + [1] * 30 + [0] * 10 + [1] * 20\\ntargets = [0] * 200 + [1] * 30 + [1] * 10 + [0] * 20\\n\\npreds = torch.tensor(preds)\\ntargets = torch.tensor(targets)\\n\\n# define method for printing metrics\\ndef _print_some_metrics(preds, targets, num_classes):\\n    precision = metrics.classification.Precision(\\n        num_classes=None, is_multiclass=False)\\n    recall = metrics.classification.Recall(\\n        num_classes=None, is_multiclass=False)\\n    f1 = metrics.classification.F1(num_classes=num_classes)\\n    f1beta = metrics.classification.FBeta(\\n        num_classes=num_classes,\\n        beta=2\\n    )\\n\\n    accuracy = metrics.classification.Accuracy()\\n    avg_precision = metrics.classification.AveragePrecision(\\n        num_classes=None)\\n    confusion_matrix = metrics.ConfusionMatrix(num_classes=2)\\n\\n    # print results\\n    print(\"Precision:\\\\n{}\\\\n\".format(precision(preds, targets)))\\n    print(\"Recall:\\\\n{}\\\\n\".format(recall(preds, targets)))\\n    print(\"F1:\\\\n{}\\\\n\".format(f1(preds, targets)))\\n    print(\"F1-Beta:\\\\n{}\\\\n\".format(f1beta(preds, targets)))\\n\\n    print(\"AVG Precision:\\\\n{}\\\\n\".format(avg_precision(preds, targets)))\\n    print(\"Accuracy:\\\\n{}\\\\n\".format(accuracy(preds, targets)))\\n    print(\"ConfMat:\\\\n{}\\\\n\".format(confusion_matrix(preds, targets)))\\n\\n_print_some_metrics(preds, targets, num_classes=2)\\nWhich gives me these results:\\n\\nPrecision:\\n0.6000000238418579\\n\\n\\nRecall:\\n0.75\\n\\n\\nF1:\\n0.8846153616905212\\n\\n\\nF1-Beta:\\n0.8846153616905212\\n\\n\\nAVG Precision:\\n0.48846155405044556\\n\\n\\nAccuracy:\\n0.8846153616905212\\n\\n\\nConfMat:\\ntensor([[200.,  20.],\\n[ 10.,  30.]])\\n\\nHowever, when calculating precision by hand (TP / TP + FN) with the numbers from the contingency table, I get 30 / 50 = 0.6\\nWhy does applying the precision class result in this (small) deviation?\\nFurther, when logging the metrics on epoch_end steps inside my model, I am not able to reproduce the logged precision, recall or accuraccy numbers on the validation set with the output from the contingency table, logged on the same steps (I haven\\'t validated the other metrics yet by hand).\\nIt would be great to get some help on how to correctly apply the new metrics API for a binary use case.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjUxMDg5con': 'nvm\\ncheers',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjY0NDkwcon': 'I need to pretrain the encode and decode section of an autoencoder first, then later attach a transformer in the middle of the encode and decode section. When I load the weights of the encode and decode section when pretraining it first, while initializing the weights of the transformer section, will I get an error about missing layers?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjY1OTM5con': \"My code is like this:\\nmodel = MyLitModel()\\ntrainer = Trainer(gpus=1)\\ntrainer.validate(model, dataloader)\\nHowever, I got an AttributeError:\\nAttributeError: type object 'Trainer' has no attribute 'validate'\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjY5NDYxcon': \"I want to resume training from a checkpoint, but I want to use a different learning rate, How to achieve that? I don't  really care about the training states and don't mind start a fresh training as long as the weights are proprely restored.\\nRight now I'm using resume_from_checkpoint=ckpt_file when creating the trainer, this automatically would give the old learning rate.\\nI also tried remove resume_from_checkpoint=ckpt_file, and do\\nnet_learner.load_from_checkpoint(cfg.ckpt_path, cfg=cfg)\\ntrainer.fit(net_learner, train_data_loader, val_data_loader)\\n\\nbut it seems the weights are erased, and the trainer starts from random weights.\\nAny help will be most appreciated, thanks so much!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjYwMjk0con': 'Hello,\\nI\\'m trying to train my model with multi-nodes (2 nodes, 8 gpus per each, using ddp accelator & trying without using slurm)\\nBut I got problem with GLOBAL_RANK\\nin node 1,\\ninitializing ddp: GLOBAL_RANK: 1, MEMBER: 2/16\\n...\\ninitializing ddp: GLOBAL_RANK: 7, MEMBER: 8/16\\n\\nsame as in node 2,\\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/16\\n...\\ninitializing ddp: GLOBAL_RANK: 7, MEMBER: 8/16\\n\\nAnd got stuck with repeated message like below\\nWaiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:1 (world_size=16, worker_count=13, timeout=0:30:00)\\nWaiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=16, worker_count=13, timeout=0:30:00)\\n\\nI\\'m trying to setup like this document but also got problem, like below\\n os.environ[\"MASTER_ADDR\"] = master_addr\\n os.environ[\"MASTER_PORT\"] = master_port\\n os.environ[\"WORLD_SIZE\"] = \"16\"\\n os.environ[\"NODE_RANK\"] = rank\\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 474, in fit\\n    self.accelerator.setup(self, model)\\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu.py\", line 19, in setup\\n    return super().setup(trainer, model)\\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 69, in setup\\n    self.connect_training_type_plugin(self.training_type_plugin, model)\\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 328, in connect_training_type_plugin\\n    plugin.connect(model)\\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/parallel.py\", line 68, in connect\\n    self.setup(model)\\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 95, in setup\\n    self.task_idx = self.cluster_environment.local_rank()\\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/plugins/environments/torchelastic_environment.py\", line 48, in local_rank\\n    return int(os.environ[\\'LOCAL_RANK\\'])\\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/os.py\", line 681, in __getitem__\\n    raise KeyError(key) from None\\n\\nI\\'d appreciate any help. thanks in advance',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjYwODA5con': \"Hello,\\nI was wondering if it is possible to control the trainloop behavior of a module (beyond overriding training_step()).  I want to manually override the .grad value of each parameter by myself.\\nFor example, let's say I have this routine:\\nm_0 = MyModel()\\nloader_1 = getTrainLoader(1)\\nloader_2 = getTrainLoader(2)\\nloader_3 = getTrainLoader(3)\\n\\n# train the first two models\\nm_1 = train_model_for_one_epoch(m_0, loader_1)\\nm_2 = train_model_for_one_epoch(m_1, loader_2)\\n\\n# train the third model based on the previous models\\nm_3 = MyModel()\\ncriteriton = nn.CrossEntropyLoss()\\noptimizer = optim.SGD(m_3.parameters(), lr)\\n\\n# main trainloop\\nfor data, target in loader_3:\\n    loss_1 = criteriton(m_1(data), target)\\n    loss_1.backward()\\n    grad_1 = get_gradient_vector(m_1)\\n    loss_2 = criterion(m_2(data), target)\\n    loss_2.backward()\\n    grad_2 = get_gradient_vector(m_2)\\n    \\n    # manually calculate & set gradient\\n    grad_3 = (grad_1 + grad_2) / 2.0\\n    set_model_gradient(m_3, grad_3)\\n    optimizer.step()\\n    \\nHow can I implement the final loop in the above code in PL?\\nThanks!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjYyMjcycon': 'Hello--\\nI am saving checkpoints inside my module using self.trainer.save_checkpoint(path). I am able to load these checkpoints into the model using MyModel.load_from_checkpoint(path) and trainer using Trainer(resume_from_checkpoint=path). However, both the resulting model and trainer have global_step=0 regardless of the step when saving. From the documentation I was of the impression that checkpoints saved the global step, which is important for my use case. How can I attain the global step from a checkpoint?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjc3NDg5con': \"Hi!\\nI currently use AWS SageMaker to train my PL models. I recently found out this link :\\nhttps://aws.amazon.com/fr/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/\\nSageMaker provides its own implementation of DDP and I think that would be nice to be able to use it with PL :)\\nI looked into PL code and I think I could add  this feature by extending pytorch_lightning.accelerators.accelerator.Accelerator. Does it seems like a good way to implement it? Are there some general advices/guidance you could give me about this?\\nIf the you are interested in this feature, I can make a PR once I'm done with it.\\nThank you!\\nRémi\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjcwMTE3con': 'Hi, I was wondering what is the proper way of logging metrics when using DDP. I noticed that if I want to print something inside validation_epoch_end it will be printed twice when using 2 GPUs. I was expecting validation_epoch_end to be called only on rank 0 and to receive the outputs from all GPUs, but I am not sure this is correct anymore. Therefore I have several questions:\\n\\nvalidation_epoch_end(self, outputs) - When using DDP does every subprocess receive the data processed from the current GPU or data processed from all GPUs, i.e. does the input parameter outputs contains the outputs of the entire validation set, from all GPUs?\\nIf outputs is GPU/process specific what is the proper way to calculate any metric on the entire validation set in validation_epoch_end when using DDP?\\n\\nI understand that I can solve the printing by checking self.global_rank == 0 and printing/logging only in that case, however I am trying to get a deeper understanding of what I am printing/logging in this case.\\nHere is a code snippet from my use case. I would like to be able to report f1, precision and recall on the entire validation dataset and I am wondering what is the correct way of doing it when using DDP.\\n    def _process_epoch_outputs(self,\\n                               outputs: List[Dict[str, Any]]\\n                               ) -> Tuple[torch.Tensor, torch.Tensor]:\\n        \"\"\"Creates and returns tensors containing all labels and predictions\\n\\n        Goes over the outputs accumulated from every batch, detaches the\\n        necessary tensors and stacks them together.\\n\\n        Args:\\n            outputs (List[Dict])\\n        \"\"\"\\n        all_labels = []\\n        all_predictions = []\\n\\n        for output in outputs:\\n            for labels in output[\\'labels\\'].detach():\\n                all_labels.append(labels)\\n\\n            for predictions in output[\\'predictions\\'].detach():\\n                all_predictions.append(predictions)\\n\\n        all_labels = torch.stack(all_labels).long().cpu()\\n        all_predictions = torch.stack(all_predictions).cpu()\\n\\n        return all_predictions, all_labels\\n\\n    def validation_epoch_end(self, outputs: List[Dict[str, Any]]) -> None:\\n        \"\"\"Logs f1, precision and recall on the validation set.\"\"\"\\n\\n        if self.global_rank == 0:\\n            print(f\\'Validation Epoch: {self.current_epoch}\\')\\n\\n        predictions, labels = self._process_epoch_outputs(outputs)\\n        for i, name in enumerate(self.label_columns):\\n\\n            f1, prec, recall, t = metrics.get_f1_prec_recall(predictions[:, i],\\n                                                             labels[:, i],\\n                                                             threshold=None)\\n            self.logger.experiment.add_scalar(f\\'{name}_f1/Val\\',\\n                                              f1,\\n                                              self.current_epoch)\\n            self.logger.experiment.add_scalar(f\\'{name}_Precision/Val\\',\\n                                              prec,\\n                                              self.current_epoch)\\n            self.logger.experiment.add_scalar(f\\'{name}_Recall/Val\\',\\n                                              recall,\\n                                              self.current_epoch)\\n\\n            if self.global_rank == 0:\\n                print((f\\'F1: {f1}, Precision: {prec}, \\'\\n                       f\\'Recall: {recall}, Threshold {t}\\'))',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjcwMTQ5con': \"In my usage, LightningDatamodule is currently encapsulating batch collation, moving to device, and batch transformations (via on_after_batch_transfer).\\nHowever, when I want to do inference on a bunch of inputs, I want the same steps to happen. What is the recommended way to achieve this? The problem is that Trainer drives the device transfers and hooks around it, and I don't have a Trainer during inference.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjczNjgzcon': \"I want to train my discriminator ones per 10 iterations but couldn't figure out how to implement it with lightning. Do you have any advice on this?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjg1ODg0con': 'How can I access the best validation loss in validation_epoch_end?  I am assuming the loss is stored somewhere as it is used to save the best model based on it, so I was wondering if I can somehow directly access it instead of tracking it my self.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjg2MjM2con': 'I have a PyTorch Lightning DataModule instance that defines train_dataloader, val_dataloader, and test_dataloader.\\nCurrently using a custom callback to reload the train_dataloader that will resample the data.\\nI saw that there is a Trainer flag called reload_dataloaders_every_epoch and soon to be reload_dataloaders_every_n_epochs.\\nDo these just reload the train_dataloader, or do the do all 3?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjg5OTA3con': 'Hi all. How can one make ModelCheckpoint work with multiple val dataloaders? Currently, it receives a single monitor parameter. I would like to make the checkpointing happen only when both dataloaders show improvement. Should I subclass ModelCheckpoint to receive a monitor per dataloader?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjgxNTg5con': 'Even with checkpoint_callback=False, Trainer appears to be using CheckpointConnector for some reason. Since very occasionally (once every ~30 runs) the checkpoint is either deleted too early or is never created in the first place (no idea which one), the whole experiment fails, as shown in the log below. Since CheckpointConnector does not appear to be doing anything important when running locally, is it possible to eliminate it without breaking the training process?\\nTraceback (most recent call last):\\n  File \".\\\\my_code\\\\run_automated.py\", line 95, in <module>\\n    main(experiment, config, dataset)\\n  File \"D:\\\\GIT\\\\my_code\\\\processing.py\", line 117, in main\\n    trainer.fit(model, dataloader_train, dataloader_val)\\n  File \"c:\\\\users\\\\pluczak\\\\.conda\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 513, in fit\\n    self.dispatch()\\n  File \"c:\\\\users\\\\pluczak\\\\.conda\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 553, in dispatch\\n    self.accelerator.start_training(self)\\n  File \"c:\\\\users\\\\pluczak\\\\.conda\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\accelerators\\\\accelerator.py\", line 74, in start_training\\n    self.training_type_plugin.start_training(trainer)\\n  File \"c:\\\\users\\\\pluczak\\\\.conda\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\plugins\\\\training_type\\\\training_type_plugin.py\", line 111, in start_training\\n    self._results = trainer.run_train()\\n  File \"c:\\\\users\\\\pluczak\\\\.conda\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 609, in run_train\\n    self._pre_training_routine()\\n  File \"c:\\\\users\\\\pluczak\\\\.conda\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 600, in _pre_training_routine\\n    self.checkpoint_connector.restore_weights()\\n  File \"c:\\\\users\\\\pluczak\\\\.conda\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\connectors\\\\checkpoint_connector.py\", line 65, in restore_weights\\n    max_suffix = self.max_ckpt_in_folder(dir_path_hpc, \"hpc_ckpt_\")\\n  File \"c:\\\\users\\\\pluczak\\\\.conda\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\connectors\\\\checkpoint_connector.py\", line 372, in max_ckpt_in_folder\\n    files = [os.path.basename(f[\"name\"]) for f in fs.listdir(dir_path)]\\n  File \"c:\\\\users\\\\pluczak\\\\.conda\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\fsspec\\\\spec.py\", line 1122, in listdir\\n    return self.ls(path, detail=detail, **kwargs)\\n  File \"c:\\\\users\\\\pluczak\\\\.conda\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\fsspec\\\\implementations\\\\local.py\", line 51, in ls\\n    return [self.info(f) for f in paths]\\n  File \"c:\\\\users\\\\pluczak\\\\.conda\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\fsspec\\\\implementations\\\\local.py\", line 51, in <listcomp>\\n    return [self.info(f) for f in paths]\\n  File \"c:\\\\users\\\\pluczak\\\\.conda\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\fsspec\\\\implementations\\\\local.py\", line 61, in info\\n    out = os.stat(path, follow_symlinks=False)\\nFileNotFoundError: [WinError 2] Nie można odnaleźć określonego pliku: \\'D:/GIT/my_code/04d2a63662b34828946b5545646c063f.pt\\'',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk1NTY1con': 'Is it possible to use automatic optimization with accumulate_grad_batches and performance trick zero_grad(set_to_none=True)?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk2NjUzcon': \"I want build a Super-Resolution Network with multiple optimizer.\\nThe code is below,\\n    def configure_optimizers(self):\\n        d_optimizer = torch.optim.Adam([{'params': self.parameters()}], lr=self.lr, betas=(0.5, 0.9))\\n        g_optimizer = torch.optim.Adam([{'params': self.parameters()}], lr=self.lr, betas=(0.5, 0.9))\\n        id_optimizer = torch.optim.Adam([{'params': self.parameters()}], lr=self.lr, betas=(0.5, 0.9))\\n        recon_optimizer = torch.optim.Adam([{'params': self.parameters()}], lr=self.lr, betas=(0.5, 0.9))\\n        # use multi optimizer\\n        return [g_optimizer, d_optimizer, id_optimizer, recon_optimizer]\\ndef training_step(self, batch, batch_idx, optimizer_idx):\\n        print('optimizer_idx', optimizer_idx)\\n        # print('criterionG', next(self.criterionG.parameters()).requires_grad)\\n        # print('generator', next(self.generator.parameters()).requires_grad)\\n\\n        lr_img, id_label, hr_img = batch\\n\\n        fake_img = self(lr_img)\\n        d_fake = self.discriminator(fake_img)\\n        d_real = self.discriminator(hr_img)\\n\\n        # train generator\\n        if optimizer_idx == 0:\\n            # log sampled images\\n            grid = torchvision.utils.make_grid(fake_img)\\n            self.logger.experiment.add_image('generated_images', grid, 0)\\n            g_loss = self.g_loss_function(d_fake, fake_img, hr_img)\\n            g_loss.requires_grad_(True)\\n\\n            return {'loss': g_loss}\\n\\n        # train discriminator\\n        elif optimizer_idx == 1:\\n            d_fake_loss = torch.mean(d_fake)\\n            d_real_loss = torch.mean(d_real)\\n            d_loss = (d_fake_loss + d_real_loss)/2\\n            tqdm_dict ={'d_loss': d_loss}\\n            self.log('d_loss', d_loss)\\n\\n            return {'d_loss': d_loss}\\n\\n        # fine-tuning arcface model\\n        elif optimizer_idx == 2:\\n            fake_img = self.conv1(fake_img)\\n            pred = self.recognition(fake_img)\\n            loss = self.loss_function(pred, id_label)\\n            self.log('id_loss', loss)\\n            tqdm_dict = {'id_loss': loss}\\n            output = OrderedDict({\\n                'id_loss': loss,\\n                'progress_bar': tqdm_dict,\\n                'log': tqdm_dict\\n            })\\n            return output\\n\\n        # training reconstruction model\\n        elif optimizer_idx == 3:\\n            fake_lr = self.reconstruction(fake_img)\\n            loss = self.recon_loss_function(hr_img, fake_lr)\\n            self.log('recon_loss', loss)\\n            tqdm_dict = {'recon_loss': loss}\\n            output = OrderedDict({\\n                'recon_loss': loss,\\n                'pregress_bar': tqdm_dict,\\n                'log': tqdm_dict\\n            })\\n            return output\\nBut, i got this error in 'if optimizer_idx == 0:'\\n    closure_loss = closure_loss / self.trainer.accumulate_grad_batches\\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'int'\\n\\nCan you give me a advice?\\nThank you.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk2ODM0con': 'I need to use srun run python, so how does set Trainer of pl` correctly?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk3MTA2con': 'How to manually call model.eval() or model.train() inside the lightning module? I happen to have several models and not all of them need to be updated during each forward pass. Thanks!',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk3NTY3con': 'Hi,\\nI was wondering how could i access trainer parameters from a LightningModule.\\nI want indeed to do setup that depend on max_epochs. Configure a linear decay lr scheduler and also i want to take some special action on the validation step on the last epoch.\\nThanks for the help!',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk3NjM1con': \"I would like to create an embedding that does not fit in the GPU memory\\nbut can fit in the CPU memory.\\nSelect the subset for a batch, send it to the GPU at the start of mini-batch.\\nGPU_tensor = embedding(idx)\\nThen at the end of training update the CPU embedding from the GPU embedding.\\nI am using\\npl.Trainer( gpus=[0,1], distributed_backend='ddp')\\nand probably will need accumulate_grad_batches\\nAny idea for how to do this ?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk5MDkzcon': 'I implemented the validation loop like this (using validation_epoch_end because the number of labels in every batch is variable):\\nclass MyModel(pl.LightningModule):\\n\\n    def validation_step(self, batch, batch_idx):\\n        return {\\'target\\': ... ,\\'pred\\': ...}\\n\\n    def validation_epoch_end(self, outputs) -> None:\\n        targets = torch.cat([o[\\'target\\'] for o in outputs])\\n        preds = torch.cat([o[\\'pred\\'] for o in outputs])\\n        accuracy = (preds == targets).sum().item() / len(preds)\\n        self.log(\"val_acc\", accuracy * 100)\\n\\nI verified that accuracy is computed correctly (as a float). However, I can\\'t see it in the console. The console output looks very odd, because the progress bar is repeated during validation (it looks okay during training), and the \"Validating\" prefix changed back to \"Epoch 0\" (the number of epochs also changed from 74 to 79).\\nEpoch 0:   6%|▋         | 5/79 [00:04<01:03,  1.16it/s, loss=6.29]\\nValidating: 0it [00:00, ?it/s]\\nValidating:   0%|          | 0/74 [00:00<?, ?it/s]\\nEpoch 0:  10%|█         | 8/79 [00:04<00:39,  1.80it/s, loss=6.29]\\nEpoch 0:  14%|█▍        | 11/79 [00:04<00:28,  2.41it/s, loss=6.29]\\nEpoch 0:  18%|█▊        | 14/79 [00:04<00:21,  2.98it/s, loss=6.29]\\nEpoch 0:  22%|██▏       | 17/79 [00:04<00:17,  3.51it/s, loss=6.29]\\nEpoch 0:  25%|██▌       | 20/79 [00:04<00:14,  4.04it/s, loss=6.29]\\nEpoch 0:  29%|██▉       | 23/79 [00:05<00:12,  4.52it/s, loss=6.29]\\nEpoch 0:  33%|███▎      | 26/79 [00:05<00:10,  5.00it/s, loss=6.29]\\nEpoch 0:  37%|███▋      | 29/79 [00:05<00:09,  5.45it/s, loss=6.29]\\nEpoch 0:  41%|████      | 32/79 [00:05<00:07,  5.88it/s, loss=6.29]\\nEpoch 0:  44%|████▍     | 35/79 [00:05<00:07,  6.27it/s, loss=6.29]\\nEpoch 0:  48%|████▊     | 38/79 [00:05<00:06,  6.65it/s, loss=6.29]\\nEpoch 0:  52%|█████▏    | 41/79 [00:05<00:05,  6.99it/s, loss=6.29]\\nEpoch 0:  56%|█████▌    | 44/79 [00:05<00:04,  7.34it/s, loss=6.29]\\nEpoch 0:  59%|█████▉    | 47/79 [00:06<00:04,  7.67it/s, loss=6.29]\\nEpoch 0:  63%|██████▎   | 50/79 [00:06<00:03,  7.99it/s, loss=6.29]\\nEpoch 0:  67%|██████▋   | 53/79 [00:06<00:03,  8.29it/s, loss=6.29]\\nEpoch 0:  71%|███████   | 56/79 [00:06<00:02,  8.58it/s, loss=6.29]\\nEpoch 0:  75%|███████▍  | 59/79 [00:06<00:02,  8.88it/s, loss=6.29]\\nEpoch 0:  78%|███████▊  | 62/79 [00:06<00:01,  9.15it/s, loss=6.29]\\nEpoch 0:  82%|████████▏ | 65/79 [00:06<00:01,  9.40it/s, loss=6.29]\\nEpoch 0:  86%|████████▌ | 68/79 [00:07<00:01,  9.63it/s, loss=6.29]\\nEpoch 0:  90%|████████▉ | 71/79 [00:07<00:00,  9.85it/s, loss=6.29]\\nEpoch 0:  94%|█████████▎| 74/79 [00:07<00:00, 10.10it/s, loss=6.29]\\nEpoch 0:  97%|█████████▋| 77/79 [00:07<00:00, 10.31it/s, loss=6.29]\\nEpoch 0: 100%|██████████| 79/79 [00:07<00:00, 10.41it/s, loss=6.29]\\nEpoch 1:   8%|▊         | 6/79 [00:03<00:46,  1.56it/s, loss=5.08]\\nValidating: 0it [00:00, ?it/s]\\nValidating:   0%|          | 0/74 [00:00<?, ?it/s]\\nEpoch 1:  11%|█▏        | 9/79 [00:04<00:31,  2.25it/s, loss=5.08]\\nEpoch 1:  15%|█▌        | 12/79 [00:04<00:23,  2.90it/s, loss=5.08]\\n\\nWhat is the right way to implement validation accuracy? I\\'m new to lightning, but I looked at the documentation and couldn\\'t find the answer. Thanks for your help.\\nA few related posts:\\n\\n#5774 (proposed solution is too complex)\\n#1906 (out of date, as validation_epoch_end shouldn\\'t return anything now)\\nhttps://forums.pytorchlightning.ai/t/computing-validation-accuracy-at-the-end-of-each-epoch/188/2 (out of date, as EvalResult has been removed)\\n\\nUpdate:\\nI added self.log(\"val_loss\", ...) to validation_step, but even this does not log anything in the progress bar (or anywhere in the console). It seems that self.log is not working at all.\\nUpdate:\\nI finally made it work by explicitly specifying all these parameters: self.log(\"val_acc\", raw_accuracy * 100, prog_bar=True, on_step=False, on_epoch=True). (However, it still doesn\\'t work for val_loss, which has on_step=True, on_epoch=False.)  The repeated progress bar problem is not fixed.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMjkxODc0con': \"I'm creating a classifier that first trains a VAE then passes it into a convolutional network. The psudo code below kind of describes it:\\nclass VAE(pl.LightningModule):\\n# ...\\n\\nclass ConvNetwork(pl.LightningModule):\\n    def __init__(self, vae):\\n        # Trying both ways: pass in entire model vs loading checkpoint\\n        # self.vae = vae\\n        # self.vae = VAE.load_from_checkpoint(vae)\\n        freeze_training(self.vae) # sets all params to requries_grad=False\\n\\n        self.sub_network = nn.Sequential(\\n            # Mix of convolutional layers, ReLU activations, and Batch Normalization\\n        )\\n\\n    def forward(self, data):\\n         vae_decoded_results = self.vae(data)\\n         results_that_differ_wildly = self.sub_network(vae_decoded_results)\\n        \\nIf I train the VAE and pass in the entire model before training the convolutional network, I get good training/validation results. What I would prefer, however, is to train the VAE in a separate script, save off checkpoints, then pass the path of the checkpoint into the convolutional network. Then in the convolutional network's init I load the vae network, freeze training on it, and proceed to train the convolutional network. When I do this, my training results seem okay, but my validation results are all over the place. Some things I've checked:\\n\\nAfter loading the VAE from a checkpoint, I verified the model parameters perfectly match the VAE that produced the checkpoint.\\nIn my forward function for the convolutional network I call the VAE's forward function. The results at this step differ by less than 1% between loading a checkpoint and passing in an entire model.\\nAfter passing the VAE forward() results into the first stage of my Convolution network (consists of some convolution layers, ReLU activations, and batch normalization) I get very different results.\\n\\nI can't for the life of me figure out why using the results from a loaded model would so wildly differ from the results of a model I train and pass in all in one script, especially when the parameters and vae output appear to match. I'm sure I'm just missing something stupid.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjkyOTQzcon': \"I would like my wandb logger to just place their data under wandb dir, and checkpointcallback to save ckpts under dir_path I specified.\\nAnd I don't want pl to create lightning_logs and files under it, but I can't set logger=False b/c I use a logger. Is there any suggestion ?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMjkzNzMxcon': \"Greetings,\\nI can only show metrics of variables calculated on training step but can't show validation step metrics on the progress bar. How can show a metric in the validation step ? self.log(...., prog_bar=True) does not work.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzA0NjU2con': 'Hi all,\\nI am trying to train my LightningModule but I seem to keep getting the error TypeError: unsupported operand type(s) for /: \\'NoneType\\' and \\'int\\' on the line closure_loss = closure_loss / self.accumulate_grad_batches, in the function training_step() in the file training_loop.py.\\nI think it might be something to do with how I format my LightningModule, so here is what my LightningModule looks like\\nclass HPAModelV1(pl.LightningModule):\\n  def __init__(self):\\n    super().__init__()\\n\\n    #self.lossfunc = F.cross_entropy\\n    self.lossfunc = F.nll_loss\\n\\n    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=3, padding=7)\\n    self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\\n    self.conv3 = nn.Conv2d(16, 16, kernel_size=5, stride=1, padding=1)\\n    self.dense = nn.Linear(16, 19)\\n\\n  def forward(self, x): #input size is (256, 3, 256, 256)\\n\\n    x = x.float()\\n    \\n    out = self.conv1(x)\\n    out = F.relu(out)\\n    out = F.max_pool2d(out, 3) # output is (bs, 16, 30, 30)\\n    \\n    out = self.conv2(out)\\n    out = F.relu(out)\\n    out = F.max_pool2d(out, 3) # output is (bs, 16, 10, 10)\\n\\n    out = self.conv3(out)\\n    out = F.relu(out)\\n    out = F.max_pool2d(out, 8) # output is (bs, 16, 1, 1)\\n\\n    # dense layer\\n    out = out.reshape(out.size()[0], 16)\\n    out = self.dense(out)\\n\\n    return out\\n\\n  def configure_optimizers(self):\\n    optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\\n    return optimizer \\n\\n\\n  def training_step(self, batch, batchidx):\\n    # set labels and data\\n    x = batch[0]\\n    y = batch[1]\\n    \\n\\n    # compute loop\\n    preds = self(x)\\n  \\n  \\n    probs = F.softmax(preds, dim=1)\\n \\n\\n    # compute the loss function\\n    J = self.lossfunc(probs, y)\\n \\n   \\n\\n    # compute accuracy \\n    acc = accuracy(probs, y)\\n\\n    \\n    #log for weights and biases\\n    self.log(\\'training loss (step)\\', J)\\n    self.log(\\'training accuracy (step)\\', acc)\\n    self.log(\\'mean training loss (epoch)\\', J, on_step=False, on_epoch=True)\\n    self.log(\\'mean training accuracy (epoch)\\', acc, on_step=False, on_epoch=True)\\n\\n\\n\\n    # add information to the progress bar\\n    pbar =  {\\'train_acc\\': acc, \\'train_loss\\' : J}\\n\\n    return J, acc\\n\\n  def validation_step(self, valbatch, valbatchidx):\\n    # use the same training step on the val set\\n\\n    valJ, valAcc = self.training_step(valbatch, valbatchidx)\\n\\n    # log for wb\\n    self.log(\\'validation loss (step)\\', valJ)\\n    self.log(\\'validation accuracy (step)\\', valAcc)\\n    self.log(\\'mean validation loss (epoch)\\', valJ, on_step=False, on_epoch=True)\\n    self.log(\\'mean validation accuracy (epoch)\\', valAcc, on_step=False, on_epoch=True)\\n\\n    return valJ, valAcc\\n\\n  def validation_epoch_end(self, valStepOutputs):\\n    pass\\nAnd if it may help in diagnosing the cause of the issue, here is the stack trace and output of of the Trainer:\\nGPU available: False, used: False\\nTPU available: True, using: 1 TPU cores\\nGlobal seed set to 0\\n\\n  | Name  | Type   | Params\\n---------------------------------\\n0 | conv1 | Conv2d | 448   \\n1 | conv2 | Conv2d | 2.3 K \\n2 | conv3 | Conv2d | 6.4 K \\n3 | dense | Linear | 323   \\n---------------------------------\\n9.5 K     Trainable params\\n0         Non-trainable params\\n9.5 K     Total params\\n0.038     Total estimated model params size (MB)\\nEpoch 0: 0%\\n0/7759 [00:02<?, ?it/s]\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-29-caf15077ca9b> in <module>()\\n      2 os.environ[\\'WANDB_CONSOLE\\'] = \\'on\\'\\n      3 trainer = Trainer(logger=wbLogger, tpu_cores=1, deterministic=True, max_epochs=epochNum, replace_sampler_ddp=False, num_sanity_val_steps=0)\\n----> 4 trainer.fit(HPAModelV1(), trainDL, valDL)\\n      5 \\n      6 print(time.time() - t0)\\n\\n23 frames\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\\n    497 \\n    498         # dispath `start_training` or `start_testing` or `start_predicting`\\n--> 499         self.dispatch()\\n    500 \\n    501         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\\n    544 \\n    545         else:\\n--> 546             self.accelerator.start_training(self)\\n    547 \\n    548     def train_or_test_or_predict(self):\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\\n     71 \\n     72     def start_training(self, trainer):\\n---> 73         self.training_type_plugin.start_training(trainer)\\n     74 \\n     75     def start_testing(self, trainer):\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/tpu_spawn.py in start_training(self, trainer)\\n    264             del os.environ[\"XLA_USE_BF16\"]\\n    265         self._close_logger(trainer)\\n--> 266         xmp.spawn(self.new_process, **self.xmp_spawn_kwargs)\\n    267 \\n    268     def start_testing(self, trainer) -> None:\\n\\n/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py in spawn(fn, args, nprocs, join, daemon, start_method)\\n    384   pf_cfg = _pre_fork_setup(nprocs)\\n    385   if pf_cfg.num_devices == 1:\\n--> 386     _start_fn(0, pf_cfg, fn, args)\\n    387   else:\\n    388     return torch.multiprocessing.start_processes(\\n\\n/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py in _start_fn(index, pf_cfg, fn, args)\\n    321   # environment must be fully setup before doing so.\\n    322   _setup_replication()\\n--> 323   fn(gindex, *args)\\n    324 \\n    325 \\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/tpu_spawn.py in new_process(self, process_idx, trainer, mp_queue)\\n     98         self.barrier(\"pre-run-stage\")\\n     99 \\n--> 100         results = trainer.train_or_test_or_predict()\\n    101 \\n    102         self.__save_end_of_training_weights(self.lightning_module)\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in train_or_test_or_predict(self)\\n    554 \\n    555         else:\\n--> 556             results = self.run_train()\\n    557 \\n    558         return results\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\\n    635                 with self.profiler.profile(\"run_training_epoch\"):\\n    636                     # run train epoch\\n--> 637                     self.train_loop.run_training_epoch()\\n    638 \\n    639                 if self.max_steps and self.max_steps <= self.global_step:\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\\n    495             # ------------------------------------\\n    496             with self.trainer.profiler.profile(\"run_training_batch\"):\\n--> 497                 batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\\n    498 \\n    499             # when returning -1 from train_step, we end epoch early\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx, dataloader_idx)\\n    657 \\n    658                         # optimizer step\\n--> 659                         self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\\n    660 \\n    661                     else:\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in optimizer_step(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\\n    436             on_tpu=self.trainer._device_type == DeviceType.TPU and _TPU_AVAILABLE,\\n    437             using_native_amp=using_native_amp,\\n--> 438             using_lbfgs=is_lbfgs,\\n    439         )\\n    440 \\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py in optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\\n   1388             # wraps into LightingOptimizer only for running step\\n   1389             optimizer = LightningOptimizer._to_lightning_optimizer(optimizer, self.trainer, optimizer_idx)\\n-> 1390         optimizer.step(closure=optimizer_closure)\\n   1391 \\n   1392     def optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: Optimizer, optimizer_idx: int):\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/optimizer.py in step(self, closure, *args, **kwargs)\\n    212             profiler_name = f\"optimizer_step_and_closure_{self._optimizer_idx}\"\\n    213 \\n--> 214         self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\\n    215         self._total_optimizer_step_calls += 1\\n    216 \\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/optimizer.py in __optimizer_step(self, closure, profiler_name, **kwargs)\\n    132 \\n    133         with trainer.profiler.profile(profiler_name):\\n--> 134             trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\\n    135 \\n    136     def step(self, *args, closure: Optional[Callable] = None, **kwargs):\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py in optimizer_step(self, optimizer, opt_idx, lambda_closure, **kwargs)\\n    275         )\\n    276         if make_optimizer_step:\\n--> 277             self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\\n    278         self.precision_plugin.post_optimizer_step(optimizer, opt_idx)\\n    279         self.training_type_plugin.post_optimizer_step(optimizer, opt_idx, **kwargs)\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/tpu.py in run_optimizer_step(self, optimizer, optimizer_idx, lambda_closure, **kwargs)\\n     32 \\n     33     def run_optimizer_step(self, optimizer: Optimizer, optimizer_idx: int, lambda_closure: Callable, **kwargs):\\n---> 34         xm.optimizer_step(optimizer, barrier=False, optimizer_args={\\'closure\\': lambda_closure, **kwargs})\\n     35 \\n     36     def all_gather(self, tensor: Union[torch.Tensor], group: Optional[Any] = None, sync_grads: bool = False):\\n\\n/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py in optimizer_step(optimizer, barrier, optimizer_args, groups)\\n    779   \"\"\"\\n    780   reduce_gradients(optimizer, groups=groups)\\n--> 781   loss = optimizer.step(**optimizer_args)\\n    782   if barrier:\\n    783     mark_step()\\n\\n/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py in wrapper(*args, **kwargs)\\n     86                 profile_name = \"Optimizer.step#{}.step\".format(obj.__class__.__name__)\\n     87                 with torch.autograd.profiler.record_function(profile_name):\\n---> 88                     return func(*args, **kwargs)\\n     89             return wrapper\\n     90 \\n\\n/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)\\n     25         def decorate_context(*args, **kwargs):\\n     26             with self.__class__():\\n---> 27                 return func(*args, **kwargs)\\n     28         return cast(F, decorate_context)\\n     29 \\n\\n/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py in step(self, closure)\\n     64         if closure is not None:\\n     65             with torch.enable_grad():\\n---> 66                 loss = closure()\\n     67 \\n     68         for group in self.param_groups:\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in train_step_and_backward_closure()\\n    652                         def train_step_and_backward_closure():\\n    653                             result = self.training_step_and_backward(\\n--> 654                                 split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\\n    655                             )\\n    656                             return None if result is None else result.loss\\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\\n    745         with self.trainer.profiler.profile(\"training_step_and_backward\"):\\n    746             # lightning module hook\\n--> 747             result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\\n    748             self._curr_step_result = result\\n    749 \\n\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in training_step(self, split_batch, batch_idx, opt_idx, hiddens)\\n    325 \\n    326 \\n--> 327             closure_loss = closure_loss / self.trainer.accumulate_grad_batches\\n    328 \\n    329             # the loss will get scaled for amp. avoid any modifications to it\\n\\nTypeError: unsupported operand type(s) for /: \\'NoneType\\' and \\'int\\'\\n\\nThank you, and sorry for all the text\\nA',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzA1ODA1con': 'I have read the GAN Demo, it is for two losses.\\nSuppose I have two modules A and B. The training_step is:\\ndef training_step(self, batch, batch_idx, optimizer_idx):\\n        x, y = batch\\n        a1, a2 = self.A(x[0])\\n        preds = self.B(x[1], a1, a2)\\n        loss = loss_fn(preds, y)\\n        return loss\\n\\nHow to train A and B with different optimizer or learning rate?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzA1OTk3con': 'If the TensorBoard logger is set up as shown\\nlogger = TensorBoardLogger(name=\"MyModel\")\\ncheckpoint_callback = ModelCheckpoint(\\n    filename=\"{epoch}-{step}-{val_loss:.2f}\",\\n    monitor=\"val_loss\",\\n    save_top_k=5,\\n)\\ntrainer = pl.Trainer(\\n    default_root_dir=ROOT_DIR,\\n    callbacks=[checkpoint_callback],\\n    logger=[logger],\\n)\\nhow do we configure the checkpoints to be written to directories that are automatically named version_0, version_1, the way it is if you do not pass a logger to Trainer?\\ntrainer = pl.Trainer(\\n    default_root_dir=ROOT_DIR,\\n    callbacks=[checkpoint_callback],\\n)\\nIf we pass in a logger to Trainer, the checkpoints are written to\\n<root_path>/<experiment_name>/<integer>/checkpoints\\n\\nwhile the tensorboard logs and hparams.yaml are written to\\n<root_path>/<experiment_name>/version_<integer>/\\n\\nIf we do not pass in a logger to Trainer, then checkpoint files, Tensorboard files and hparams.yaml are all written to the same directory\\n<root_path>/<experiment_name>/version_<integer>/\\n\\nHow can both checkpoints and tensorboard files we written to the same version_<integer> directory?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzA4MTQycon': 'Hi,\\nI try to implement an autoencoder where the latent space is not euclidean but has hyperbolic and spherical geometry. I use geoopt for this. They recommend to use double precision for numerical stability. When I run my LightningModule with precision=32 I encounter NaNs in the training. When I change to precision=64 I get this pickling error:\\nTraceback (most recent call last):\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in run_train\\n    self.train_loop.run_training_epoch()\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 494, in run_training_epoch\\n    self.check_checkpoint_callback(True)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 149, in check_checkpoint_callback\\n    cb.on_validation_end(self.trainer, model)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 238, in on_validation_end\\n    self.save_checkpoint(trainer)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 281, in save_checkpoint\\n    self._save_none_monitor_checkpoint(trainer, monitor_candidates)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 660, in _save_none_monitor_checkpoint\\n    self._save_model(trainer, filepath)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 410, in _save_model\\n    self._do_save(trainer, filepath)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 422, in _do_save\\n    self.save_function(filepath, self.save_weights_only)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py\", line 336, in save_checkpoint\\n    self.checkpoint_connector.save_checkpoint(filepath, weights_only)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 393, in save_checkpoint\\n    self.trainer.accelerator.save_checkpoint(_checkpoint, filepath)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 490, in save_checkpoint\\n    self.training_type_plugin.save_checkpoint(checkpoint, filepath)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 254, in save_checkpoint\\n    atomic_save(checkpoint, filepath)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py\", line 63, in atomic_save\\n    torch.save(checkpoint, bytesbuffer)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/torch/serialization.py\", line 372, in save\\n    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/torch/serialization.py\", line 476, in _save\\n    pickler.dump(obj)\\n_pickle.PicklingError: Can\\'t pickle <function LinkPredictionModule.training_step at 0x7f08220f0ee0>: it\\'s not the same object as stereographic_link_prediction.Models.Modules.LinkPredictionModule.training_step\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"main.py\", line 40, in <module>\\n    trainer.fit(module, datamodule=datamodule)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 480, in fit\\n    self.dispatch()\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 523, in dispatch\\n    self.accelerator.start_training(self)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 95, in start_training\\n    self.training_type_plugin.start_training(trainer)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 142, in start_training\\n    self._results = trainer.run_stage()\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 535, in run_stage\\n    self.run_train()\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 634, in run_train\\n    self.train_loop.on_train_end()\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 117, in on_train_end\\n    self.check_checkpoint_callback(should_update=True, is_last=True)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 149, in check_checkpoint_callback\\n    cb.on_validation_end(self.trainer, model)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 238, in on_validation_end\\n    self.save_checkpoint(trainer)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 281, in save_checkpoint\\n    self._save_none_monitor_checkpoint(trainer, monitor_candidates)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 660, in _save_none_monitor_checkpoint\\n    self._save_model(trainer, filepath)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 410, in _save_model\\n    self._do_save(trainer, filepath)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 422, in _do_save\\n    self.save_function(filepath, self.save_weights_only)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py\", line 336, in save_checkpoint\\n    self.checkpoint_connector.save_checkpoint(filepath, weights_only)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 393, in save_checkpoint\\n    self.trainer.accelerator.save_checkpoint(_checkpoint, filepath)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 490, in save_checkpoint\\n    self.training_type_plugin.save_checkpoint(checkpoint, filepath)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 254, in save_checkpoint\\n    atomic_save(checkpoint, filepath)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py\", line 63, in atomic_save\\n    torch.save(checkpoint, bytesbuffer)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/torch/serialization.py\", line 372, in save\\n    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/torch/serialization.py\", line 476, in _save\\n    pickler.dump(obj)\\n_pickle.PicklingError: Can\\'t pickle <function LinkPredictionModule.training_step at 0x7f08220f0ee0>: it\\'s not the same object as stereographic_link_prediction.Models.Modules.LinkPredictionModule.training_step\\nException ignored in: <function tqdm.__del__ at 0x7f086346a430>\\nTraceback (most recent call last):\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/tqdm/std.py\", line 1145, in __del__\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/tqdm/std.py\", line 1299, in close\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/tqdm/std.py\", line 1492, in display\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/tqdm/std.py\", line 1148, in __str__\\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/tqdm/std.py\", line 1450, in format_dict\\nTypeError: cannot unpack non-iterable NoneType object\\n\\nI think this error comes from inconsistencies in my Module. But I do not know how to find them. Do you have any tips to track down the inconsistencies that disturb pickling?\\nHowever, when I run with precision=16 I do not run into NaNs. As this precision should be more numerically unstable than precision=32 my only explanation for this is that there is automatic gradient clipping going on or so in 16-bit precision. Is this correct? And where do I find these values? Then I could apply them to 32-bit precision training.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzAwMjAwcon': 'Hi, I started noticing the following warning message after setting up a new conda environment with Pytorch 1.8.1, which is an update from my previous environment that uses Pytorch 1.7.0.\\n\\nEpoch 0:   0%|\\n[W reducer.cpp:1050] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters. This flag results in an extra traversal of the autograd graph every iteration, which can adversely affect performance. If your model indeed never has any unused parameters, consider turning this flag off. Note that this warning may be a false positive your model has flow control causing later iterations to have unused parameters. (function operator())\\n\\nAny idea if this is a real concern? How can we disable find_unused_parameters?\\ntrainer = pl.Trainer(\\n    val_check_interval=0.1,\\n    gpus=-1,\\n    accelerator=\"ddp\",\\n    callbacks=[checkpoint_callback, early_stop_callback],\\n    precision=16,\\n)\\nPackages:\\n\\npytorch 1.8.1\\npytorch-lightning 1.2.6\\ncudatoolkit 11.1.1\\ncudnn 8.0.5\\npython 3.8',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzAxMTc3con': \"I am using Latest PT-Lightning and your upgrade to ModelCheckpoint broke my functionality to my codebase.\\nV1.2.6 specifically coz I started facing these issues yesterday.  Downgraded to V1.2.5. Please check once. The dict based metrics returned from the train_step or validation_step are not captured in the ModelCheckpointCallback\\n monitor_candidates = self._monitor_candidates(trainer)\\nmonitor_candidates doesn't have any values returned from the trainer. And I am not using the Result class. I am returning a pure dictionary.\\nDidn't have time to create replicate the bug exactly in a notebook because I am on a tight deadline. But please do check.\\nAs I have more time, I will update more on the issue.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzAxNjgwcon': 'Thanks for great framework.\\nI tried to train with tpu (Google Cloud Platform Environment). I encounter error like this:\\nkaki_ai@kaki-ins:~/kopite-bot$ python3 train_blender.py\\n16:14:31 | Overriding opt[\"no_cuda\"] to True (previously: False)\\n16:14:31 | Loading model with `--beam-block-full-context false`\\n16:14:31 | loading dictionary from /home/kaki_ai/ParlAI/data/models/blender/blender_90M/model.dict\\n16:14:31 | num words = 54944\\n16:14:32 | DEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\\n16:14:33 | Total parameters: 87,508,992 (87,508,992 trainable)\\n16:14:33 | Loading existing model params from /home/kaki_ai/ParlAI/data/models/blender/blender_90M/model\\nTraceback (most recent call last):\\n  File \"train_blender.py\", line 47, in <module>\\n    val_dataloader=test_loader,\\n  File \"/home/kaki_ai/kopite-bot/training/lightning_base.py\", line 135, in fit\\n    accumulate_grad_batches=self.accumulate_grad_batches,\\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 39, in insert_env_defaults\\n    return fn(self, **kwargs)\\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 321, in __init__\\n    replace_sampler_ddp, deterministic, precision, amp_backend, amp_level, plugins\\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 91, in __init__\\n    self.tpu_cores = device_parser.parse_tpu_cores(tpu_cores)\\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/device_parser.py\", line 113, in parse_tpu_cores\\n    raise MisconfigurationException(\\'No TPU devices were found.\\')\\npytorch_lightning.utilities.exceptions.MisconfigurationException: No TPU devices were found.\\n\\nIf you have any doubts, please help me. Thank you!',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzAyNDAycon': \"I'm responsible for implementing pytorch_lightning.loggers.neptune.NeptuneLogger.\\nRight now we're going through massive api update for neptune client, which requires updating NeptuneLogger in your repository (PyTorchLightning/pytorch-lightning).\\nTheoretically we could implement our class NeptuneLogger(LightningLoggerBase) in our repo, and import this class as module in PyTorchLightning/pytorch-lightning.\\nIn such configuration we could easily update logger in our repo without having to create PR in PyTorchLightning/pytorch-lightning, when updating our api again in the future.\\nAre there any reasons against such solution?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzE0MDUxcon': \"My model has the property that I can prepare the test data in multiple different ways, which results in a set of equally plausible predictions for each data point (one prediction for each way of preparing the test data). By combining these predictions, it is possible to slightly boost overall performance on the test set. Right now, I do this in the following (abstract) way:\\n\\nfor iPrep in range(nPrep):\\n   preppedData=prepare_data(testData,iPrep)\\n   predictions[iPrep]=trainer.test(model,preppedData)\\n\\nfinal_predictions=combinePredictions(predictions)\\n\\n\\n(obviously it is much longer in reality)\\nis there a proper, 'lightning' way of hiding this loop inside the model, so I can still use the trainer for this, but only call it once?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzE1NTUzcon': 'Can a pl.LightningModule be used from native pytorch?\\nWe are writing a library, and the use of pl.LightningModule for our modules is convenient, particularly because each module knows its device.\\nHowever, our clients might be using native pytorch, and want to include our LightningModule as an nn.Module in their code.\\nFWIW, our LightningModule currently is used purely for forward inference and currently passes no gradients, nor is it trainable.\\n\\nAre there any interoperability pitfalls in having a LightningModule be an nn.Module in a pure pytorch codebase?\\nAre the benefits gained by using pl.LightningModules in our codebase no longer relevant when called from a pure pytorch codebase, particularly given that we pass back no gradients?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzEwODM0con': 'I am trying to run trainer.fit with only models passed. However, I am getting the following error.\\nTraceback (most recent call last):\\n  File \"train.py\", line 116, in <module>\\n    main_func(args)\\n  File \"train.py\", line 55, in main_func\\n    trainer.fit(model, train_dataloader=train_dataloader, val_dataloaders=val_dataloader)\\n  File \"/netscratch/gsingh/EVN/Anaconda/envs/lighting_neptune/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 443, in fit\\n    self.model_connector.copy_trainer_model_properties(model)\\n  File \"/netscratch/gsingh/EVN/Anaconda/envs/lighting_neptune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/model_connector.py\", line 39, in copy_trainer_model_properties\\n    m.precision = self.trainer.precision\\n  File \"/netscratch/gsingh/EVN/Anaconda/envs/lighting_neptune/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 982, in __setattr__\\n    raise TypeError(\"cannot assign \\'{}\\' as child module \\'{}\\' \"\\nTypeError: cannot assign \\'int\\' as child module \\'precision\\' (torch.nn.Module or None expected)\\n\\nI am unable to figure this out. Any help would be appreciated\\n    trainer = Trainer(\\n        logger=logger, resume_from_checkpoint=args.resume_from_checkpoint, max_epochs=args.num_epochs,\\n        accumulate_grad_batches=args.gradient_accumulation_steps, \\n        default_root_dir=args.experiment_path, checkpoint_callback=ckpt, num_sanity_val_steps=args.num_sanity_val_steps, \\n        gradient_clip_val=args.gradient_clip_val, gpus=1, auto_select_gpus=True, sync_batchnorm=args.sync_batchnorm\\n    )\\n                        \\n    criterion = torch.nn.CrossEntropyLoss()\\n    criterion_val = torch.nn.CrossEntropyLoss()\\n    \\n    model = Model(args, tr_dl=train_dataloader, val_dl=val_dataloader, test_dl=test_dataloader, criterion=criterion, criterion_val=criterion_val)\\n    trainer.fit(model)\\nHere precision value is the default.\\nThank You',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzExMzgwcon': 'Why\\nnew_model = MyModel.load_from_checkpoint(checkpoint_path=\"example.ckpt\")\\n\\ninstead of\\nMyModel.load_from_checkpoint(checkpoint_path=\"example.ckpt\")\\n\\nThe former one is not friendly to native PyTorch users. And cost me an afternoon to find the bug.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzExNDI0con': 'Please see same question on Stack Overflow.\\nWhen using Accuracy with a class that should be ignored, meaning it has labels but can never be predicted, the scoring is wrong, because it is calculated with the never predicted labels that should be ignored.\\nHow to use Accuracy while ignoring some class?\\nThanks :)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzEyMzgzcon': 'I am using Pytorch Lightning 1.2.6 to train my models using DDP and TensorBoard is the default logger used by Lightning.\\nMy code is setup to log the training and validation loss on each training and validation step respectively.\\nclass MyLightningModel(pl.LightningModule):\\n\\n    def training_step(self, batch):\\n        x, labels = batch\\n        out = self(x)\\n        loss = F.mse_loss(out, labels)\\n        self.log(\"train_loss\", loss)\\n        return loss\\n\\n    def validation_step(self, batch):\\n        x, labels = batch\\n        out = self(x)\\n        loss = F.mse_loss(out, labels)\\n        self.log(\"val_loss\", loss)\\n        return loss\\nTensorBoard correctly plots both the train_loss and val_loss charts in the SCALERS tab. However, in the HPARAMS tab, on the left side bar, only hp_metric is visible under Metrics.\\n\\nHowever, in the HPARAMS tab, on the left side bar, only hp_metric is visible under Metrics.\\n\\nHow can we add train_loss and val_loss to the Metrics section? This way, we will be able to use val_loss in the PARALLEL COORDINATES VIEW instead of hp_metric.\\nImage showing hp_metric and no val_loss:\\n\\nUsing Pytorch 1.8.1, Pytorch Lightning 1.2.6, TensorBoard 2.4.1',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzEzODI5con': 'Hello, I am trying to get pruning to work within my lightning model. I have tried multiple methods, but have not been able to get the ModelPruning module to work.\\nWhen I try this:\\ndef compute_amount(epoch):\\n    # the sum of all returned values need to be smaller than 1\\n    if epoch == 10:\\n        return 0.5\\n\\n    elif epoch == 50:\\n        return 0.25\\n\\n    elif 75 < epoch < 99 :\\n        return 0.01\\n    else:\\n      return 0\\n\\nprune = ModelPruning(\\n            pruning_fn=\\'l1_unstructured\\',\\n            parameters_to_prune=[(model.model.conv1, \\'weight\\'),\\n                            (model.model.conv2, \\'weight\\'),\\n                            (model.model.lin1, \\'weight\\'),\\n                            (model.model.lin2, \\'weight\\'),\\n                            (model.model.upconv1, \\'weight\\'),\\n                            (model.model.upconv2, \\'weight\\')],\\n            amount=compute_amount,\\n            use_global_unstructured=True,\\n        )\\n\\ntrainer = pl.Trainer(gpus=-1, max_epochs = 15, logger=wandb_logger, profiler=SimpleProfiler(\"/content/bla\"), progress_bar_refresh_rate=20, callbacks=[prune])\\ntrainer.fit(model, dm)\\n\\nI get this output:\\n\\nMisconfigurationException                 Traceback (most recent call last)\\n in ()\\n11\\n12 trainer = pl.Trainer(gpus=-1, max_epochs = 15, logger=wandb_logger, profiler=SimpleProfiler(\"/content/bla\"), progress_bar_refresh_rate=20, callbacks=[prune])\\n---> 13 trainer.fit(model, dm)\\n4 frames\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\\n457         # ----------------------------\\n458         self.call_setup_hook(model)\\n--> 459         self.call_hook(\"on_before_accelerator_backend_setup\", model)\\n460         self.accelerator.setup(self, model)  # note: this sets up self.lightning_module\\n461\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in call_hook(self, hook_name, *args, **kwargs)\\n1089             if hasattr(self, hook_name):\\n1090                 trainer_hook = getattr(self, hook_name)\\n-> 1091                 trainer_hook(*args, **kwargs)\\n1092\\n1093             # next call hook in lightningModule\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py in on_before_accelerator_backend_setup(self, model)\\n33         \"\"\"Called in the beginning of fit and test\"\"\"\\n34         for callback in self.callbacks:\\n---> 35             callback.on_before_accelerator_backend_setup(self, model)\\n36\\n37     def setup(self, model, stage: str):\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/pruning.py in on_before_accelerator_backend_setup(self, trainer, pl_module)\\n360     def on_before_accelerator_backend_setup(self, trainer, pl_module: LightningModule):\\n361         parameters_to_prune = self.sanitize_parameters_to_prune(\\n--> 362             pl_module, self._parameters_to_prune, parameter_names=self._parameter_names\\n363         )\\n364\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/pruning.py in sanitize_parameters_to_prune(pl_module, parameters_to_prune, parameter_names)\\n441             if missing_modules or missing_parameters:\\n442                 raise MisconfigurationException(\\n--> 443                     \"Some provided parameters_to_tune don\\'t exist in the model.\"\\n444                     f\" Found missing modules: {missing_modules} and missing parameters: {missing_parameters}\"\\n445                 )\\nMisconfigurationException: Some provided parameters_to_tune don\\'t exist in the model. Found missing modules: [Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), Linear(in_features=784, out_features=32, bias=True), Linear(in_features=32, out_features=784, bias=True), ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))] and missing parameters: []\\n\\nWhich makes it seem like ModelPruning is not finding my model\\'s weights, though I do not understand why. If I put into ModelPruning weights that do not exist in the model. It catches that problem during ModelPruning\\'s init. Which is not happening here. So that does not seem to be the problem.\\nI have also tried bypassing the parameters_to_prune parameter altogether.\\ndef compute_amount(epoch):\\n    # the sum of all returned values need to be smaller than 1\\n    if epoch == 10:\\n        return 0.5\\n\\n    elif epoch == 50:\\n        return 0.25\\n\\n    elif 75 < epoch < 99 :\\n        return 0.01\\n    else:\\n      return 0\\n\\nprune = ModelPruning(\\n            pruning_fn=\\'l1_unstructured\\',\\n            parameter_names=[\\'weight\\', \\'bias\\'],\\n            amount=compute_amount,\\n            use_global_unstructured=True,\\n        )\\n\\ntrainer = pl.Trainer(gpus=-1, max_epochs = 15, logger=wandb_logger, profiler=SimpleProfiler(\"/content/bla\"), progress_bar_refresh_rate=20, callbacks=[prune])\\ntrainer.fit(model, dm)\\n\\nThis works right until it does not. One epoch runs and then it snags on a callback within the validation step.\\n\\n\\nTypeError                                 Traceback (most recent call last)\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\\n636                     # run train epoch\\n--> 637                     self.train_loop.run_training_epoch()\\n638\\n56 frames\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\\n576         if should_check_val:\\n--> 577             self.trainer.run_evaluation(on_epoch=True)\\n578\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in run_evaluation(self, max_batches, on_epoch)\\n750         # hook\\n--> 751         self.evaluation_loop.on_evaluation_end()\\n752\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in on_evaluation_end(self, *args, **kwargs)\\n99         else:\\n--> 100             self.trainer.call_hook(\\'on_validation_end\\', *args, **kwargs)\\n101\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in call_hook(self, hook_name, *args, **kwargs)\\n1090                 trainer_hook = getattr(self, hook_name)\\n-> 1091                 trainer_hook(*args, **kwargs)\\n1092\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py in on_validation_end(self)\\n184         for callback in self.callbacks:\\n--> 185             callback.on_validation_end(self, self.lightning_module)\\n186\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in on_validation_end(self, trainer, pl_module)\\n211         \"\"\"\\n--> 212         self.save_checkpoint(trainer, pl_module)\\n213\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in save_checkpoint(self, trainer, pl_module)\\n261         # Mode 2: save the last checkpoint\\n--> 262         self._save_last_checkpoint(trainer, pl_module, monitor_candidates)\\n263\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in _save_last_checkpoint(self, trainer, pl_module, ckpt_name_metrics)\\n545         else:\\n--> 546             self._save_model(last_filepath, trainer, pl_module)\\n547         if (\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in _save_model(self, filepath, trainer, pl_module)\\n334         if self.save_function is not None:\\n--> 335             self.save_function(filepath, self.save_weights_only)\\n336         else:\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/properties.py in save_checkpoint(self, filepath, weights_only)\\n326     def save_checkpoint(self, filepath, weights_only: bool = False) -> None:\\n--> 327         self.checkpoint_connector.save_checkpoint(filepath, weights_only)\\n328\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py in save_checkpoint(self, filepath, weights_only)\\n396         # dump states as a checkpoint dictionary object\\n--> 397         checkpoint = self.dump_checkpoint(weights_only)\\n398         if self.trainer.is_global_zero:\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py in dump_checkpoint(self, weights_only)\\n283             # dump callbacks\\n--> 284             checkpoint[\\'callbacks\\'] = self.trainer.on_save_checkpoint(checkpoint)\\n285\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py in on_save_checkpoint(self, checkpoint)\\n221             else:\\n--> 222                 state = callback.on_save_checkpoint(self, self.lightning_module, checkpoint)\\n223             if state:\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/pruning.py in on_save_checkpoint(self, trainer, pl_module, checkpoint)\\n399             # prune a copy so training can continue with the same buffers\\n--> 400             copy = deepcopy(pl_module.to(\"cpu\"))\\n401             self.make_pruning_permanent(copy)\\n/usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\\n179                 else:\\n--> 180                     y = _reconstruct(x, memo, *rv)\\n181\\n/usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\\n280         if deep:\\n--> 281             state = deepcopy(state, memo)\\n282         if hasattr(y, \\'setstate\\'):\\n/usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\\n149     if copier:\\n--> 150         y = copier(x, memo)\\n151     else:\\n/usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\\n240     for key, value in x.items():\\n--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\\n242     return y\\n/usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\\n179                 else:\\n--> 180                     y = _reconstruct(x, memo, *rv)\\n181\\n/usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\\n280         if deep:\\n--> 281             state = deepcopy(state, memo)\\n282         if hasattr(y, \\'setstate\\'):\\n/usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\\n149     if copier:\\n--> 150         y = copier(x, memo)\\n151     else:\\n/usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\\n240     for key, value in x.items():\\n--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\\n242     return y\\n/usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\\n179                 else:\\n--> 180                     y = _reconstruct(x, memo, *rv)\\n181\\n/usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\\n280         if deep:\\n--> 281             state = deepcopy(state, memo)\\n282         if hasattr(y, \\'setstate\\'):\\n/usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\\n149     if copier:\\n--> 150         y = copier(x, memo)\\n151     else:\\n/usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\\n240     for key, value in x.items():\\n--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\\n242     return y\\n/usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\\n168                     if reductor:\\n--> 169                         rv = reductor(4)\\n170                     else:\\nTypeError: cannot serialize \\'_io.TextIOWrapper\\' object\\nDuring handling of the above exception, another exception occurred:\\nTypeError                                 Traceback (most recent call last)\\n in ()\\n11\\n12 trainer = pl.Trainer(gpus=-1, max_epochs = 15, logger=wandb_logger, profiler=SimpleProfiler(\"/content/bla\"), progress_bar_refresh_rate=20, callbacks=[prune])\\n---> 13 trainer.fit(model, dm)\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\\n497\\n498         # dispath start_training or start_testing or start_predicting\\n--> 499         self.dispatch()\\n500\\n501         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\\n544\\n545         else:\\n--> 546             self.accelerator.start_training(self)\\n547\\n548     def train_or_test_or_predict(self):\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\\n71\\n72     def start_training(self, trainer):\\n---> 73         self.training_type_plugin.start_training(trainer)\\n74\\n75     def start_testing(self, trainer):\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_training(self, trainer)\\n112     def start_training(self, trainer: \\'Trainer\\') -> None:\\n113         # double dispatch to initiate the training loop\\n--> 114         self._results = trainer.run_train()\\n115\\n116     def start_testing(self, trainer: \\'Trainer\\') -> None:\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\\n667         finally:\\n668             # hook\\n--> 669             self.train_loop.on_train_end()\\n670\\n671     def run_evaluation(self, max_batches=None, on_epoch=False):\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in on_train_end(self)\\n132         # when a checkpoint was saved at the last step\\n133         self.trainer.global_step -= 1\\n--> 134         self.check_checkpoint_callback(should_update=True, is_last=True)\\n135         self.trainer.global_step += 1\\n136\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in check_checkpoint_callback(self, should_update, is_last)\\n162\\n163             for cb in callbacks:\\n--> 164                 cb.on_validation_end(self.trainer, model)\\n165\\n166     def check_early_stopping_callback(self, should_update):\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in on_validation_end(self, trainer, pl_module)\\n210         checkpoints can be saved at the end of the val loop\\n211         \"\"\"\\n--> 212         self.save_checkpoint(trainer, pl_module)\\n213\\n214     def on_save_checkpoint(self, trainer, pl_module, checkpoint: Dict[str, Any]) -> Dict[str, Any]:\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in save_checkpoint(self, trainer, pl_module)\\n260\\n261         # Mode 2: save the last checkpoint\\n--> 262         self._save_last_checkpoint(trainer, pl_module, monitor_candidates)\\n263\\n264     def __validate_init_configuration(self):\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in _save_last_checkpoint(self, trainer, pl_module, ckpt_name_metrics)\\n544             trainer.training_type_plugin.rpc_save_model(self._save_model, last_filepath, trainer, pl_module)\\n545         else:\\n--> 546             self._save_model(last_filepath, trainer, pl_module)\\n547         if (\\n548             self.last_model_path and self.last_model_path != last_filepath\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in _save_model(self, filepath, trainer, pl_module)\\n333         # delegate the saving to the trainer\\n334         if self.save_function is not None:\\n--> 335             self.save_function(filepath, self.save_weights_only)\\n336         else:\\n337             raise ValueError(\".save_function() not set\")\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/properties.py in save_checkpoint(self, filepath, weights_only)\\n325\\n326     def save_checkpoint(self, filepath, weights_only: bool = False) -> None:\\n--> 327         self.checkpoint_connector.save_checkpoint(filepath, weights_only)\\n328\\n329     @Property\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py in save_checkpoint(self, filepath, weights_only)\\n395         \"\"\"\\n396         # dump states as a checkpoint dictionary object\\n--> 397         checkpoint = self.dump_checkpoint(weights_only)\\n398         if self.trainer.is_global_zero:\\n399             # write the checkpoint dictionary on the file\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py in dump_checkpoint(self, weights_only)\\n282         if not weights_only:\\n283             # dump callbacks\\n--> 284             checkpoint[\\'callbacks\\'] = self.trainer.on_save_checkpoint(checkpoint)\\n285\\n286             optimizer_states = []\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py in on_save_checkpoint(self, checkpoint)\\n220                 state = callback.on_save_checkpoint(self, self.lightning_module)  # noqa: parameter-unfilled\\n221             else:\\n--> 222                 state = callback.on_save_checkpoint(self, self.lightning_module, checkpoint)\\n223             if state:\\n224                 callback_states[type(callback)] = state\\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/pruning.py in on_save_checkpoint(self, trainer, pl_module, checkpoint)\\n398             prev_device = pl_module.device\\n399             # prune a copy so training can continue with the same buffers\\n--> 400             copy = deepcopy(pl_module.to(\"cpu\"))\\n401             self.make_pruning_permanent(copy)\\n402             checkpoint[\"state_dict\"] = copy.state_dict()\\n/usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\\n178                     y = x\\n179                 else:\\n--> 180                     y = _reconstruct(x, memo, *rv)\\n181\\n182     # If is its own copy, don\\'t memoize.\\n/usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\\n279     if state is not None:\\n280         if deep:\\n--> 281             state = deepcopy(state, memo)\\n282         if hasattr(y, \\'setstate\\'):\\n283             y.setstate(state)\\n/usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\\n148     copier = _deepcopy_dispatch.get(cls)\\n149     if copier:\\n--> 150         y = copier(x, memo)\\n151     else:\\n152         try:\\n/usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\\n239     memo[id(x)] = y\\n240     for key, value in x.items():\\n--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\\n242     return y\\n243 d[dict] = _deepcopy_dict\\n/usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\\n178                     y = x\\n179                 else:\\n--> 180                     y = _reconstruct(x, memo, *rv)\\n181\\n182     # If is its own copy, don\\'t memoize.\\n/usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\\n279     if state is not None:\\n280         if deep:\\n--> 281             state = deepcopy(state, memo)\\n282         if hasattr(y, \\'setstate\\'):\\n283             y.setstate(state)\\n/usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\\n148     copier = _deepcopy_dispatch.get(cls)\\n149     if copier:\\n--> 150         y = copier(x, memo)\\n151     else:\\n152         try:\\n/usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\\n239     memo[id(x)] = y\\n240     for key, value in x.items():\\n--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\\n242     return y\\n243 d[dict] = _deepcopy_dict\\n/usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\\n178                     y = x\\n179                 else:\\n--> 180                     y = _reconstruct(x, memo, *rv)\\n181\\n182     # If is its own copy, don\\'t memoize.\\n/usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\\n279     if state is not None:\\n280         if deep:\\n--> 281             state = deepcopy(state, memo)\\n282         if hasattr(y, \\'setstate\\'):\\n283             y.setstate(state)\\n/usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\\n148     copier = _deepcopy_dispatch.get(cls)\\n149     if copier:\\n--> 150         y = copier(x, memo)\\n151     else:\\n152         try:\\n/usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\\n239     memo[id(x)] = y\\n240     for key, value in x.items():\\n--> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\\n242     return y\\n243 d[dict] = _deepcopy_dict\\n/usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\\n167                     reductor = getattr(x, \"reduce_ex\", None)\\n168                     if reductor:\\n--> 169                         rv = reductor(4)\\n170                     else:\\n171                         reductor = getattr(x, \"reduce\", None)\\nTypeError: cannot serialize \\'_io.TextIOWrapper\\' object\\n\\nI am a bit overwhelmed by that error. Does anyone know what I am doing wrong? Or have I found a bug?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzI1MzIxcon': \"Hi @awaelchli and thanks for your time, as you asked in pull requests, i am pinging you here\\nFor other who see this, it's a discussion about Trainer.predict method where it is running BatchNorm Layers, code is below:\\nhttps://colab.research.google.com/drive/1jujP4F_prSmbRz-F_wGfWPTKGOmY5DPE?usp=sharing\\nWhat is the problem with my approach?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzI2NTIwcon': 'I found that in the code below sigmoid activation applied before binary_cross_entropy_with_logits loss:\\n\\n  \\n    \\n      pytorch-lightning/pl_examples/domain_templates/computer_vision_fine_tuning.py\\n    \\n    \\n        Lines 196 to 231\\n      in\\n      71b4611\\n    \\n  \\n  \\n    \\n\\n        \\n          \\n           def __build_model(self): \\n        \\n\\n        \\n          \\n               \"\"\"Define model layers & loss.\"\"\" \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n               # 1. Load pre-trained network: \\n        \\n\\n        \\n          \\n               model_func = getattr(models, self.backbone) \\n        \\n\\n        \\n          \\n               backbone = model_func(pretrained=True) \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n               _layers = list(backbone.children())[:-1] \\n        \\n\\n        \\n          \\n               self.feature_extractor = nn.Sequential(*_layers) \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n               # 2. Classifier: \\n        \\n\\n        \\n          \\n               _fc_layers = [ \\n        \\n\\n        \\n          \\n                   nn.Linear(2048, 256), \\n        \\n\\n        \\n          \\n                   nn.ReLU(), \\n        \\n\\n        \\n          \\n                   nn.Linear(256, 32), \\n        \\n\\n        \\n          \\n                   nn.Linear(32, 1), \\n        \\n\\n        \\n          \\n               ] \\n        \\n\\n        \\n          \\n               self.fc = nn.Sequential(*_fc_layers) \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n               # 3. Loss: \\n        \\n\\n        \\n          \\n               self.loss_func = F.binary_cross_entropy_with_logits \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n           def forward(self, x): \\n        \\n\\n        \\n          \\n               \"\"\"Forward pass. Returns logits.\"\"\" \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n               # 1. Feature extraction: \\n        \\n\\n        \\n          \\n               x = self.feature_extractor(x) \\n        \\n\\n        \\n          \\n               x = x.squeeze(-1).squeeze(-1) \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n               # 2. Classifier (returns logits): \\n        \\n\\n        \\n          \\n               x = self.fc(x) \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n               return torch.sigmoid(x) \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n           def loss(self, logits, labels): \\n        \\n\\n        \\n          \\n               return self.loss_func(input=logits, target=labels) \\n        \\n    \\n  \\n\\n\\nFrom the documentation https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss:\\nThis loss combines a Sigmoid layer and the BCELoss in one single class. \\nIs that performed intentionally? Or it\\'s just a bug?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzI2NTUwcon': 'I use one node and 4 gpus for training. And I use dali dataloader, I don\\'t know why my gpu util is low, and training is also slow. About 1:30 per epoch, I train for 200 epoches, which will cost 5 hours. It\\'s slower than the project mmclassification, which only cost 3.5 hours. Compared to mmclassification project which can only support torch.utils.data.dataloader, I think if I use dali_dataloader, it will accelerate my training. But as you can see, it\\'s the opposite. I don\\'t know why. Could anyone give me some advice? I use cifar10 dataset. And I train on slurm.\\n\\nHere is my code.\\nmain.py\\nimport pytorch_lightning as pl\\nfrom pytorch_lightning.callbacks import ModelCheckpoint\\nfrom net import ResNet18\\nif __name__ == \\'__main__\\':\\n    model = ResNet18()\\n    trainer = pl.Trainer( max_epochs=200,log_every_n_steps=1,\\n        log_gpu_memory=\\'min_max\\',gpus=4,num_nodes=1,accelerator=\\'ddp\\',\\n        fast_dev_run=False,callbacks=[ModelCheckpoint(monitor=\\'val_accuracy\\',mode=\\'max\\')],\\n        progress_bar_refresh_rate=1,replace_sampler_ddp=False)\\n    trainer.fit(model)\\nnet.py\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport pytorch_lightning as pl\\nfrom dataloader import dali_DataLoader,HybridPipe,dali_CIFAR10\\nclass BasicBlock(nn.Module):\\n    expansion = 1\\n\\n    def __init__(self, in_planes, planes, stride=1):\\n        super(BasicBlock, self).__init__()\\n        self.conv1 = nn.Conv2d(\\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\\n        self.bn1 = nn.BatchNorm2d(planes)\\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\\n                               stride=1, padding=1, bias=False)\\n        self.bn2 = nn.BatchNorm2d(planes)\\n\\n        self.shortcut = nn.Sequential()\\n        if stride != 1 or in_planes != self.expansion*planes:\\n            self.shortcut = nn.Sequential(\\n                nn.Conv2d(in_planes, self.expansion*planes,\\n                          kernel_size=1, stride=stride, bias=False),\\n                nn.BatchNorm2d(self.expansion*planes)\\n            )\\n\\n    def forward(self, x):\\n        out = F.relu(self.bn1(self.conv1(x)))\\n        out = self.bn2(self.conv2(out))\\n        out += self.shortcut(x)\\n        out = F.relu(out)\\n        return out\\n\\n\\nclass Bottleneck(nn.Module):\\n    expansion = 4\\n\\n    def __init__(self, in_planes, planes, stride=1):\\n        super(Bottleneck, self).__init__()\\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\\n        self.bn1 = nn.BatchNorm2d(planes)\\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\\n                               stride=stride, padding=1, bias=False)\\n        self.bn2 = nn.BatchNorm2d(planes)\\n        self.conv3 = nn.Conv2d(planes, self.expansion *\\n                               planes, kernel_size=1, bias=False)\\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\\n\\n        self.shortcut = nn.Sequential()\\n        if stride != 1 or in_planes != self.expansion*planes:\\n            self.shortcut = nn.Sequential(\\n                nn.Conv2d(in_planes, self.expansion*planes,\\n                          kernel_size=1, stride=stride, bias=False),\\n                nn.BatchNorm2d(self.expansion*planes)\\n            )\\n\\n    def forward(self, x):\\n        out = F.relu(self.bn1(self.conv1(x)))\\n        out = F.relu(self.bn2(self.conv2(out)))\\n        out = self.bn3(self.conv3(out))\\n        out += self.shortcut(x)\\n        out = F.relu(out)\\n        return out\\n\\n\\nclass ResNet(pl.LightningModule):\\n    def __init__(self, block, num_blocks, num_classes=10):\\n        super(ResNet, self).__init__()\\n        self.in_planes = 64\\n\\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\\n                               stride=1, padding=1, bias=False)\\n        self.bn1 = nn.BatchNorm2d(64)\\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\\n        self.linear = nn.Linear(512*block.expansion, num_classes)\\n        self.correct = 0\\n        self.total_size = 0\\n    def _make_layer(self, block, planes, num_blocks, stride):\\n        strides = [stride] + [1]*(num_blocks-1)\\n        layers = []\\n        for stride in strides:\\n            layers.append(block(self.in_planes, planes, stride))\\n            self.in_planes = planes * block.expansion\\n        return nn.Sequential(*layers)\\n\\n    def forward(self, x):\\n        out = F.relu(self.bn1(self.conv1(x)))\\n        out = self.layer1(out)\\n        out = self.layer2(out)\\n        out = self.layer3(out)\\n        out = self.layer4(out)\\n        out = F.avg_pool2d(out, 4)\\n        out = out.view(out.size(0), -1)\\n        out = self.linear(out)\\n        return out\\n    def training_step(self, batch, batch_idx):\\n        x, y = batch\\n        x = self(x)\\n        loss_fn = nn.CrossEntropyLoss()\\n        loss = loss_fn(x,y)\\n        predicted = torch.argmax(x, dim=1, keepdim=False)\\n        self.correct += (predicted == y).sum().item()\\n        self.total_size += y.size(0)\\n        self.log(\\'train_loss\\', loss,prog_bar=True, logger=True)\\n        self.log(\\'train_accuracy\\', self.correct/self.total_size,prog_bar=True, logger=True)\\n        return loss\\n    def validation_step(self, batch, batch_idx):\\n        x, y = batch\\n        x = self(x)\\n        loss_fn = nn.CrossEntropyLoss()\\n        loss = loss_fn(x,y)\\n        predicted = torch.argmax(x, dim=1, keepdim=False)\\n        self.correct += (predicted == y).sum().item()\\n        self.total_size += y.size(0)\\n        self.log(\\'val_loss\\', loss,on_step=False, on_epoch=True,prog_bar=True, logger=True)\\n        self.log(\\'val_accuracy\\', self.correct/self.total_size,prog_bar=True, logger=True)\\n        return loss\\n    def validation_epoch_end(self,out):\\n        self.log(\\'val_accuracy\\', self.correct/self.total_size,prog_bar=True, logger=True)\\n        self.correct=0\\n        self.total_size=0\\n    def train_epoch_end(self,out):\\n        self.log(\\'train_accuracy\\', self.correct/self.total_size,prog_bar=True, logger=True)\\n        self.correct=0\\n        self.total_size=0\\n    def configure_optimizers(self):\\n        optimizer = torch.optim.SGD(self.parameters(), lr=0.1)\\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [100,150], gamma=0.1, last_epoch=-1, verbose=False)\\n        return [optimizer],[scheduler]\\n    def train_dataloader(self):\\n        loader = dali_DataLoader(pipelines=HybridPipe(dali_CIFAR10(root=\\'./data\\'), batch_size=32, pad_ratio=1.25,num_threads=4,\\n             is_distribute=True, crop_size=32,ramdom_flip=True,\\n             normalize=dict(mean=[125.307, 122.961, 113.8575],std=[51.5865, 50.847, 51.255])))\\n        return loader\\n\\n    def val_dataloader(self):\\n        loader = dali_DataLoader(pipelines=HybridPipe(dali_CIFAR10(root=\\'./data\\',test_mode=True), batch_size=100,\\n             normalize=dict(mean=[125.307, 122.961, 113.8575],std=[51.5865, 50.847, 51.255])))\\n        return loader\\ndef ResNet18():\\n    return ResNet(BasicBlock, [2, 2, 2, 2])\\ndataloader.py\\nimport os,sys,math,random,pickle\\nimport torch\\nimport numpy as np\\nimport torch.distributed as dist\\ntry:\\n    from nvidia import dali\\n    from nvidia.dali.pipeline import Pipeline\\n    import nvidia.dali.types as types\\n    import nvidia.dali.fn as fn\\n    import nvidia.dali.ops as ops\\n    from nvidia.dali.plugin.pytorch import DALIClassificationIterator\\nexcept:\\n    print(\\'Could not import DALI\\')\\nclass dali_DataLoader():\\n    def __init__(self, pipelines, **kwargs):\\n        pipelines.build()\\n        try:\\n            self._dali_iterator = DALIClassificationIterator(pipelines=pipelines, size=len(pipelines.iterator.indices))\\n            self.sampler = pipelines.iterator\\n        except:\\n            self._dali_iterator = DALIClassificationIterator(pipelines=pipelines, reader_name=\\'Reader\\')\\n            self.sampler = self\\n    def set_epoch(self,epoch):\\n        pass\\n    def __iter__(self):\\n        return self\\n\\n    def __len__(self):\\n        return int(math.ceil(self._dali_iterator._size / self._dali_iterator.batch_size))\\n    def __next__(self):\\n        \\n        try:\\n            data = next(self._dali_iterator)\\n        except StopIteration:\\n            self._dali_iterator.reset()\\n            raise StopIteration\\n        # Decode the data output\\n        input = data[0][\\'data\\']\\n        target = data[0][\\'label\\'].squeeze().long()\\n\\n        return input,target\\nclass identity():\\n    def __call__(self,x,*tmp,**kargs):\\n        return x\\nclass HybridPipe(Pipeline):\\n    def __init__(self,dataset, batch_size, file_root=None,filelist_path=None,num_threads=1, pad_ratio=1,is_distribute=True, resize=None,crop_size=[0,0],ramdom_flip=False,normalize=None,random_rotate_degree=None):\\n        device_id = torch.cuda.current_device()\\n        print(\"device_id\",device_id)\\n        super(HybridPipe, self).__init__(batch_size, num_threads, device_id, seed=12 + device_id)\\n        \\n        if is_distribute:\\n            if filelist_path is not None:\\n                if file_root is None:\\n                    raise Exception(\"if provide filelist_path, then must provide file_root\")\\n                else:\\n                    self.input = ops.readers.File(file_root=file_root,file_list=filelist_path,num_shards=dist.get_world_size(),prefetch_queue_depth=num_threads,read_ahead=True,shard_id=dist.get_rank())\\n                    self.decode = ops.decoders.Image(device=\"mixed\", output_type=types.RGB)\\n                    self.use_file=True\\n            else:\\n                self.iterator = iter(Distribute_Input_Iter(dataset, batch_size))\\n                #self.input = ops.ExternalSource(source=self.iterator, num_outputs=2)\\n                self.input = ops.ExternalSource()\\n                self.input_label = ops.ExternalSource()\\n                self.use_file=False\\n        else:\\n            if filelist_path is not None:\\n                if file_root is None:\\n                    raise Exception(\"if provide filelist_path, then must provide file_root\")\\n                else:\\n                    self.input = ops.readers.File(file_root=file_root,file_list=filelist_path,num_shards=dist.get_world_size(),prefetch_queue_depth=num_threads,read_ahead=True,shard_id=dist.get_rank())\\n                    self.decode = ops.decoders.Image(device=\"mixed\", output_type=types.RGB)\\n                    self.use_file=True\\n            else:\\n                self.iterator = iter(Normal_Input_Iter(dataset, batch_size))\\n                self.input = ops.ExternalSource()\\n                self.input_label = ops.ExternalSource()\\n                self.use_file=False\\n        dali_device = \"gpu\"\\n        \\n        if isinstance(resize,(tuple,list)) and len(resize)==2:\\n            self.resize = ops.Resize(size=tuple(resize))\\n        elif isinstance(resize,(int, float)):\\n            self.resize = ops.Resize(size=tuple(resize,resize))\\n        else:\\n            self.resize = identity()\\n        if normalize is not None and isinstance(normalize,dict):\\n            self.mean = normalize.get(\\'mean\\',0)\\n            self.std = normalize.get(\\'std\\',1)\\n        else:\\n            self.mean = 0\\n            self.std = 1\\n        if isinstance(crop_size, (int, float)):\\n            crop_size = [crop_size,crop_size]\\n        if (len(crop_size)==2 and (crop_size[0]==0 or crop_size[1]==0)):\\n            self.crop = identity()\\n        else:\\n            self.crop = ops.Crop(device=dali_device, crop_h=crop_size[0], crop_w=crop_size[1])\\n        if pad_ratio>1:\\n            self.pad = ops.Paste(device=dali_device, ratio=pad_ratio, fill_value=0)\\n        else:\\n            self.pad = identity()\\n        self.cmnp = ops.CropMirrorNormalize(device=\"gpu\",\\n                                            dtype=types.FLOAT,\\n                                            output_layout=types.NCHW,\\n                                            mean=self.mean,\\n                                            std=self.std\\n                                            )\\n        if ramdom_flip:\\n            self.coin = ops.random.CoinFlip(probability=0.5)\\n        else:\\n            self.coin = lambda :0\\n        if random_rotate_degree is not None:\\n            try:\\n                tmp = math.abs(int(random_rotate_degree))\\n                self.degree = ops.random.Uniform(range=(-tmp, tmp))\\n                self.rotate = ops.Rotate()\\n            except:\\n                self.degree = lambda :0\\n                self.rotate = identity()\\n        else:\\n            self.degree = lambda :0\\n            self.rotate = identity()\\n        \\n    def iter_setup(self):\\n        if not self.use_file:\\n            (images, labels) = self.iterator.__next__()\\n            self.feed_input(self.jpegs, images, layout=\"HWC\")\\n            self.feed_input(self.labels, labels)\\n\\n    def define_graph(self):\\n        rng = self.coin()\\n        print()\\n        if self.use_file:\\n            self.jpegs,self.labels = self.input(name=\"Reader\")\\n            self.jpegs = self.decode(self.jpegs)\\n        else:\\n            self.jpegs= self.input()\\n            self.labels = self.input_label()\\n        output = self.jpegs\\n        output = self.resize(output)\\n        output = self.rotate(output, angle=self.degree())\\n        output = self.pad(output.gpu())\\n        output = self.crop(output)\\n        output = self.cmnp(output, mirror=rng)\\n        return [output, self.labels]\\nclass Distribute_Input_Iter():\\n    def __init__(self,dataset, batch_size, num_replicas=None,rank=None,shuffle=True,seed=0,drop_last=False):\\n        if num_replicas is None:\\n            if not dist.is_available():\\n                raise RuntimeError(\"Requires distributed package to be available\")\\n            num_replicas = dist.get_world_size()\\n            #num_replicas = 1\\n        if rank is None:\\n            if not dist.is_available():\\n                raise RuntimeError(\"Requires distributed package to be available\")\\n            rank = dist.get_rank()\\n            #rank = 0\\n        if rank >= num_replicas or rank < 0:\\n            raise ValueError(\\n                \"Invalid rank {}, rank should be in the interval\"\\n                \" [0, {}]\".format(rank, num_replicas - 1))\\n        self.dataset = dataset\\n        self.batch_size = batch_size\\n        self.num_replicas = num_replicas\\n        self.rank = rank\\n        self.epoch = 0\\n        self.drop_last = drop_last\\n        \\n        # If the dataset length is evenly divisible by # of replicas, then there\\n        # is no need to drop any data, since the dataset will be split equally.\\n        if self.drop_last and len(self.dataset) % self.num_replicas != 0:  # type: ignore\\n            # Split to nearest available length that is evenly divisible.\\n            # This is to ensure each rank receives the same amount of data when\\n            # using this Sampler.\\n            self.num_samples = math.ceil(\\n                # `type:ignore` is required because Dataset cannot provide a default __len__\\n                # see NOTE in pytorch/torch/utils/data/sampler.py\\n                (len(self.dataset) - self.num_replicas) / self.num_replicas  # type: ignore\\n            )\\n        else:\\n            self.num_samples = math.ceil(len(self.dataset) / self.num_replicas)  # type: ignore\\n        self.total_size = self.num_samples * self.num_replicas\\n        self.shuffle = shuffle\\n        self.seed = seed\\n        self.epoch=0\\n        indices = list(range(len(self.dataset)))  # type: ignore\\n\\n        if not self.drop_last:\\n            # add extra samples to make it evenly divisible\\n            padding_size = self.total_size - len(indices)\\n            if padding_size <= len(indices):\\n                indices += indices[:padding_size]\\n            else:\\n                indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size]\\n        else:\\n            # remove tail of data to make it evenly divisible.\\n            indices = indices[:self.total_size]\\n        assert len(indices) == self.total_size,\\'len(indices) != self.total_size\\'\\n\\n        # subsample\\n        indices = indices[self.rank:self.total_size:self.num_replicas]\\n        assert len(indices) == self.num_samples,\\'len(indices) != self.num_samples\\'\\n        self.indices = indices\\n    def set_epoch(self,epoch):\\n        self.epoch = epoch\\n    def __iter__(self):\\n        self.i = 0\\n        self.n = len(self.indices)\\n        return self\\n    def __next__(self):\\n        batch = []\\n        labels = []\\n        should_shuffle = False\\n        \\n        for _ in range(self.batch_size):\\n            if self.i % self.n == self.n-1:\\n                should_shuffle = True\\n            img, label = self.dataset.__getitem__(self.indices[self.i])\\n            batch.append(img)\\n            labels.append(label)\\n            self.i = (self.i + 1) % self.n\\n        if should_shuffle:\\n            random.shuffle(self.indices)\\n        return (batch, labels)\\nclass Normal_Input_Iter():\\n    def __init__(self,dataset, batch_size):\\n        self.dataset = dataset\\n        self.batch_size = batch_size\\n        self.indices = list(range(len(self.dataset)))\\n    def __iter__(self):\\n        self.i = 0\\n        self.n = len(self.dataset)\\n        return self\\n    def __next__(self):\\n        batch = []\\n        labels = []\\n        should_shuffle = False\\n        \\n        for _ in range(self.batch_size):\\n            if self.i % self.n == self.n-1:\\n                should_shuffle = True\\n            img, label = self.dataset.__getitem__(self.indices[self.i])\\n            batch.append(img)\\n            labels.append(label)\\n            self.i = (self.i + 1) % self.n\\n        if should_shuffle:\\n            random.shuffle(self.indices)\\n        return (batch, labels)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzI3MTU3con': 'Am I right in calling the tune method as follows?\\ntrainer.tune(model, train_dataloader, val_dataloader) \\nHere is the stacktrace of the error I am getting.\\ntrainer.tune(model, train_dataloader, val_dataloader)\\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in tune\\n    self.tuner.tune(model, train_dataloader, val_dataloaders, datamodule)\\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py\", line 63, in tune\\n    self.lr_find(model, update_attr=True)\\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py\", line 136, in lr_find\\n    self.setup_trainer(model, train_dataloader, val_dataloaders, datamodule)\\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py\", line 44, in setup_trainer\\n    self.trainer.train_loop.setup_fit(model, train_dataloader, val_dataloaders, datamodule)\\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 120, in setup_fit\\n    self.trainer.config_validator.verify_loop_configurations(model)\\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py\", line 36, in verify_loop_configurations\\n    self.__verify_train_loop_configuration(model)\\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py\", line 58, in __verify_train_loop_configuration\\n    raise MisconfigurationException(\\npytorch_lightning.utilities.exceptions.MisconfigurationException: No `train_dataloader()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()`\\n to be defined.\\n\\ntrainer.fit(model, train_dataloader, val_dataloader) works fine. I don\\'t define the data loader as a method in my model, rather I pass one in to my .fit methods. This seems to work well for the .fit method, but not for .tune.\\nFWIW, here is my model definition\\nclass PretrainedResnet50FT(pl.LightningModule):\\n    \\n    @staticmethod\\n    def add_model_specific_args(parent_parser):\\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\\n        parser.add_argument(\\'--num_classes\\', type=int, default=2)\\n        parser.add_argument(\\'--lr\\', type=float, default=1e-3)\\n        return parser\\n\\n    def __init__(self, hparams):\\n        super().__init__()\\n        self.hparams = hparams\\n\\n        image_modules = list(models.resnet50(pretrained=True, progress=False).children())[:-1]\\n        self.resnet = nn.Sequential(*image_modules)\\n        self.classifier = nn.Linear(2048, self.hparams.num_classes)\\n\\n    def forward(self, x):\\n        out = self.resnet(x)\\n        out = torch.flatten(out, 1)\\n        out = self.classifier(out)\\n        return out\\n\\n    def step(self, who, batch, batch_nb):    \\n        x, task_labels, slide_id = batch\\n        \\n        #Define logits over the task and source embeddings\\n        task_logits = self(x)\\n\\n        #Define loss values over the logits\\n        loss = task_loss = F.cross_entropy(task_logits, task_labels, reduction = \"mean\")                \\n                \\n        #Train acc\\n        task_preds = task_logits.argmax(-1)\\n        task_acc = pl.metrics.functional.accuracy(task_preds, task_labels)\\n        \\n        #F1\\n        task_f1 = pl.metrics.functional.f1(task_preds, task_labels, num_classes = self.hparams.num_classes)\\n\\n        self.log(who + \\'_loss\\', loss)\\n        self.log(who + \\'_acc\\', task_acc)\\n        self.log(who + \\'_f1\\', task_f1)\\n\\n        return loss\\n\\n    def training_step(self, batch, batch_nb):\\n        # REQUIRED\\n        loss = self.step(\\'train\\', batch, batch_nb)\\n        return loss\\n\\n    def validation_step(self, batch, batch_nb):\\n        loss = self.step(\\'val\\', batch, batch_nb)\\n        return loss\\n        \\n    def test_step(self, batch, batch_nb):\\n        loss = self.step(\\'test\\', batch, batch_nb)\\n        return loss\\n\\n    def configure_optimizers(self):\\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzI5MjQ0con': \"I'm trying to use Layer-wise relevance propagation as part of the training and validation loop of training my model, but it requires gradients to be present in order to calculate the relevance. I'm trying to figure out the best way of approaching this in a Lightning-friendly way. I can already do it for the training loop, but the validation loop is what's giving me problems. The only way I can think to do it at the moment is once the validation loop finishes, re-run the validation dataset with gradients enabled without calling the optimiser's .step() function. I'd really appreciate some guidance here before I dive in. Thanks!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzIwNDE5con': 'Hello everyone,\\nI have upgraded pytorch-lightning to 1.2.6 recently, the behavior of dp seems different from 1.2.0. To be specific, the returned values of validation_step() are automatically reduced before sent to validation_epoch_end(). However, the metrics I use need the original predictions of each sample instead of the reduced values. Is there a way to disable the automatic reduce and pass the whole predictions to validation_epoch_end()? Note that the validation_step_end() is not implemented in my model.\\ncc: @tchaton',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzM0MTk2con': 'If I save scalar log like this,\\nself.logger.experiment.add_scalars(‘loss/nll’, {‘train’: trainloss, ‘valid’: validloss})\\n\\nHow to monitor valid loss in Modelcheckpoint?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzM0MTk4con': 'I want to better understand the setup and prepare_data methods in multi gpu scenariu in context of NLP and text processing.\\nI have prepared the DataModule which process json line file with pairs of sentence for translation task. The file contains 10M lines.\\nIn prepare_data() I open the data file, read it to memory, do some basic filtering (remove to long sentences and do some sorting based on length in order to group similar length sentences together) then I write it to another file (filtered_data.json).\\nNext in the setup() method I read filtered_data.json and split it to train and valid.\\nI can perform split deterministically so train and valid splits will always have the same elements or I can split randomly then each GPU process will have a different train and valid sets.\\nWhen using it in multi-gpu (2 GPUs) each process will have its own copy of the train and valid set (am I right?). Which approach is better in context data utilization, random or deterministically?\\nI do not fully understand how distributed DataLoader handles these two approaches? Could someone explain it in detail?\\nIf data are loaded deterministically then all GPU processes, especially forward and backward pass will return the same values (for gpu 1 and 2), it is efficient? How gradients are merged and how network weight updates will be performed.\\nOr maybe the second (random split) approach is better because gradients computed on different samples and merged from 2 gpus will result in a better estimation of the true gradient.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzM4MDM3con': 'Hey!\\nI tried to change to using the new way of logging through MetricCollection and self.log_dict instead of logging every metric through self.log on training step and test_epoch_end. However, each metric is then logged as [metric_name]_epoch_[epoch_number] which creates a new graph for every epoch instead of allowing me to use epoch on the x-axis of my graphs (on Comet, if that is relevant). Is there a way to control this behaviour of log_dict, or do I just have to keep logging \"manually\" to control log name?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzM5NjY1con': \"It took me quite some time playing with PyTorch Lightning and reading through the docs before I realized I can attach the data-related methods ([train|val]_dataloader, prepare_data) directly to LightningModule and then pass only the module to the trainer, rather than having to create a separate LightningDataModule class or passing the data loaders explicitly. When thinking of LightningModule as a task, rather than a model, this makes actually perfect sense. After all, things like batch size and learning rate are inherently related. But since I haven't seen it really explicitly documented, I wonder whether the use of data methods directly on the LightningModule is considered standard?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzMyMTA0con': '\"monitor (Optional[str]) – quantity to monitor. By default it is None which saves a checkpoint only for the last epoch.\"\\nwhen i trainning a model, i set the \\'monitor\\' to None, it should save the last epoch as the doc says. but it still save depend on the val_loss, it always save the model with lowest val_loss.\\ni also try another way, set the \\'save_last\\' to True. while this needs to set a monitor. And if i set save_top_k to 0, it will save nothing; if set to 1, it will save 2 models, the best one and the last one. But i just want to save the last one.\\nis this a bug or i made sth wrong?  is there a way to save model with epoch asigned myself? such as the last 3 epochs?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzMyNzMxcon': '🐛 Bug\\nHello, I\\'m trying to use Pytorch Lightning in order to speed up my ESR GAN renders on Windows 10.\\nHowever, when I ran the installation code and attempt to run Cupscale (which I use as a GUI for ESR GAN), I get an error saying \"Pytorch compiled without CUDA\".\\nIs there a way to choose to install specifically the CUDA version of Pytorch with Lightning install, or are the two incompatible? If the latter is the case, that\\'s not good to hear. If the former, can I have such a code?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzMzNzUxcon': 'If I have 100 training examples and 100 validation examples, and I run on a single gpu with a batch size of 10, the tqdm bar will show 20 epoch iterations. If I run on 2 gpus with ddp and the same batch size, the tqdm bar will still show 20 epoch iterations, but isnt the effective batch size now 20 instead of 10 because theres 2 gpus? Shouldnt the total number of iterations be half?\\nThanks for any clarification.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzMzODc0con': 'I want the checkpoint and the logs stay in the same place while only the logs are uploaded to wandb server.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ1Njc0con': 'Once the model is trained, what would be the best way to use it to do a forward pass in a large dataset (that must be divided in batches not to fill GPU memory) and obtain the output?\\nWhat I normally do is to use the LightningModule as a torch Module. However, that forces me to: write a for, explicitly transfer to the device each batch, make sure to cal .eval(), concatenate the output of each batch... all the things that pytorch-lightning saves me from doing during training. But I guess I am missing an obvious simpler option?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ2Mzk5con': 'Hi, I know that there is EarlyStopping if validation metrics are deteriorating. But I was wondering if it was possible to stop training if after say epoch 10, the accuracy hasn’t reached say 20%. If such a callback doesn’t exist, any thoughts on how I can get started on the implementation of it?\\nFor context I am running a distributed hyper-parameter optimizer and I know that the “good” hyper-parameter set will get me to 50% accuracy by epoch 5.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ2ODUxcon': \"I would like to have a step called before the first training step, and that yet necessitates the dataloader\\ne.g. (mock code)\\nclass Scaler(nn.Module):\\n    '''center target data'''\\n     def __init__(self, dims):\\n         self.mean = nn.Parameter(torch.tensor(dims))\\n         self.n = nn.Parameters(torch.zeros(1))\\n\\n     def forward(self, batch):\\n          input, target = batch\\n          if self.training:\\n              self.mean += target.mean(0)\\n              self.n += 1\\n          else:\\n              return input, (target - self.mean)/self.n\\n\\nclass MySystem(pl.LightningModule):\\n    def __init__(self, scaler_dims, model_dims):\\n        self.model = nn.Linear(**model_dims)\\n        self.scaler = Scaler(self.dims).train()\\n\\n    def on_first_epoch(self, dataloader):  # <---- not sure where this should live\\n         # learn to scale the dataset\\n         for batch in dataloader:\\n               self.scaler(batch)\\n\\n    def training_step(self, batch, batch_idx):\\n         self.scaler.eval()\\n         input, target = self.scaler(batch)\\n         pred = self.model(input)\\n         loss = F.l1_loss(pred, target)\\n         return loss\\n      \\n\\ndm = MyDataModule()\\nsystem = MySystem()\\ntrainer = pl.Trainer()\\ntrainer.fit(system, dm) \\nI'm not clear on how to do this with PL's API: nn.LithningModule.setup() does not have access to the dataloader.\\nAny advice?\\nThanks!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ3MzEycon': \"Hi,\\nI want to save a copy of the running script of each experiment.\\nI have tried to apply the following callback:\\nimport os\\nfrom pathlib import Path\\n\\nclass MyCopyingCallback(pl.Callback):\\n\\n    def on_init_end(self, trainer):\\n        log_dir = trainer.logger.log_dir\\n        Path(log_dir).mkdir(parents=True, exist_ok=True)\\n        shutil.copy2(os.path.realpath(__file__), os.path.join(log_dir, os.path.basename(os.path.realpath(__file__))))\\n\\nThe problem is that I ended up with two copies of my running script under two different folders (I'm using the ddp backend)\\nAny ideas on how to solve this issue?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ3MzM1con': '',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ3NDQzcon': 'I see that there is an option to return just an optimizer in the configure_optimizers function. What will be the default scheduler in that case?\\nBTW I tried very hard to find the answer in the docs and could not find it..',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ5MDYxcon': 'How could someone shuffle the training dataloader (using Datamodule) on each epoch?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQxMjI5con': \"Good day, I'm currently working on two models which train on the same data. I'd like to integrate the two pre-trained models into one and use it for transfer learning. The combination is written as such (you can copy paste to run it).\\nimport pytorch_lightning as pl\\nimport torch\\nimport torch.nn.functional as F\\nfrom torch.utils.data import DataLoader, TensorDataset\\n\\nclass MyModelA(pl.LightningModule):\\n    def __init__(self, hidden_dim = 10):\\n        super(MyModelA, self).__init__()\\n        self.fc1 = torch.nn.Linear(hidden_dim, 2)\\n        self.save_hyperparameters()\\n\\n    def configure_optimizers(self):\\n        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\\n        return optimizer\\n        \\n    def forward(self, x):\\n        x = self.fc1(x)\\n        return x\\n\\n    def training_step(self, batch, batch_idx):\\n        x,y = batch\\n        return F.mse_loss(self.forward(x), y)\\n    \\nclass MyModelB(pl.LightningModule):\\n    def __init__(self, hidden_dim = 10):\\n        super(MyModelB, self).__init__()\\n        self.fc1 = torch.nn.Linear(hidden_dim, 2)\\n        self.save_hyperparameters()\\n    \\n    def configure_optimizers(self):\\n        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\\n        return optimizer\\n        \\n    def forward(self, x):\\n        x = self.fc1(x)\\n        return x\\n\\n    def training_step(self, batch, batch_idx):\\n        x,y = batch\\n        return F.mse_loss(self.forward(x), y)\\n\\nclass MyEnsemble(pl.LightningModule):\\n    def __init__(self, modelA, modelB):\\n        super(MyEnsemble, self).__init__()\\n        self.modelA = modelA\\n        self.modelB = modelB\\n        self.modelA.freeze()\\n        self.modelB.freeze()\\n        self.classifier = torch.nn.Linear(4, 2)\\n\\n        #self.save_hyperparameters() # Uncomment to show error\\n\\n    def configure_optimizers(self):\\n        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\\n        return optimizer\\n        \\n    def forward(self, x):\\n        x1 = self.modelA(x)\\n        x2 = self.modelB(x)\\n        x = torch.cat((x1, x2), dim=1)\\n        x = self.classifier(x)\\n        return x\\n\\n    def training_step(self, batch, batch_idx):\\n        x, y = batch\\n        return F.mse_loss(self.forward(x), y)\\n\\ndl = DataLoader(TensorDataset(torch.randn(1000, 10), \\n                              torch.randn(1000, 2)), \\n                batch_size = 10)\\n\\nmodelA = MyModelA()\\nmodelB = MyModelB()\\n\\n# pretrained modelA and modelB\\ntrainerA = pl.Trainer(gpus = 0, max_epochs = 5, progress_bar_refresh_rate = 50)\\ntrainerA.fit(modelA, dl)\\ntrainerB = pl.Trainer(gpus = 0, max_epochs = 5, progress_bar_refresh_rate = 50)\\ntrainerB.fit(modelB, dl)\\n\\n# modelA and modelB contains pretrained weights\\nmodel = MyEnsemble(modelA, modelB)\\n\\ntrainer = pl.Trainer(gpus = 0, max_epochs = 5, progress_bar_refresh_rate = 50)\\ntrainer.fit(model, dl)\\nAt first the code worked fine. However, I would like to save the hyperparameters of the ensemble module, but adding self.save_hyperparameters() at the end of the ensemble module __init__ return this error.\\nValueError: dictionary update sequence element #0 has length 1; 2 is required\\n\\nHence my question is how can I combine two or more lightning modules in a single module and save its hyperparameters? Or is there any alternative way to do so?\\nThanks in advance!\\nEDIT: Code updated to show both modelA and modelB are pretrained.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQzNzY5con': \"Hello pytorch-lightning community,\\nmy training hangs when training on multi-nodes; on single node with multiple GPUs runs fine :/\\nIt baffles me that although the global rank ID seems right, the member output has 4 instead of 8 in the denominator.\\nSince I run in a slurm environment, do I have to add the SLURMEnvironment plugin in the Trainer? I tried to add it alongside the DDPPlugin but it was not accepted (Found invalid type for plugin <class 'pytorch_lightning.plugins.environments.slurm_environment.SLURMEnvironment'>. Expected a precision or training type plugin)\\nThe job submission file has the corresponding lines:\\n#SBATCH --gres=gpu:4\\n#SBATCH --nodes=2\\n#SBATCH --exclusive\\nsrun --ntasks=8 python3 coolModel.py 2>&1 | tee log.train\\nI attach the output and the code below...\\npytorch version:  1.8.1\\npytorch-lightning version:  1.2.4\\nCheers,\\nNikos\\nmultNodeTraining.txt\\n###########python code\\nimport os\\nimport torch\\nfrom torch.nn import functional as F\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.datasets import MNIST\\nimport torchvision.transforms as transforms\\nimport pytorch_lightning as pl\\nfrom pytorch_lightning import Trainer\\n#from test_tube import Experiment\\nclass database(pl.LightningDataModule):\\ndef __init__(self):\\n    super().__init__()\\n\\n#def setup(self, stage=None):\\ndef train_dataloader(self):\\n    return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\\n\\ndef val_dataloader(self):\\n    return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\\n\\ndef test_dataloader(self):\\n    return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\\n\\nclass CoolModel(pl.LightningModule):\\ndef __init__(self):\\n    super(CoolModel, self).__init__()\\n    # not the best model...\\n    self.l1 = torch.nn.Linear(28 * 28, 10)\\n\\ndef forward(self, x):\\n    return torch.relu(self.l1(x.view(x.size(0), -1)))\\n\\ndef my_loss(self, y_hat, y):\\n    return F.cross_entropy(y_hat, y)\\n\\ndef training_step(self, batch, batch_nb):\\n    x, y = batch\\n    y_hat = self.forward(x)\\n    return {'loss': self.my_loss(y_hat, y)}\\n\\ndef validation_step(self, batch, batch_nb):\\n    x, y = batch\\n    y_hat = self.forward(x)\\n    return {'val_loss': self.my_loss(y_hat, y)}\\n\\ndef validation_end(self, outputs):\\n    avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\\n    return {'avg_val_loss': avg_loss}\\n\\ndef configure_optimizers(self):\\n    return [torch.optim.Adam(self.parameters(), lr=0.02)]\\n\\nif name=='main':\\ndm = database()\\nmodel = CoolModel()\\n#exp = Experiment(save_dir=os.getcwd())\\n#checkp1 = pl.callbacks.ModelCheckpoint(\\n#    monitor='loss', save_top_k=2, dirpath='./', mode='min', save_last=True\\n#    ) \\ntrainer = Trainer(\\n        max_epochs=10,\\n        gpus=4, num_nodes=2, accelerator='ddp',\\n        plugins= pl.plugins.DDPPlugin(find_unused_parameters=False), #pl.plugins.SLURMEnvironment],\\n        #progress_bar_refresh_rate=0\\n        )\\n# train on 32 gpus across 4 nodes (make sure to submit appropriate SLURM job)\\n# trainer = Trainer(experiment=exp, max_nb_epochs=1, gpus=[0, 1, 2, 3, 4, 5, 6, 7], nb_gpu_nodes=4)\\ntrainer.fit(model, dm)\\n# view tensorflow logs\\nprint(f'View tensorboard logs by running\\\\ntensorboard --logdir {os.getcwd()}')\\nprint('and going to http://localhost:6006 on your browser')\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU0NTUycon': '[1.3.0] - 2021-05-06\\nAdded\\n\\nAdded support for the EarlyStopping callback to run at the end of the training epoch (#6944)\\nAdded synchronization points before and after setup hooks are run (#7202)\\nAdded a teardown hook to ClusterEnvironment (#6942)\\nAdded utils for metrics to scalar conversions (#7180)\\nAdded utils for NaN/Inf detection for gradients and parameters (#6834)\\nAdded more explicit exception message when trying to execute trainer.test() or trainer.validate() with fast_dev_run=True (#6667)\\nAdded LightningCLI class to provide simple reproducibility with minimum boilerplate training CLI (#4492, #6862, #7156, #7299)\\nAdded gradient_clip_algorithm argument to Trainer for gradient clipping by value (#6123).\\nAdded a way to print to terminal without breaking up the progress bar (#5470)\\nAdded support to checkpoint after training steps in ModelCheckpoint callback (#6146)\\nAdded TrainerStatus.{INITIALIZING,RUNNING,FINISHED,INTERRUPTED} (#7173)\\nAdded Trainer.validate() method to perform one evaluation epoch over the validation set (#4948)\\nAdded LightningEnvironment for Lightning-specific DDP (#5915)\\nAdded teardown() hook to LightningDataModule (#4673)\\nAdded auto_insert_metric_name parameter to ModelCheckpoint (#6277)\\nAdded arg to self.log that enables users to give custom names when dealing with multiple dataloaders (#6274)\\nAdded teardown method to BaseProfiler to enable subclasses defining post-profiling steps outside of __del__ (#6370)\\nAdded setup method to BaseProfiler to enable subclasses defining pre-profiling steps for every process (#6633)\\nAdded no return warning to predict (#6139)\\nAdded Trainer.predict config validation (#6543)\\nAdded AbstractProfiler interface (#6621)\\nAdded support for including module names for forward in the autograd trace of PyTorchProfiler (#6349)\\nAdded support for the PyTorch 1.8.1 autograd profiler (#6618)\\nAdded outputs parameter to callback\\'s on_validation_epoch_end & on_test_epoch_end hooks (#6120)\\nAdded configure_sharded_model hook (#6679)\\nAdded support for precision=64, enabling training with double precision (#6595)\\nAdded support for DDP communication hooks (#6736)\\nAdded artifact_location argument to MLFlowLogger which will be passed to the MlflowClient.create_experiment call (#6677)\\nAdded model parameter to precision plugins\\' clip_gradients signature (#6764, #7231)\\nAdded is_last_batch attribute to Trainer (#6825)\\nAdded LightningModule.lr_schedulers() for manual optimization  (#6567)\\nAdded MpModelWrapper in TPU Spawn (#7045)\\nAdded max_time Trainer argument to limit training time (#6823)\\nAdded on_predict_{batch,epoch}_{start,end} hooks (#7141)\\nAdded new EarlyStopping parameters stopping_threshold and divergence_threshold (#6868)\\nAdded debug flag to TPU Training Plugins (PT_XLA_DEBUG) (#7219)\\nAdded new UnrepeatedDistributedSampler and IndexBatchSamplerWrapper for tracking distributed predictions (#7215)\\nAdded trainer.predict(return_predictions=None|False|True) (#7215)\\nAdded BasePredictionWriter callback to implement prediction saving (#7127)\\nAdded trainer.tune(scale_batch_size_kwargs, lr_find_kwargs) arguments to configure the tuning algorithms (#7258)\\nAdded tpu_distributed check for TPU Spawn barrier (#7241)\\nAdded device updates to TPU Spawn for Pod training (#7243)\\nAdded warning when missing Callback and using resume_from_checkpoint (#7254)\\nDeepSpeed single file saving (#6900)\\nAdded Training type Plugins Registry (#6982, #7063, #7214, #7224)\\nAdd ignore param to save_hyperparameters (#6056)\\n\\nChanged\\n\\nChanged LightningModule.truncated_bptt_steps to be property (#7323)\\nChanged EarlyStopping callback from by default running EarlyStopping.on_validation_end if only training is run. Set check_on_train_epoch_end to run the callback at the end of the train epoch instead of at the end of the validation epoch (#7069)\\nRenamed pytorch_lightning.callbacks.swa to pytorch_lightning.callbacks.stochastic_weight_avg (#6259)\\nRefactor RunningStage and TrainerState usage (#4945, #7173)\\n\\nAdded RunningStage.SANITY_CHECKING\\nAdded TrainerFn.{FITTING,VALIDATING,TESTING,PREDICTING,TUNING}\\nChanged trainer.evaluating to return True if validating or testing\\n\\n\\nChanged setup() and teardown() stage argument to take any of {fit,validate,test,predict} (#6386)\\nChanged profilers to save separate report files per state and rank (#6621)\\nThe trainer no longer tries to save a checkpoint on exception or run callback\\'s on_train_end functions (#6864)\\nChanged PyTorchProfiler to use torch.autograd.profiler.record_function to record functions (#6349)\\nDisabled lr_scheduler.step() in manual optimization  (#6825)\\nChanged warnings and recommendations for dataloaders in ddp_spawn (#6762)\\npl.seed_everything will now also set the seed on the DistributedSampler (#7024)\\nChanged default setting for communication of multi-node training using DDPShardedPlugin (#6937)\\ntrainer.tune() now returns the tuning result (#7258)\\nLightningModule.from_datasets() now accepts IterableDataset instances as training datasets. (#7503)\\nChanged resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075)\\nAutomatically set sync_batchnorm for training_type_plugin (#6536)\\nAllowed training type plugin to delay optimizer creation (#6331)\\nRemoved ModelSummary validation from train loop on_trainer_init (#6610)\\nMoved save_function to accelerator (#6689)\\nUpdated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)\\nImproved verbose logging for EarlyStopping callback (#6811)\\nRun ddp_spawn dataloader checks on Windows (#6930)\\nUpdated mlflow with using resolve_tags (#6746)\\nMoved save_hyperparameters to its own function (#7119)\\nReplaced _DataModuleWrapper with __new__ (#7289)\\nReset current_fx properties on lightning module in teardown (#7247)\\nAuto-set DataLoader.worker_init_fn with seed_everything (#6960)\\nRemove model.trainer call inside of dataloading mixin (#7317)\\nSplit profilers module (#6261)\\nEnsure accelerator is valid if running interactively (#5970)\\nDisabled batch transfer in DP mode (#6098)\\n\\nDeprecated\\n\\nDeprecated outputs in both LightningModule.on_train_epoch_end and Callback.on_train_epoch_end hooks (#7339)\\nDeprecated Trainer.truncated_bptt_steps in favor of LightningModule.truncated_bptt_steps (#7323)\\nDeprecated outputs in both LightningModule.on_train_epoch_end and Callback.on_train_epoch_end hooks (#7339)\\nDeprecated LightningModule.grad_norm in favor of pytorch_lightning.utilities.grads.grad_norm (#7292)\\nDeprecated the save_function property from the ModelCheckpoint callback (#7201)\\nDeprecated LightningModule.write_predictions and LightningModule.write_predictions_dict (#7066)\\nDeprecated TrainerLoggingMixin in favor of a separate utilities module for metric handling (#7180)\\nDeprecated TrainerTrainingTricksMixin in favor of a separate utilities module for NaN/Inf detection for gradients and parameters (#6834)\\nperiod has been deprecated in favor of every_n_val_epochs in the ModelCheckpoint callback (#6146)\\nDeprecated trainer.running_sanity_check in favor of trainer.sanity_checking (#4945)\\nDeprecated Profiler(output_filename) in favor of dirpath and filename (#6621)\\nDeprecated PytorchProfiler(profiled_functions) in favor of record_functions (#6349)\\nDeprecated @auto_move_data in favor of trainer.predict (#6993)\\nDeprecated Callback.on_load_checkpoint(checkpoint) in favor of Callback.on_load_checkpoint(trainer, pl_module, checkpoint) (#7253)\\nDeprecated metrics in favor of torchmetrics (#6505, #6530, #6540, #6547, #6515, #6572, #6573, #6584, #6636, #6637, #6649, #6659, #7131)\\nDeprecated the LightningModule.datamodule getter and setter methods; access them through Trainer.datamodule instead (#7168)\\nDeprecated the use of Trainer(gpus=\"i\") (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)\\n\\nRemoved\\n\\nRemoved the exp_save_path property from the LightningModule (#7266)\\nRemoved training loop explicitly calling EarlyStopping.on_validation_end if no validation is run (#7069)\\nRemoved automatic_optimization as a property from the training loop in favor of LightningModule.automatic_optimization (#7130)\\nRemoved evaluation loop legacy returns for *_epoch_end hooks (#6973)\\nRemoved support for passing a bool value to profiler argument of Trainer (#6164)\\nRemoved no return warning from val/test step (#6139)\\nRemoved passing a ModelCheckpoint instance to Trainer(checkpoint_callback) (#6166)\\nRemoved deprecated Trainer argument enable_pl_optimizer and automatic_optimization (#6163)\\nRemoved deprecated metrics (#6161)\\n\\nfrom pytorch_lightning.metrics.functional.classification removed to_onehot, to_categorical, get_num_classes, roc, multiclass_roc, average_precision, precision_recall_curve, multiclass_precision_recall_curve\\nfrom pytorch_lightning.metrics.functional.reduction removed reduce, class_reduce\\n\\n\\nRemoved deprecated ModelCheckpoint arguments prefix, mode=\"auto\" (#6162)\\nRemoved mode=\\'auto\\' from EarlyStopping (#6167)\\nRemoved epoch and step arguments from ModelCheckpoint.format_checkpoint_name(), these are now included in the metrics argument (#7344)\\nRemoved legacy references for magic keys in the Result object (#6016)\\nRemoved deprecated LightningModule hparams setter (#6207)\\nRemoved legacy code to log or include metrics in the progress bar by returning them in a dict with the \"log\"/\"progress_bar\" magic keys. Use self.log instead (#6734)\\nRemoved trainer.fit() return value of 1. It has no return now (#7237)\\nRemoved logger_connector legacy code (#6733)\\nRemoved unused mixin attributes (#6487)\\n\\nFixed\\n\\nFixed NaN errors in progress bars when training with iterable datasets with no length defined (#7306)\\nFixed attaching train and validation dataloaders when reload_dataloaders_every_epoch=True and num_sanity_val_steps=0 (#7207)\\nAdded a barrier in the accelerator teardown to synchronize processes before execution finishes (#6814)\\nFixed multi-node DDP sub-process launch by using local_rank instead of global_rank for main process assertion (#7061)\\nFixed incorrect removal of WORLD_SIZE environment variable in DDP training when launching with torch distributed/torchelastic (#6942)\\nMade the Plugin.reduce method more consistent across all Plugins to reflect a mean-reduction by default (#6011)\\nMove lightning module to correct device type when using LightningDistributedWrapper (#6070)\\nDo not print top-k verbose log with ModelCheckpoint(monitor=None) (#6109)\\nFixed ModelCheckpoint(save_top_k=0, save_last=True) not saving the last checkpoint (#6136)\\nFixed .teardown(stage=\\'fit\\') and .on_fit_{start,end}() getting called during trainer.test (#6386)\\nFixed LightningModule all_gather on cpu tensors (#6416)\\nFixed torch distributed not available in setup hook for DDP (#6506)\\nFixed trainer.tuner.{lr_find,scale_batch_size} not setting the Trainer state properly (#7258)\\nFixed bug where the learning rate schedulers did not follow the optimizer frequencies (#4868)\\nFixed pickle error checker to now check for pickle.PickleError to catch all pickle errors (#6917)\\nFixed a bug where the outputs object passed to LightningModule.training_epoch_end was different from the object passed to the on_train_end_epoch hook (#6969)\\nFixed a bug where the outputs passed to train_batch_end would be listed even when using a single optimizer and no truncated backprop through time steps (#6969)\\nFixed bug for trainer error handling which would cause hang for distributed training (#6864)\\nFixed self.device not returning the correct device in replicas of data-parallel (#6414)\\nFixed lr_find trying beyond num_training steps and suggesting a too high learning rate (#7076)\\nFixed logger creating incorrect version folder in DDP with repeated Trainer.fit calls (#7077)\\nFixed metric objects passed directly to self.log not being reset correctly (#7055)\\nFixed CombinedLoader in distributed settings for validation / testing (#7102)\\nFixed the save_dir in WandbLogger when the run was initiated externally (#7106)\\nFixed num_sanity_val_steps affecting reproducibility of training data shuffling (#7014)\\nFixed resetting device after fitting/evaluating/predicting (#7188)\\nFixed bug where trainer.tuner.scale_batch_size(max_trials=0) would not return the correct batch size result (#7262)\\nFixed metrics not being properly logged with precision=16 and manual_optimization (#7228)\\nFixed BaseFinetuning properly reloading optimizer_states when using resume_from_checkpoint (#6891)\\nFixed parameters_to_ignore not properly set to DDPWrapper (#7239)\\nFixed parsing of fast_dev_run=True with the built-in ArgumentParser (#7240)\\nFixed handling an IterableDataset that fails to produce a batch at the beginning of an epoch (#7294)\\nFixed LightningModule.save_hyperparameters() when attempting to save an empty container (#7268)\\nFixed apex not properly instantiated when running with ddp (#7274)\\nFixed optimizer state not moved to GPU (#7277)\\nFixed custom init args for WandbLogger (#6989)\\nFixed a bug where an error would be raised if the train dataloader sometimes produced None for a batch (#7342)\\nFixed examples (#6600, #6638, #7096, #7246, #6357, #6476, #6294, #6373, #6088, #7398)\\nResolved schedule step bug for PyTorch Profiler (#6674, #6681)\\nUpdated logic for checking TPUs availability (#6767)\\nResolve TPU miss rendezvous (#6781)\\nFixed auto-scaling mode when calling tune method on trainer (#7321)\\nFixed finetuning complex models correctly unfreezes (#6880)\\nEnsure we set the eval/train flag correctly on accelerator model (#6877)\\nSet better defaults for rank_zero_only.rank when training is launched with SLURM and torchelastic (#6802)\\nFixed matching the number of outputs of backward with forward for AllGatherGrad (#6625)\\nFixed the gradient_clip_algorithm has no effect (#6928)\\nFixed CUDA OOM detection and handling (#6934)\\nFixed unfreeze_and_add_param_group expects modules rather than module (#6822)\\nFixed DPP + SyncBN when move on device (#6838)\\nFixed missing arguments in lr_find call (#6784)\\nFixed set_default_tensor_type to torch.DoubleTensor with precision=64 (#7108)\\nFixed NeptuneLogger.log_text(step=None) (#7194)\\nFixed importing torchtext batch (#6365, #6323, #6211)\\n\\nContributors\\n@akihironitta, @alessiobonfiglio, @amisev, @amogkam, @ananthsub, @ArvinZhuang, @ashleve, @asnorkin, @awaelchli, @BloodAxe, @bmahlbrand, @Borda, @borisdayma, @camruta, @carmocca, @ceshine, @dbonner, @dhkim0225, @EdwardJB, @EliaCereda, @EricCousineau-TRI, @ethanwharris, @FlorianMF, @hemildesai, @ifsheldon, @kaushikb11, @mauvilsa, @maxfrei750, @mesejo, @ramonemiliani93, @rohitgr7, @s-rog, @sadiqj, @scart97, @SeanNaren, @shuyingsunshine21, @SkafteNicki, @SpontaneousDuck, @stllfe, @tchaton, @THasthika, @vballoli\\nIf we forgot someone due to not matching commit email with GitHub account, let us know :]\\nThis discussion was created from the release Lightning CLI, PyTorch Profiler, Improved Early Stopping.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU1MTUxcon': 'Hi, I\\'m currently trying to finetune a pretrained BERT model for intent classification using Huggingface\\'s Transformers library and Pytorch Lightning.\\nThe structure is simple where a linear classifier is simply put on the BERT encoder.\\nI want to get the same result at the same seed setting, but although the whole setting including the seed is identical, the result changes.\\nI thought if I pre-fix the seed using seed_everything and set the flag workers=True, I can get the exact same result, but I don\\'t know what the problem is.\\nThe fun fact is that all executions save the exact same best checkpoint with identical valid accuracy and actually the flow of the training seems also the same.\\nBut after executing the test, the results are not same.\\nThe main codes are as follows.\\nfrom transformers import BertConfig, BertTokenizer, BertModel\\nfrom pytorch_lightning import Trainer, seed_everything\\nfrom pytorch_lightning.callbacks import ModelCheckpoint\\n\\ndef run(args):\\n    # For directory setting\\n    ...\\n\\n    # Tokenizer & Model => This model is a pre-trained encoder, so I think there is no need to fix a random seed.\\n    config = BertConfig.from_pretrained(\\'bert-base-uncased\\')\\n    tokenizer = BertTokenizer.from_pretrained(\\'bert-base-uncased\\')\\n    model = BertModel.from_pretrained(\\'bert-base-uncased\\')\\n\\n    ...\\n    \\n    print(\"Loading datasets...\")\\n    # For data loading\\n    train_set = Dataset(...)\\n    valid_set = Dataset(...)\\n    test_set = Dataset(...)\\n\\n    total_train_steps = int(len(train_set) / batch_size * num_epochs)\\n    warmup_steps = int(total_train_steps * warmup_prop)\\n    \\n    # Random seed fixing for intent classification layer\\n    seed_everything(0, workers=True)\\n    \\n    # Lightning Module setting  \\n    module = TrainModule(model)\\n\\n    # Dataloaders\\n    ppd = PadCollate(...)\\n    \\n    # Reset random seed for data shuffle\\n    seed_everything(0, workers=True)\\n\\n    train_loader = DataLoader(train_set, collate_fn=ppd.pad_collate, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\\n    valid_loader = DataLoader(valid_set, collate_fn=ppd.pad_collate, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\\n    test_loader = DataLoader(test_set, collate_fn=ppd.pad_collate, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\\n\\n    print(\"Setting pytorch lightning callback & trainer...\")\\n    # Model checkpoint callback\\n    filename = \"best_ckpt_{epoch}_{train_all_acc:.4f}_{valid_all_acc:.4f}\"\\n    monitor = \"valid_all_acc\"\\n    \\n    checkpoint_callback = ModelCheckpoint(\\n        dirpath=save_dir,\\n        filename=filename,\\n        verbose=True,\\n        monitor=monitor,\\n        mode=\\'max\\',\\n        every_n_val_epochs=1,\\n    )\\n    \\n    # Trainer setting\\n    trainer = Trainer(\\n        check_val_every_n_epoch=1,\\n        gpus=gpu,\\n        auto_select_gpus=True,\\n        num_nodes=num_nodes,\\n        max_epochs=num_epochs,\\n        gradient_clip_val=max_grad_norm,\\n        num_sanity_val_steps=0,\\n        deterministic=True,\\n        callbacks=[checkpoint_callback]\\n    )\\n    \\n    print(\"Train starts.\")\\n    trainer.fit(model=module, train_dataloader=train_loader, val_dataloaders=valid_loader)\\n    print(\"Training done.\")\\n    \\n    print(\"Test starts.\")\\n    trainer.test(model=module, test_dataloaders=test_loader, ckpt_path=\\'best\\')\\n    \\n    print(\"GOOD BYE.\")\\nAlso, TrainModule is designed like below.\\nfrom torch import nn as nn\\nimport pytorch_lightning as pl\\n\\nclass TrainModule(pl.LightningModule):\\n    def __init__(self, args, encoder):\\n        super().__init__()\\n        \\n        self.args = args\\n        self.save_hyperparameters(args)\\n        \\n        self.encoder = encoder\\n        self.output_layer = IntentDetection(args)\\n        self.output_layer.init_params()\\n        \\n        self.loss_func = nn.CrossEntropyLoss()\\n        \\n    def forward(self, input_ids, padding_masks=None):  # input_ids: (B, L), padding_masks: (B, L)\\n        hidden_states = self.encoder(input_ids=input_ids, attention_mask=padding_masks)[0]  # (B, L, d_h)\\n            \\n        return self.output_layer(hidden_states[:, 0])  # (B, L, C) or  (B, C)\\n    \\n    def training_step(self, batch, batch_idx): \\n        ...\\n\\nclass IntentDetection(nn.Module):\\n    def __init__(self, args):\\n        super(IntentDetection, self).__init__()\\n        \\n        self.hidden_size = args.hidden_size\\n        self.num_classes = args.num_classes\\n        \\n        self.linear = nn.Linear(self.hidden_size, self.num_classes)\\n        \\n    def forward(self, hiddens):\\n        # hiddens: (B, d_h)\\n        \\n        return self.linear(hiddens)  # (B, C)\\n    \\n    def init_params(self):\\n        nn.init.xavier_uniform_(self.linear.weight)  \\nI also post the current training environment.\\n\\nOS: Ubuntu 18.04.5 LTS\\nPython version: 3.8.5\\nPytorch version: 1.7.1+cu110\\nPytorch Lightning version: 1.3.0\\nGPU: A100-SXM4-40GB (DGX)\\nCUDA version: 11.0\\n\\nAnd I got 3 different results when I run the same codes 3 times.\\n\\nThis is odd, since when I run the exact same code above in different environment, I could get the identical results from a same seed, even the number of workers in a dataloader is different.\\nI also attach the environment which I was able to get the perfect reproducibility.\\n\\nOS: CentOs Linux release 7.9.2009 (Core)\\nPython version: 3.7.4\\nPytorch version: 1.7.1+cu110\\nPytorch Lightning version: 1.3.0\\nGPU: RTX 3090\\nCUDA version: 11.2\\n\\nIt will be really great if anyone can help me to solve this issue...\\nThank you very much.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU1MjUzcon': 'Hi!\\nI\\'ve trained a model successfully. I now want to have a look at the model predictions. I\\'ve overridden the predict_dataloader method in my DataModule:\\ndef predict_dataloader(self):\\n        pred_loader = torch.utils.data.DataLoader(\\n            self.val_ds, batch_size=1, num_workers=4)\\n        return pred_loader\\n\\nThen, I initialize the model using my checkpoint and call the predict method:\\ncheckpoint_dir = os.path.join(root_dir, \"logs2/epoch=199-val_loss=0.26-val_dice=1.67.ckpt\")\\n\\n# initialize the data module \\ndata = KeriDataModule(data_dir=data_dir, pix_dim=(0.6, 0.6, 0.937))\\n\\n# initialize the LightningModule (from checkpoint)\\nnet = Net.load_from_checkpoint(checkpoint_path=checkpoint_dir)\\n\\n# initialize Lightning\\'s trainer\\ntrainer = pl.Trainer(gpus=[0])\\n\\nresults = trainer.predict(net, data)\\n\\nUnfortunately, I get the following error\\nPredicting: 0it [43:22, ?it/s]\\n\\nGPU available: True, used: True\\nTPU available: False, using: 0 TPU cores\\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\\n\\n\\nPredicting: 0it [00:00, ?it/s]\\n\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-12-4bd0b31315bd> in <module>\\n     10 trainer = pl.Trainer(gpus=[0])\\n     11 \\n---> 12 results = trainer.predict(net, data)\\n     13 print(results)\\n     14 \\n\\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in predict(self, model, dataloaders, datamodule, return_predictions)\\n    629         self.data_connector.attach_data(model, predict_dataloaders=dataloaders, datamodule=datamodule)\\n    630 \\n--> 631         results = self._run(model)\\n    632 \\n    633         assert self.state.stopped\\n\\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run(self, model)\\n    754 \\n    755         # dispatch `start_training` or `start_evaluating` or `start_predicting`\\n--> 756         self.dispatch()\\n    757 \\n    758         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\\n\\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\\n    793             self.accelerator.start_evaluating(self)\\n    794         elif self.predicting:\\n--> 795             self.accelerator.start_predicting(self)\\n    796         else:\\n    797             self.accelerator.start_training(self)\\n\\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py in start_predicting(self, trainer)\\n    100 \\n    101     def start_predicting(self, trainer: \\'pl.Trainer\\') -> None:\\n--> 102         self.training_type_plugin.start_predicting(trainer)\\n    103 \\n    104     def pre_dispatch(self, trainer: \\'pl.Trainer\\') -> None:\\n\\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_predicting(self, trainer)\\n    150     def start_predicting(self, trainer: \\'pl.Trainer\\') -> None:\\n    151         # double dispatch to initiate the predicting loop\\n--> 152         self._results = trainer.run_stage()\\n    153 \\n    154     def training_step(self, *args, **kwargs):\\n\\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_stage(self)\\n    804             return self.run_evaluate()\\n    805         if self.predicting:\\n--> 806             return self.run_predict()\\n    807         return self.run_train()\\n    808 \\n\\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_predict(self)\\n   1071             dataloader = self.accelerator.process_dataloader(dataloader)\\n   1072             dl_max_batches = self.predict_loop.max_batches[dataloader_idx]\\n-> 1073             for batch_idx, batch in enumerate(dataloader):\\n   1074                 if batch is None:\\n   1075                     continue\\n\\nTypeError: \\'KeriDataModule\\' object is not iterable\\n\\nI don\\'t understand what I messed up ^^\\'. Any help / tip would be greatly appreciated :)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU4MTIxcon': \"The CLI has a flag --gpus\\nIn a system with more than 1 GPU, is there a way to select the GPU you want to run on from CLI?\\nI tried --gpus [1] to select cuda:1 but it doesn't work.\\nAlso the auto gpu selection didn't work for me.  It tried to put the job on cuda:0, but cuda:0 didn't have enough memory to run it.\\nin the end I resorted to CUDA_VISIBLE_DEVICES, but that seems silly.\\nThanks!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU4NDI1con': 'I am using MLFlowLogger to log my experiment into Azure ML. Everything works fine but I noticed that when I ask the logger to store a metric every step (instead of every epoch), the logger does not increase the step number but instead keeps overwriting the current step in the batch. That is, if I have 5 minibatches for each epoch, and I have 10 epochs, the logger will overwrite step 1-5 continuously, instead of logging step 1-50. Something I noticed is that MLFlow/AzureML log metrics according to steps, regardless of whether it is a epoch or not, perhaps this is causing some issues.\\nWould you be able to help?\\nAn example here (7 steps only for train_loss but I ran more than 70 epochs)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU5MDcxcon': 'But I keep getting this error. I have no what it\\'s about. A hint will be really helpful. Thanks in advance.\\n\\nAttributeError                            Traceback (most recent call last)\\n in \\n----> 1 trainer.fit(LitPlant, train_dataloader=train_loader, val_dataloaders=valid_loader)\\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\\n447         # ----------------------------\\n448         # setup data, etc...\\n--> 449         self.train_loop.setup_fit(model, train_dataloader, val_dataloaders, datamodule)\\n450\\n451         # hook\\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py in setup_fit(self, model, train_dataloader, val_dataloaders, datamodule)\\n112         # clean hparams\\n113         if hasattr(model, \"hparams\"):\\n--> 114             parsing.clean_namespace(model.hparams)\\n115\\n116         # links data to the trainer\\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/parsing.py in clean_namespace(hparams)\\n73         hparams_dict = hparams.dict\\n74\\n---> 75     del_attrs = [k for k, v in hparams_dict.items() if not is_picklable(v)]\\n76\\n77     for k in del_attrs:\\nAttributeError: \\'property\\' object has no attribute \\'items\\'',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUwMzU1con': \"Hi there\\nPytorch docs recommends using channels last when training vision models in mixed precision. To enable, you need to do two changes:\\n\\nMove you model to channel last format: model = model.to(memory_format=torch.channels_last) # Replace with your model. This can be done in on_fit_start callback hook as model.to performs an in place modification:\\n\\nclass ChannelsLast(pl.Callback):\\n    def on_fit_start(self, trainer, pl_module: pl.LightningModule) -> None:\\n        # Inplace model modification\\n        pl_module.to(memory_format=torch.channels_last)\\n\\n\\nMove input data to channel last format before feeding to the model: input = input.to(memory_format=torch.channels_last).\\n\\nMy problem is in step 2. I don't find any PyTorch lightning hook that allows me to make this modification to the batch :/. The only options left are to add it as data transforms (that must be used in conjunction with the callback) or doing all channel last related logic inside the LightningModule. I would prefer to avoid this last solution as it could clutter the LightningModule with unnecessary code.\\nDo you know a to do step 2 in a callback?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUwODY2con': 'Hi\\nI need to compute some metrics that are not quick to compute (eg. Frechet Inception Distance).\\nIt is too expensive to compute them every validation epoch. Instead I would like to compute the metric once after every training epoch (or after some arbitrary number of steps). To do so, I need to be able to access the training dataset at the end of every training epoch, compute the metric and log it.\\nIt is not obvious how to do this, as I cannot access the training data-set during “on_epoch_end” or one of the other end of epoch hooks.\\nIs there a good solution for this?\\nThanks\\nInigo.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUxNDc0con': '🐛 Bug\\nI get an error The metric val_acc_0_step/epoch_0 does not contain a single element thus it cannot be converted to float. when running training loop.\\nFull stack trace\\n File \"main.py\", line 76, in <module>\\n    run_training()\\n  File \"main.py\", line 69, in run_training\\n    trainer.fit(model, dm)\\n  File \"/opt/conda/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py\", line 484, in safe_patch_function\\n    patch_function(call_original, *args, **kwargs)\\n  File \"/opt/conda/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py\", line 241, in patch_with_managed_run\\n    result = patch_function(original, *args, **kwargs)\\n  File \"/opt/conda/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py\", line 296, in fit\\n    return _run_and_log_function(self, original, args, kwargs)\\n  File \"/opt/conda/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py\", line 288, in _run_and_log_function\\n    result = original(self, *args, **kwargs)\\n  File \"/opt/conda/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py\", line 440, in call_original\\n    original_result = original(*og_args, **og_kwargs)\\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\\n    self.dispatch()\\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\\n    self.accelerator.start_training(self)\\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\\n    self.training_type_plugin.start_training(trainer)\\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\\n    self._results = trainer.run_train()\\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 637, in run_train\\n    self.train_loop.run_training_epoch()\\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 577, in run_training_epoch\\n    self.trainer.run_evaluation(on_epoch=True)\\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 732, in run_evaluation\\n    self.evaluation_loop.log_evaluation_step_metrics(output, batch_idx)\\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 336, in log_evaluation_step_metrics\\n    self.__log_result_step_metrics(step_log_metrics, step_pbar_metrics, batch_idx)\\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 351, in __log_result_step_metrics\\n    self.trainer.logger_connector.log_metrics(metrics_by_epoch, {}, step=batch_idx)\\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py\", line 222, in log_metrics\\n    scalar_metrics = self.trainer.metrics_to_scalars(metrics)\\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/logging.py\", line 51, in metrics_to_scalars\\n    f\"The metric `{k}` does not contain a single element\"\\npytorch_lightning.utilities.exceptions.MisconfigurationException: The metric `val_acc_0_step/epoch_0` does not contain a single element thus it cannot be converted to float. Found `tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n        0.])`\\n\\nTo Reproduce\\nI train the model below, which is multi-class / multi-dimensional classifier (have multiple class categories trained in one hierarchical model). The error seem to come from the following piece (running in loop is so that I can specify number of classes, which is different for different dimensions):\\nfor i in range(len(y_true)):\\n            self.valid_acc[i](preds[i], y_true[i])\\n            self.log(f\\'val_acc_{i}\\', self.valid_acc[i], on_step=True, on_epoch=True)    \\n\\nI was getting the same error when trying to save confusion matrix.\\nCode sample\\n\\nclass OntologyTaggerModel(pl.LightningModule):\\n    def __init__(self,\\n                 num_classes,\\n                 model_name=\\'bert-base-cased\\',\\n                 learning_rate=3e-6,\\n                 **kwargs):\\n\\n        super().__init__()\\n        self.save_hyperparameters()\\n        self.learning_rate = learning_rate\\n        self.model = BertForMulticlassSequenceClassification.from_pretrained(model_name, num_classes=num_classes)\\n        self.valid_acc = nn.ModuleList([torchmetrics.Accuracy(average=None, num_classes=num_class) for num_class in torch.tensor(num_classes)])\\n        self.valid_f1 = torchmetrics.F1(multiclass=True, mdmc_average=\\'samplewise\\')\\n        self.cm = nn.ModuleList([torchmetrics.ConfusionMatrix(num_classes=num_class) for num_class in torch.tensor(num_classes)])\\n        \\n\\n    def forward(self, *input, **kwargs):\\n        return self.model(*input, **kwargs)\\n\\n    def training_step(self, batch, batch_idx):\\n        x, y_true = batch\\n        loss, _ = self(x, labels=y_true)\\n        return loss\\n\\n    def validation_step(self, batch, batch_idx):\\n        x, y_true = batch\\n        _, y_preds = self(x, labels=y_true)\\n        preds = [torch.argmax(y_pred, axis=1) for y_pred in y_preds]\\n        for i in range(len(y_true)):\\n            self.valid_acc[i](preds[i], y_true[i])\\n            self.log(f\\'val_acc_{i}\\', self.valid_acc[i], on_step=True, on_epoch=True)        \\n        self.valid_f1(torch.stack(preds), torch.stack(y_true))\\n        self.log(\\'f1\\', self.valid_f1, on_step=True, on_epoch=True)\\n\\n    def configure_optimizers(self):\\n        \\'Prepare optimizer and schedule (linear warmup and decay)\\'\\n        return torch.optim.Adam(params=self.parameters(), lr=self.learning_rate)\\n\\n    def training_epoch_end(self, training_step_outputs):\\n        avg_loss = torch.tensor([x[\\'loss\\']\\n                                 for x in training_step_outputs]).mean()\\n        self.log(\\'train_loss\\', avg_loss)\\n\\n        print(f\\'###score: train_loss### {avg_loss}\\')\\n\\n    def validation_epoch_end(self, val_step_outputs):\\n        for i in range(len(self.valid_acc)):\\n            acc = self.valid_acc[i].compute()\\n            self.log(f\\'val_score_{i}\\', acc)\\n        f1 = self.valid_f1.compute()\\n        self.log(\\'f1\\', f1)\\n        print(f\\'###score: val_score### {acc}\\')\\n\\n\\nExpected behavior\\nMetrics should be rendered irrespective of dimension\\nEnvironment\\npytorch-lightning==1.2.7\\ndatasets==1.4.1\\nmlflow==1.16.0\\ntorchmetrics=0.3.1\\ntorch=1.7.1\\npython=3.6',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUxNDcycon': \"I'm looking to train on chunks of my entire dataset at a time per epoch (preferably over every n epochs, but this is not yet implemented officially), since the size of my dataset exceeds my total RAM. I'd therefore like to update the data in the DataLoaders every epoch. From all the examples I've seen, the actual data content is saved in memory a class attribute (in the following code snippet, it's saved in self.mnist_train and self.mnist_val). It seems that train_dataloader() and val_dataloader() only read self.mnist_train and self.mnist_val into DataLoaders.\\nFrom my understanding, reload_dataloaders_every_epoch=True calls train_dataloader() and val_dataloader() at every epoch, but I don't see the point of doing this if self.mnist_test and self.mnist_train aren't actually being changed. How do I make train_dataloader() and test_dataloader() return new data every epoch?\\nclass MyDataModule(pl.LightningDataModule):\\n\\n    def __init__(\\n        self,\\n        batch_size: int = 32,\\n    ):\\n        super().__init__()\\n        dataset = MNIST(_DATASETS_PATH, train=True, download=True, transform=transforms.ToTensor())\\n        self.mnist_test = MNIST(_DATASETS_PATH, train=False, download=True, transform=transforms.ToTensor())\\n        self.mnist_train, self.mnist_val = random_split(dataset, [55000, 5000])\\n        self.batch_size = batch_size\\n\\n    def train_dataloader(self):\\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\\n\\n    def val_dataloader(self):\\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\\n\\n    def test_dataloader(self):\\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUxNTAwcon': \"Hello!\\nI'm working on an inference engine where I don't have any data before I start training. The idea is that at each epoch, a new data point is generated by a simulator and that data is fed to the NN only at that epoch.\\nI'm wondering if there is a way to do this with pytorch-lightning. I looked at the IterableDataset class but the only thing I came up with is to generate the data beforehand and then iterate over it.\\nThanks!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUxNTEzcon': \"Hi\\nI'm trying to calculate some metrics and generate some images to save at the end of each epoch.\\nI put this code in the on_train_epoch_end() (I also tried using a custom callback) but the function seems to be called in the middle of the epochs, approximately 3-4 times per epoch.\\nSurely this isn't intended behaviour? Could it be to do with me using a combined loader?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzY0MTE1con': 'When I use an IterableDataset, the Trainer.fit() gives something like:\\nEpoch 0: : 21it [02:41,  7.69s/it, loss=0.663, v_num=18]  \\n\\nWhich does not display a progress. Even if I specify a validation interval, there is no progress shown. Is there a way I can specify an \"epoch length\", or something like that for IterableDataset?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzY1MjM1con': 'Hi.\\nIt seems like validation_step runs simultaneously at the end of training.\\nAs soon as validation_step start, the percentage of gpu memory allocated is shooting up and RuntimeError: CUDA out of memory occur.\\nHow can i fix it?\\nThanks in advance!',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzY1OTU1con': \"I want to marry lightning and https://pytorch-geometric.readthedocs.io/en/latest/ or in particular https://pytorch-geometric-temporal.readthedocs.io/en/latest/\\nWhen following the basic examples on their website such as for the ChickenpoxDatasetLoader() a RecurrentGCN is constructed. For me being a total newbie for lightning it is already pretty clear how t convert that to a regular lightning module - kudos to the easy API so far.\\nHowever, it is rather unclear for me how to put the training loop into a lightning compatible trainer:\\nfrom tqdm import tqdm\\nmodel = RecurrentGCN(node_features = 4) # chickenpox\\nmodel\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\\nmodel.train()\\nfor epoch in tqdm(range(200)):\\n    cost = 0\\n    for time, snapshot in enumerate(train_dataset):\\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\\n    cost = cost / (time+1)\\n    cost.backward()\\n    optimizer.step()\\n    optimizer.zero_grad() \\nWould I need a custom trainer in lightning?\\nIn particular, I need to be able to handle temporal graphs with snapshots over time i.e. think of a social network of people who can perform a hobby at a certain location (lat, long ) and timestamp. I guess it would be fine to discretize the time to i.e. weekly or monthly slices, but the graph is dynamic.\\nimport pandas as pd\\ndf = pd.DataFrame({'person_1':[], 'person_2':[], 'time':[], 'lat':[], 'long':[], 'hobby':[]})\\ndisplay(df)\\nI want to perform link prediction - i.e. recommend new friends based on similar hobbies in similar locations & time ranges.\\nWith that being said: in the pytorch-geometric-temporal framework they denote snapshots over time (this is not meant as batches, currently they assume that a snapshot contains all the data in a single batch for that particular span of time).\\nHowever, the default trainer does not offer this functionality to iterate over snapshots and to me it is unclear how to include it.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzY2NjQzcon': 'Hey,\\nIm wondering why the patience parameter has been removed from the Trainer class.\\nIm pretty sure that there once was this parameter and I used it a lot.\\nHas it just been moved to another class, or is there a workaround such that I can still use patience when training my Model?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzY4OTgycon': 'I wanted to set shuffle to False. So, i tried\\nsampler = torch.utils.data.distributed.DistributedSampler(dataset, shuffle=False)\\ndataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\\n\\nI am getting an error\\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\\n\\nPlease anyone tell me how to use custom sampler.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzYwODg1con': 'In https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/predict_loop.py#L111:\\n        predictions = self.trainer.accelerator.predict_step(args)\\n\\n        if predictions is None:\\n            self.warning_cache.warn(\"predict returned None if it was on purpose, ignore this warning...\")\\n\\n        self.trainer.call_hook(\"on_predict_batch_end\", predictions, batch, batch_idx, dataloader_idx)\\n\\n        if self.should_store_predictions:\\n            self.predictions[dataloader_idx].append(predictions)\\nShouldn\\'t .cpu() be called on predictions before appending them to self.predictions so that predictions accumulate con CPU memory, not GPU memory?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzYzNDA2con': 'Howdy! 🤠 I came across something in the PytorchVideo repository that I wasn\\'t so sure about. They\\'re using 2 accuracy metric classes in their LightningModule for train and val. I\\'ve been using 1 for everything and its making me wonder if I was wrong for some reason?\\nHaven\\'t dove into that code in a while...are the metrics being accumulated separately across the different loops, or are they accumulated together?\\nHere\\'s the snippet in question...\\n class VideoClassificationLightningModule(pytorch_lightning.LightningModule): \\n     def __init__(self, args): \\n         \"\"\" \\n         This LightningModule implementation constructs a PyTorchVideo ResNet, \\n         defines the train and val loss to be trained with (cross_entropy), and \\n         configures the optimizer. \\n         \"\"\" \\n         self.args = args \\n         super().__init__() \\n         self.train_accuracy = pytorch_lightning.metrics.Accuracy() \\n         self.val_accuracy = pytorch_lightning.metrics.Accuracy() \\nI would normally just have self.accuracy and use that in both the train and val steps. Is that incorrect?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzc0OTYxcon': \"Hi!\\nWhen I use no multi-gpu settings and just a single GPU and the following parameters:\\nbatch_size = 16\\naccumulate_grad_batches=2\\nmy effective batch size is 32. My question is how accumulate_grad_batches and DDP interact.\\nIf I am using 2 GPUS that are on the same machine and i use the parameters\\nbatch_size = 16\\naccumulate_grad_batches=2\\naccelerator='ddp'\\ngpus=[0,1]\\n\\nis my effective batch size now 64?\\nThanks for the help!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzc4OTQzcon': \"Hi all, I use multi-GPU to train the model, this error happens when saving the checkpoint. But using a single GPU to train the model, everything works fine.\\nTypeError: cannot pickle '_thread.lock' object\\n\\nI wonder if it is because I nest the trainer inside another model, the codes for a nested trainer is as followed:\\nself.model.trainer = self.trainer\\n\\nSo many thanks in advance.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzcwNzMwcon': 'Hi,\\nI was wondering if algorithms implemented with pytorch lightning can take advantage of deep learning boost  hardware:\\nhttps://www.intel.com/content/www/us/en/artificial-intelligence/deep-learning-boost.html\\nhttps://github.com/oneapi-src/oneDNN\\nAs far as I know it should be compatible with vanilla pytorch',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzcwOTA4con': \"I really liked @auto_move_data as it gave me a nice and easy way to pass (a single) input to a model, without caring about moving data structures to devices, but now it will be removed 😢\\nI realize that you want people to use trainer.predict() but this requires a dataloader, right?, which at least for me oftentimes is overly complicated while developing.\\nAs far as I see the dataloader/collate/transforms also have to be different from the normal ones, since these typically provide batches with inputs and targets, whereas the forward function only takes inputs.\\nLikely I'm missing something, can you maybe give some hints about this?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzcxOTM0con': 'I’m currently attempting to make a Multi-GPU-supported CLIP training script, but am hitting a wall. I need to compute two matrices that are composed of whole batch statistics before I can compute loss. Namely, I need to compute the image and text embeddings of an entire batch. Only then can I compute the sub batch losses.\\nHow can I first calculate and share the whole batch matrices across GPUs before computing losses?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzczODE0con': \"It seems that if I don't pass in an optimizer to my manual backward, my gradient clipping passes through a None object. I thought passing an optimizer to manual backward was not required. It turns out, passing an optimizer doesn't solve the bug.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzg0MDEzcon': 'After .fit() concludes, I run first .validate(ckpt_path=None) and then .test(ckpt_path=None). I use the exact same code for both of them: validation_step is identical to test_step, validation_epoch_end matches test_epoch_end, and val_dataloader is identical to test_dataloader. My trainer also has limit_val_batches=1.0, running the whole dataset. Yet, the two values I get are dramatically different: test error is far lower, typically up to an order of magnitude. What is going on in the test phase that could cause this?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzg1NzEwcon': 'I\\'m using the below callback\\n`checkpoint_callback = pl.callbacks.ModelCheckpoint(\\ndirpath=other_arguments.output_dir,\\nmonitor=\"val_loss\",\\nsave_top_k=other_arguments.save_top_k,\\nsave_last=other_arguments.save_last,\\nmode=\\'min\\'\\n)\\ntrain_params = dict(\\n    accumulate_grad_batches=other_arguments.gradient_accumulation_steps,\\n    gpus=training_arguments.n_gpu,\\n    deterministic=True,\\n    max_epochs=other_arguments.num_train_epochs,\\n    precision=16 if training_arguments.fp_16 else 32,\\n    amp_level=training_arguments.opt_level,\\n    gradient_clip_val=training_arguments.max_grad_norm,\\n    checkpoint_callback=checkpoint_callback,\\n    fast_dev_run=other_arguments.do_fast_dev_run,\\n)`\\n\\nThe output dir is empty which means the checkpoints are not getting saved. There is no error as well',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzg3NTAxcon': \"Hi, can you please tell me if I am doing something wrong here, especially the manual update for the generator and the critic:\\nclass Generator(nn.Module):\\n    def __init__(self, latent_dim=64, img_shape=None):\\n        super().__init__()\\n        self.img_shape = img_shape\\n        self.init_size = 8 #self.img_shape[1] // 4\\n\\n        self.l1 = nn.Sequential(\\n            nn.Linear(latent_dim, 64*self.init_size**2), nn.LeakyReLU(0.2, inplace=True))\\n        self.conv_blocks = nn.Sequential(\\n            nn.BatchNorm2d(64),\\n            nn.Upsample(scale_factor=2),\\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0),\\n            nn.BatchNorm2d(64),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Upsample(scale_factor=2),\\n            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=0),\\n            nn.BatchNorm2d(32),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Upsample(scale_factor=2),\\n            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=0),\\n            nn.BatchNorm2d(16),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Upsample(scale_factor=2),\\n            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, padding=1),\\n            nn.BatchNorm2d(8),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(in_channels=8, out_channels=img_shape[0], kernel_size=3, padding=1),\\n            nn.Tanh()\\n        )\\n    \\n    def forward(self, z):\\n        out = self.l1(z)\\n        out = out.view(out.shape[0], 64, self.init_size, self.init_size)\\n        img = self.conv_blocks(out)\\n        return img\\n\\nclass Critic(nn.Module):\\n    def __init__(self, img_shape):\\n        super().__init__()\\n        self.disc = nn.Sequential(\\n            nn.Conv2d(in_channels=img_shape[0], out_channels=16, kernel_size=4, stride=2),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(16, 32, kernel_size=4, stride=2),\\n            nn.BatchNorm2d(32),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\\n            nn.BatchNorm2d(64),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(64, 128, kernel_size=4, stride=2),\\n            nn.BatchNorm2d(128),\\n            nn.LeakyReLU(0.2, inplace=True),\\n        )\\n\\n        # The height and width of downsampled image\\n        #\\n        ds_size = 2 ** 4\\n\\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size, 1))\\n    def forward(self, img):\\n        out = self.disc(img)\\n        # import pdb; pdb.set_trace()\\n        out = out.view(out.shape[0], -1)\\n        validity = self.adv_layer(out)\\n        return validity       \\n\\nclass WGANGP(pl.LightningModule):\\n    def __init__(self, latent_dim=128, lr=0.0002, lambda_pen=10, crit_repeats=5):\\n        super().__init__()\\n        self.save_hyperparameters()\\n        self.latent_dim = latent_dim\\n        self.lr = lr\\n        self.lambda_pen = lambda_pen\\n        self.crit_repeats = crit_repeats\\n        self.b1 = 0.0\\n        self.b2 = 0.9\\n\\n        ### initializing networks\\n        img_shape = (1, 100, 100)\\n        self.generator = Generator(self.latent_dim, img_shape)\\n        self.critic = Critic(img_shape)\\n\\n        # application of weight\\n        self.generator.apply(self.weights_init)\\n        self.critic.apply(self.weights_init)\\n        #\\n        self.validation_z = torch.randn(10, self.latent_dim)\\n        self.example_input_array = torch.zeros(10, self.latent_dim)\\n\\n        # Important: This property activates manual optimization.\\n        self.automatic_optimization = False # True - Auto // # False - Manual update\\n\\n    def forward(self, z):\\n        return self.generator(z)\\n\\n    ### weight initialization\\n    def weights_init(self, m):\\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\\n            torch.nn.init.normal_(m.weight, 0.0, 0.02)\\n        if isinstance(m, nn.BatchNorm2d):\\n            torch.nn.init.normal_(m.weight, 0.0, 0.02)\\n            torch.nn.init.constant_(m.bias, 0)\\n        if isinstance(m, nn.Linear):\\n            torch.nn.init.normal_(m.weight, 0.0, 0.02)\\n            torch.nn.init.constant_(m.bias, 0)\\n        \\n    def training_step(self, batch, batch_idx):\\n        imgs = batch\\n\\n        # # sample noise\\n        # z = torch.randn(imgs.shape[0], self.latent_dim)\\n        # z = z.type_as(imgs)\\n\\n        # optimizers, manual access\\n        g_opt, c_opt = self.optimizers()\\n\\n        # update critic\\n        mean_iteration_critic_loss = 0\\n        for _ in range(self.crit_repeats):\\n            c_opt.zero_grad()\\n            # sample noise\\n            z = torch.randn(imgs.shape[0], self.latent_dim).type_as(imgs)\\n            # fake image\\n            fake = self(z)\\n            crit_fake_pred = self.critic(fake.detach())\\n            crit_real_pred = self.critic(imgs)\\n            # eps\\n            epsilon = torch.rand(len(imgs), 1, 1, 1, device=self.device, requires_grad=True)\\n            # gradient penalty\\n            gp = self.gradient_penalty(self.critic, imgs, fake, epsilon)\\n\\n            # critic loss\\n            critic_loss = torch.mean(crit_fake_pred) - torch.mean(crit_real_pred) + self.lambda_pen * gp\\n\\n            # Keep track of the average critic loss in this batch\\n            mean_iteration_critic_loss += critic_loss.item() / crit_repeats\\n\\n            # Update gradients\\n            self.manual_backward(critic_loss)\\n            # Update optimizer\\n            c_opt.step()\\n\\n        # log critic average loss\\n        self.log('c_loss_mean', mean_iteration_critic_loss, prog_bar=True)\\n        \\n        # update generator\\n        g_opt.zero_grad()\\n        # sample new noise\\n        z_new = torch.randn(imgs.shape[0], self.latent_dim).type_as(imgs)\\n        # new fake image\\n        fake_new = self(z_new)\\n        crit_fake_pred = self.critic(fake_new)\\n        # generator loss\\n        gen_loss = -torch.mean(crit_fake_pred)\\n\\n        # Update gradients\\n        self.manual_backward(gen_loss)\\n        # Update optimizer\\n        g_opt.step()\\n\\n        # log generator average loss\\n        self.log('g_loss', gen_loss, prog_bar=True)\\n\\n    def gradient_penalty(self, crit, real, fake, epsilon):\\n        # mix/interpolate images\\n        mixed_images = real * epsilon + fake * (1 - epsilon)\\n\\n        # Calculate the critic's scores on the mixed images\\n        mixed_scores = crit(mixed_images)\\n\\n        # Take the gradient of the scores with respect to the images\\n        gradient = torch.autograd.grad(\\n            inputs=mixed_images,\\n            outputs=mixed_scores,\\n            grad_outputs=torch.ones_like(mixed_scores),\\n            create_graph=True,\\n            retain_graph=True,\\n        )[0]\\n\\n        # Flatten the gradients so that each row captures one image\\n        gradient = gradient.view(len(gradient), -1)\\n\\n        # Calculate the magnitude of every row\\n        gradient_norm = gradient.norm(2, dim=1)\\n\\n        # Penalize the mean squared distance of the gradient norms from 1\\n        gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\\n\\n        return gradient_penalty\\n    \\n    def configure_optimizers(self):\\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.b1, self.b2))\\n        opt_c = torch.optim.Adam(self.critic.parameters(), lr=self.lr, betas=(self.b1, self.b2))\\n        return opt_g, opt_c\\n\\n    def on_epoch_end(self):\\n        z = self.validation_z.to(self.device)\\n        # log sampled images\\n        sample_imgs = self(z)\\n        grid = torchvision.utils.make_grid(sample_imgs)\\n        self.logger.experiment.add_image('generated_images', grid, self.current_epoch)\\n\\n# defining the hyperparameters\\nn_epochs = 1000\\nz_dim = 50\\nbatch_size = 64\\nlr = 0.0002\\nc_lambda = 10\\ncrit_repeats = 5\\ndesired output\\ncurrent output\\n\\nSOLVED\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzg5NjMycon': \"I'm trying to run code with PL+Deepspeed. I set my scheduler to be WarmupDecayLR, but get a warning RuntimeWarning: You are using LearningRateMonitor callback with models that have no learning rate schedulers. Please see documentation for configure_optimizers method.\\nIn the CometML logging I can't see learning rate logs. Is this the expected behaviour?\\nThank you! :)\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzgwMjMwcon': 'In my code, I would like to synchronize a tensor across all the gpus in train_step, which is a temporary variable. Is it allowed to call torch.distributed.all_reduce in this case? Or there is a specific function in pytorch_lightning that does the job?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzgyODA2con': \"By default, in Lightning everything that is returned by a dataset is collated by the data loader and shipped to the same device.\\nHowever, I am frequently in the situation where I have let's say x, y which are tensors and something like y_semantic which is in principle related to y but of higher data type, say a dictionary with some meta information about augmentations or so.\\nI don't need 'y_semantic' to be on the GPU. Is there some flag or some method that I can overwrite so that some variables stay on CPU?\\nSomething like\\ndef ship_batch(self, batch):\\n    batch[0] = batch[0].to(self.device)\\n    # ...\\n    batch[2] = batch[2].cpu() # just for illustration here\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzgzNzgxcon': \"Hello,\\nI'm designing a multi-task architecture for depth estimation and semantic segmentation. Because these are very similar tasks, I can use my existing nn.Module to learn both of them. I wrapped the nn.Module in a pl.LightningModule and got a working segmentation. Now I want to extend the LightningModule to be able to do the depth estimation.\\nI want to reuse as much code as possible to reduce errors. But the validation for both tasks is very different, because I'm using many metrics and extensive W&B logging. I first thought of subclassing the pl.LightningModule to a segmentation and a depth estimation module, but I want to be able to use both segmentation and depth in a single LightningModule later on, when the network should learn both tasks.\\nThus I created methods for the specific validation steps and set them to variables in the classes init. These get called from the overwritten methods of the class.\\nHeres some pseudo-code from my structure:\\nclass LitModel(pl.LightningModel):\\n def __init__(self, model: nn.Module,  hparams):\\n        super().__init__()\\n        # default for segmentation and depth\\n        self.hparams.update(hparams)\\n        self.model = model\\n\\n        # SEGMENTATION STUFF\\n        if segmentation:\\n            \\n            self._batch_target_name = 'label'\\n\\n            # VAL STUFF\\n            self._loss_func = CrossEntropyLoss\\n            self._val_loss_func = CrossEntropyLoss\\n            self._confusion_matrix = ConfusionMatrix(num_classes=num_classes, compute_on_step=False)\\n            self._best_metric = 0\\n            self._best_metric_name = 'mIoU'\\n\\n            self._val_reset_metric_list = [self._val_loss_func, self._confusion_matrix]\\n\\n            self._validation_step = self.seg_validation_step\\n            self._validation_epoch_end = self.seg_validation_epoch_end\\n\\n        # DEPTH STUFF\\n        elif depth:\\n            \\n            self._batch_target_name = 'depth'\\n\\n            # VAL STUFF\\n            self._loss_func = L1\\n            self._val_loss_func = MSE\\n           \\n            self._best_metric = 0\\n            self._best_metric_name = 'RMSE'\\n\\n            self._val_reset_metric_list = [self._val_loss_func]\\n\\n            self._validation_step = self.dpt_validation_step\\n            self._validation_epoch_end = self.dpt_validation_epoch_end\\n\\n    def forward(self, sample):\\n        return self.model(sample)\\n\\n    def training_step(self, batch, batch_idx):\\n        # disassemble batch/samples\\n        image = batch['image']\\n        target_scales = [batch[self._batch_target_name]]\\n\\n        # predict input images\\n        pred_scales = self.model(image)\\n\\n        # calculate losses\\n        losses = self._loss_func(pred_scales, target_scales)\\n        summed_loss = sum(losses)\\n        self.log('train/loss', summed_loss, on_step=False, on_epoch=True)  # logs mean of all losses during that epoch\\n        return {'loss': summed_loss}\\n\\n    def validation_step(self, batch, batch_idx):\\n        # disassemble batch/samples\\n        image = batch['image']\\n        gt = batch[self._batch_target_name]\\n\\n        # predict input images\\n        prediction = self.model(image)\\n\\n        return self._validation_step(batch, batch_idx, gt, prediction)\\n\\n    def seg_validation_step(self, batch, batch_idx, gt, prediction):\\n        # calculate segmentation validation losses\\n\\n    def dpt_validation_step(self, batch, batch_idx, gt, prediction):...\\n        # calculate depth validation losses\\n\\n    def on_validation_epoch_start(self) -> None:\\n        for f in self._val_reset_metric_list:\\n            f.reset()\\n\\n    def validation_epoch_end(self, outputs) -> None:\\n        metric = self._validation_epoch_end(outputs)\\n        if self._best_metric < metric:\\n            self.log(f'eval/best_{self._best_metric_name}', metric)\\n            self.log('eval/best_epoch', self.current_epoch)\\n            self._best_metric = metric\\n\\n    def seg_validation_epoch_end(self, outputs) -> metric:\\n        # calculate confusion matrix and mIoU\\n        return mIoU\\n\\n    def dpt_validation_epoch_end(self, outputs) -> metric:\\n        # calculate RMSE\\n        return RMSE\\n\\n    def configure_optimizers(self):...\\nTo me, this seems as really bad practice and I hope it doesn't hurt you to much seeing this.\\nWhat would be a better/the best way to achieve this? I'm looking for the best practice.\\nI'm coming from TensorFlow and I really really like how clean PyTorch Lightning is. It feels so good to work with it. But what I wrote up there just feels wrong, but it works 😨\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk1MDQ3con': 'Systems\\nThe style guide encourages to use systems, like this one\\nclass LitModel(LightningModule):\\n    def __init__(self, encoder: nn.Module = None, decoder: nn.Module = None):\\n        super().__init__()\\n        self.encoder = encoder\\n        self.decoder = decoder\\nI have problems loading the checkpoint for such modules.\\nBelow example fails. How can I make it work?\\nMinimal example\\nimport os\\n\\nimport torch\\nfrom torch import nn\\nfrom torch.utils.data import Dataset\\nfrom pytorch_lightning import Trainer, LightningModule\\n\\n\\nclass RandomDataset(Dataset):\\n    def __init__(self, size, length):\\n        self.len = length\\n        self.data = torch.randn(length, size)\\n\\n    def __getitem__(self, index):\\n        return self.data[index]\\n\\n    def __len__(self):\\n        return self.len\\n\\n\\nclass SystemModel(LightningModule):\\n\\n    def __init__(self, encoder: nn.Module = None, decoder: nn.Module = None, multiplier=10):\\n        super().__init__()\\n        self.save_hyperparameters(\\'multiplier\\')\\n        self.encoder = encoder\\n        self.decoder = decoder\\n        self.multiplier = multiplier\\n        print(\"type of hparams\", type(self.hparams))\\n        print(\"class of hparams type\", self.hparams.__class__.__name__)\\n\\n    def forward(self, x):\\n        return self.multiplier * self.decoder(self.encoder(x))\\n\\n    def loss(self, batch, prediction):\\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\\n\\n    def step(self, x):\\n        x = self.forward(x)\\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\\n        return out\\n\\n    def training_step(self, batch, batch_idx):\\n        output = self.forward(batch)\\n        loss = self.loss(batch, output)\\n        return {\"loss\": loss}\\n\\n    def training_step_end(self, training_step_outputs):\\n        return training_step_outputs\\n\\n    def training_epoch_end(self, outputs) -> None:\\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\\n\\n    def validation_step(self, batch, batch_idx):\\n        output = self.forward(batch)\\n        loss = self.loss(batch, output)\\n        return {\"x\": loss}\\n\\n    def validation_epoch_end(self, outputs) -> None:\\n        torch.stack([x[\\'x\\'] for x in outputs]).mean()\\n\\n    def test_step(self, batch, batch_idx):\\n        output = self.forward(batch)\\n        loss = self.loss(batch, output)\\n        return {\"y\": loss}\\n\\n    def test_epoch_end(self, outputs) -> None:\\n        torch.stack([x[\"y\"] for x in outputs]).mean()\\n\\n    def configure_optimizers(self):\\n        optimizer = torch.optim.SGD(self.parameters(), lr=0.1)\\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\\n        return [optimizer], [lr_scheduler]\\n\\n\\ntrain_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\\nval_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\\ntest_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\\n\\n# model\\nmodel = SystemModel(torch.nn.Linear(32, 16), torch.nn.Linear(16, 2), multiplier=15)\\ntrainer = Trainer(\\n    default_root_dir=os.getcwd(),\\n    limit_train_batches=1,\\n    limit_val_batches=1,\\n    max_epochs=1,\\n    weights_summary=None,\\n)\\ntrainer.fit(model, train_data, val_data)\\n\\nckpt_path = trainer.checkpoint_callback.best_model_path\\n\\n# Try to load\\n# Loading fails....\\nmodel = SystemModel.load_from_checkpoint(ckpt_path) # Fails\\n\\n# Edit: answer by Adrian\\n# the correct way to load is \\nmodel = SystemModel.load_from_checkpoint(ckpt_path, encoder=torch.nn.Linear(32, 16), decoder=torch.nn.Linear(16, 2))  \\n\\nprint(\"multiplier:\", model.multiplier)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk1MjAwcon': \"When I execute some program in PyTorch-Lightning, it implements very fast. But, at the last part, I got a message as below\\n/home/mydirectory/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: cleaning up ddp environment...\\n\\nwarnings.warn(*args, **kwargs)\\n\\nAfter this message, all the system stops. I can't see the result that I want stopped by this message.\\nCan anyone tell me how to solve it? I'm in Hurry ㅠ.ㅠ\\n(I am using it with wandb & HuggingFace.)\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk2MjAycon': \"I used torchmetrics.AveragePrecision as the metric in PL module. This metric takes a lot of gpu memory as it save all data in buffer.\\nSo I wanted to move the metric to cpu and set move_metrics_to_cpu True in PL Trainer. But the metric's buffer was still on gpu.\\nAny ideas of this?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk4NjAwcon': 'I have 2 losses for my model.\\nAnd I need the grads of the first loss to compute the second one.\\nThe pseudocode in pytorch is like:\\noptimizer.zero_grad()\\nhidden_value = model.part1(input)\\noutput = model.part2(hidden_value)\\nloss1 = criterion(output, label)\\nloss1.backward(retain_graph=True)\\nloss2 = criterion2(hidden_value.grad, label2)\\nloss2.backward()\\noptimizer.step()\\nI found an API named manual_backward() which may fit my problem.\\nHowever, I build this model on a project based on pytorch_lighting 0.6.0, and it doesn’t have this API.\\nSo, my questions are:\\n1.How can I implement my operation using pytorch_lightning 0.6.0?\\n2.If I can’t implement it in pytorch_lightning 0.6.0, which lighting version should I chose? (Please recommend a close version which may cause less error after I update the lightning.)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk5MTEwcon': 'Hey,\\nim wondering whether the patience parameter is or can be reset once we have started improving again. Reading the docs it sounds like the patience parameter is the absolute number of steps the loss is allowed to not decrease. Is it possible to have it such that the patience counter is reset to its original value whenever we have improved?',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk5NDIycon': 'I train a model with 2 GPUs， when running predictions = trainer.predict(model, datamodule) in a script, what I get is not a single variable predictions as desired, instead, I get two independent variables on two GPUs. I tried to use on_predict_end and on_predict_batch_end to aggregate the predictions on different GPUs, but it seems they change the behavior of the method trainer.predict， i.e. I can\\'t get an aggregated predicitons.  What should I do to aggregate predictions defined in predict_step when using multiple GPUs?\\n\\nTo see the issue, simple create a LightningModule with the predict_step method\\nclass Model(pl.LightningModule):\\n    ...\\n    def predict_step(self, batch, batch_idx, dataloader_idx):\\n        y = self(x)\\n        return {\"predict\":y}\\n \\nm = Modle(...)\\nand use any pl.DataModule with a predict_dataloader method\\ntrainer = pl.Trainer(gpus=2, accelerator=ddp)\\npredictions = trainer.predict(model=m, datamodule=dm)\\nbut we\\'ll get two predictions on two GPUs(they are actually the outputs of two scripts that pytorch_lightning creates for us anyway...), and we can\\'t use it as a single object for later processing in the script.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzkwNzAxcon': 'How do I correctly override gather behaviour?\\nIdeally, I would have a single place where I should specify new method and it would work with dp, ddp and ddp_spawn.',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzkyMjc2con': '🐛 Bug\\nIf there are unsued parameters in the model, should we still explicitly mentioned the trainer as follows?\\nplugins=[DDPPlugin(find_unused_parameters=True)]\\n trainer = pl.Trainer.from_argparse_args( args, weights_summary=None,  callbacks=[logging_callback],  logger=logger,         plugins=[DDPPlugin(find_unused_parameters=True)], **train_params,)',\n",
              " 'MDEwOkRpc2N1c3Npb24zMzkyNDAwcon': 'Hi,\\nWhen calling train.fit(model) I get the following TypeError. I got a couple of more similar errors but could find solutions to them (mainly due to the newer version of pl), but I could not find any fix for following error:\\n  File \"train.py\", line 540, in <module>\\n    main(args)\\n  File \"train.py\", line 506, in main\\n    logger=logger,\\n  File \"/Structure-Aware-BART/src/lightning_base.py\", line 700, in generic_train\\n    trainer.fit(model)\\n  File \"/anaconda3/envs/s-bart/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\\n    self._run(model)\\n  File \"/anaconda3/envs/s-bart/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 713, in _run\\n    self.call_setup_hook(model)  # allow user to setup lightning_module in accelerator environment\\n  File \"/anaconda3/envs/s-bart/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1161, in call_setup_hook\\n    model.setup(stage=fn)\\nTypeError: setup() got an unexpected keyword argument \\'stage\\'\\n\\nAny pointers would be much appreciated. Thx',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA1OTg5con': \"Hi !\\nI'm currently working on a segmentation network using PyTorch Lightning and MONAI.\\nContext\\n\\nIn my LightningDataModule, I preprocess my images by applying transforms (e.g., to resample them) before feeding them to my DataLoaders. For the transforms, I use a 3rd party framework called MONAI. It stores the context information of all the applied transforms in a structured nested dictionary.\\nAfter predicting the labels of my predict dataset, I would like to invert these transforms (e.g., to recover the original pixel dimensions) in my predict_step.\\n\\nProblem\\n\\nMONAI's inverting logic requires the context information to be numpy data or CPU tensors (GPU tensors are not supported). However, by default, Lightning moves all the data of my batch (including its context info) to GPU. It causes the bug I reproduced in this test example notebook (cf. last cell).\\n\\nMy question\\nHow can I tell Lightning to only move the images and their labels to GPU and keep the context info on the CPU?\\nFor further details, please refer to the following discussion Project-MONAI/MONAI#2348.\\nThanks for your help !\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA3MTg3con': \"From the training_step_end() docs it says:\\n\\nIf you later switch to ddp or some other mode, this will still be called so that you don’t have to change your code\\n\\nWhen using dp or ddp2 the first dimension is equal to the number of GPUs, and it has the per-GPU results like gpu_n_pred = training_step_outputs[n]['pred'] as shown in the example in the docs. This makes sense since there is a gather in the forward pass. For DDP though there does not need to be a gather / sync / barrier in the forward pass, only for the gradients in the backward pass.\\nSo does this just pass through the single-GPU output in the DDP case? E.g. in the same example, is training_step_outputs just the dictionary from that single GPU, like gpu_pred = training_step_outputs['pred']?\\nOr if I define this method does it add a barrier / gather as if doing outputs = self.all_gather(outputs), such that all of the GPU results are actually available like gpu_n_pred = training_step_outputs[n]['pred'] as in the DP/DDP2 case?\\nI just want to make sure I'm not slowing down my code if I define this method for the dp / ddp2 case but then almost always use standard ddp. Sorry if this was already asked or is in the docs, I tried my best to find the answer. Thanks!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA3NzMwcon': \"I am using CLI to train my model. Instead of specifying parameters directly, I provide a yaml file with variables defined.\\nSince I'm using several loggers, they have a common name parameter. So in order to start a new experiment I have to change this parameter in each logger. This raises a question is there a way to create global variable in yaml file while using CLI?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA3ODk2con': 'Question:\\nI am trying to run the EpicKitchen codes from https://github.com/epic-kitchens/C1-Action-Recognition-TSN-TRN-TSM\\nI am getting this Typeerror may be related to older and new versions of lightining module and i was not able to resolve it.\\nError:\\nTraceback (most recent call last):\\nFile \"src/test.py\", line 145, in \\nmain(parser.parse_args())\\nFile \"src/test.py\", line 139, in main\\ntrainer = Trainer(**cfg.trainer, callbacks=[saver])\\nFile \"/home/code-base/.intdoc/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 41, in overwrite_by_env_vars\\nreturn fn(self, **kwargs)\\nTypeError: init() got an unexpected keyword argument \\'row_log_interval\\'\\nCode:   test.py file\\nfrom collections import defaultdict\\nimport argparse\\nimport logging\\nimport os\\nimport pickle\\nfrom pathlib import Path\\nimport colorlog\\nimport torch\\nimport numpy as np\\nfrom omegaconf import OmegaConf\\nfrom pytorch_lightning import Callback, Trainer\\nfrom typing import Any, Dict, List, Sequence, Union\\nfrom systems import EpicActionRecogintionDataModule\\nfrom systems import EpicActionRecognitionSystem\\n\\nparser = argparse.ArgumentParser(\\n    description=\"Test model\", formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n)\\nparser.add_argument(\"checkpoint\", type=Path)\\nparser.add_argument(\"results\", type=Path)\\nparser.add_argument(\"--split\", choices=[\"val\", \"test\"], default=\"test\")\\nparser.add_argument(\\n    \"--n-frames\",\\n    type=int,\\n    help=\"Overwrite number of frames to feed model, defaults to the \"\\n    \"data.test_frame_count or data.frame_count if the former is not present\",\\n)\\nparser.add_argument(\\n    \"--batch-size\",\\n    type=int,\\n    help=\"Overwrite the batch size for loading data, defaults to learning.batch_size\",\\n)\\nparser.add_argument(\\n    \"--datadir\",\\n    default=None,\\n    help=\"Overwrite data directory in checkpoint. Useful when testing a checkpoint \"\\n    \"trained on a different machine.\",\\n)\\n\\nLOG = logging.getLogger(\"test\")\\nclass ResultsSaver(Callback):\\n    def __init__(self):\\n        super().__init__()\\n        self.results: Dict[str, Dict[str, List[Any]]] = dict()\\n    def on_test_batch_end(\\n        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx\\n    ):\\n        self._store_batch_results(\"test\", outputs)\\n\\n    def _store_batch_results(\\n        self, dataset_name: str, batch_outputs: Dict[str, Sequence[Any]]\\n    ):\\n        if dataset_name not in self.results:\\n            self.results[dataset_name] = {k: [] for k in batch_outputs.keys()}\\n\\n        for k, vs in batch_outputs.items():\\n            if isinstance(vs, torch.Tensor):\\n                vs = vs.detach().cpu().numpy()\\n            self.results[dataset_name][k].extend(vs)\\n\\n    def save_results(self, dataset_name: str, filepath: Union[str, Path]):\\n        filepath = Path(filepath)\\n        filepath.parent.mkdir(parents=True, exist_ok=True)\\n        results_dict = self.results[dataset_name]\\n        new_results_dict = {\\n            k: np.stack(vs)\\n            for k, vs in results_dict.items()\\n        }\\n\\n        with open(filepath, \"wb\") as f:\\n            pickle.dump(new_results_dict, f)\\n\\n\\ndef main(args):\\n    logging.basicConfig(level=logging.INFO)\\n\\n    handler = colorlog.StreamHandler()\\n    handler.setFormatter(\\n        colorlog.ColoredFormatter(\"%(log_color)s%(levelname)s:%(name)s:%(message)s\")\\n    )\\n\\n    logger = colorlog.getLogger(\"example\")\\n    logger.addHandler(handler)\\n\\n    ckpt = torch.load(args.checkpoint, map_location=lambda storage, loc: storage)\\n    # Publicly released checkpoints use dicts for longevity, so we need to wrap them\\n    # up in an OmegaConf object as this is what EpicActionRecognitionSystem expects.\\n    cfg = OmegaConf.create(ckpt[\"hyper_parameters\"])\\n    OmegaConf.set_struct(cfg, False)  # allow writing arbitrary keys without raising\\n    # exceptions\\n    cfg.data._root_gulp_dir = os.getcwd()  # set default root gulp dir to prevent\\n    # exceptions on instantiating the EpicActionRecognitionSystem\\n\\n    system = EpicActionRecognitionSystem(cfg)\\n    system.load_state_dict(ckpt[\"state_dict\"])\\n    if not cfg.get(\"log_graph\", True):\\n        # MTRN can\\'t be traced due to the model stochasticity so causes a JIT tracer\\n        # error, we allow you to prevent the tracer from running to log the graph when\\n        # the summary writer is created\\n        try:\\n            delattr(system, \"example_input_array\")\\n        except AttributeError:\\n            pass\\n\\n    if args.n_frames is not None:\\n        cfg.data.test_frame_count = args.n_frames\\n    if args.batch_size is not None:\\n        cfg.learning.batch_size = args.batch_size\\n    if args.datadir is not None:\\n        data_dir_key = f\"{args.split}_gulp_dir\"\\n        cfg.data[data_dir_key] = args.datadir\\n\\n    # Since we don\\'t support writing results when using DP or DDP\\n    LOG.info(\"Disabling DP/DDP\")\\n    cfg.trainer.accelerator = None\\n\\n    n_gpus = 1\\n    LOG.info(f\"Overwriting number of GPUs to {n_gpus}\")\\n    cfg.trainer.gpus = n_gpus\\n    cfg[\"test.results_path\"] = str(args.results)\\n\\n    data_module = EpicActionRecogintionDataModule(cfg)\\n    if args.split == \"val\":\\n        dataloader = data_module.val_dataloader()\\n    elif args.split == \"test\":\\n        dataloader = data_module.test_dataloader()\\n    else:\\n        raise ValueError(\\n            f\"Split {args.split!r} is not a recognised dataset split to \" f\"test on.\"\\n        )\\n\\n    saver = ResultsSaver()\\n    trainer = Trainer(**cfg.trainer, callbacks=[saver])\\n    trainer.test(system, test_dataloaders=dataloader)\\n    saver.save_results(\"test\", args.results)\\nif __name__ == \"__main__\":\\n    main(parser.parse_args())\\n\\n\\nVersions:\\nPython3.6, lightining-1.1.8, pytorch-1.7.1, Running on an linux machine\\nI am new to this library, any help would be appreciated\\nThanks in advance',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA4NjYwcon': \"Hi all, I am running a script that performs some calculations and then does a testing loop using multi GPU using a single node with DDP.\\nThe code looks like this:\\nsome prepocesing code\\n...\\nmodel = LitModel(path_weights)\\n\\ndataloader = DataLoader(dataset, batch_size=512,\\n                        shuffle=False, num_workers=32)\\ntrainer = pl.Trainer(accelerator='ddp', gpus=8)\\ntrainer.test(model, dataloader)\\nThe issue that I have is that it seems the the whole script is being computed several times, I can see that the preprocessing code is being called 8 times. It seems to be the same issue as posted in SO (https://stackoverflow.com/questions/66261729/pytorch-lightning-duplicates-main-script-in-ddp-mode)\\nAm I missing something in my code?\\nThanks!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA5MDY5con': \"Hi. I trained a model several times,\\nand I found that Pytorch-lightning sometimes appends version, e.g. model-v1.ckpt, and sometimes doesn't append.\\n(I don't know the reason because the codes of the two situations are the same)\\nHere, I want to let PyTorch-lightning do not append any version. Is there any solution?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA5MzIwcon': 'Hello,\\nI have written a small class derived from Callback in order to log some outputs during training. Initially, a directory is created, for the corresponding files/outputs to be stored:\\nclass epochCallback(pl.callbacks.Callback):\\n\\n    @pl.utilities.rank_zero_only\\n    def on_train_start(self, trainer, pl_module):\\n        if not restart:\\n            print(\"create output directory (rank {})\".format(self.model.global_rank))\\n                if not isdir(aiOutputDir +\"outputs_train\"):\\n                    os.mkdir(aiOutputDir +\"outputs_train\")\\n\\nMy intention is to create the directory only once, for the main rank/GPU, that\\'s why I use the rank_zero_only decorator.\\nWhen I try it on multiple GPUs (ddp), the code hangs. If I create an empty directory in advance, then it runs! It seems to me like a barrier is needed before creating the dir... If this is the case, how do I add this in lightning? I have also noticed that the print statement is printed for all GPUs, so I am not sure if the decorator works as it should... Should I use os.environ() instead of the decorator, and specify explicitly the global ID rank which will create the directory?\\nAny ideas? What is the standard practice here?\\nThanks in advance,\\nNikos',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDAxNjI5con': 'Hi!\\nIn my LightningDataModule, I apply preprocessing transforms to my input data before feeding it to my dataloader. In the same datamodule, I also defined the postprocessing transforms to apply after the inference.\\nself.post_transforms = Compose([\\n            AsDiscreted(keys=\"pred\", argmax=True, to_onehot=False, n_classes=3),\\n            Invertd(\\n                keys=\"pred\",  # invert the `pred` data field, also support multiple fields\\n                transform=val_transforms,\\n                loader=val_dataloader,\\n                orig_keys=\"img\",  # get the previously applied pre_transforms information on the `img` data field,\\n                                  # then invert `pred` based on this information. we can use same info\\n                                  # for multiple fields, also support different orig_keys for different fields\\n                meta_keys=\"pred_meta_dict\",  # key field to save inverted meta data, every item maps to `keys`\\n                orig_meta_keys=\"img_meta_dict\",  # get the meta data from `img_meta_dict` field when inverting,\\n                                                 # for example, may need the `affine` to invert `Spacingd` transform,\\n                                                 # multiple fields can use the same meta data to invert\\n                meta_key_postfix=\"meta_dict\",  # if `meta_keys=None`, use \"{keys}_{meta_key_postfix}\" as the meta key,\\n                                               # if `orig_meta_keys=None`, use \"{orig_keys}_{meta_key_postfix}\",\\n                                               # otherwise, no need this arg during inverting\\n                nearest_interp=True,  # change to use \"nearest\" mode in interpolation when inverting\\n                to_tensor=True,  # convert to PyTorch Tensor after inverting\\n            ),\\n            SaveImaged(keys=\"pred\", meta_keys=\"pred_meta_dict\", output_dir=\"./out\", output_postfix=\"seg\", resample=False),\\n        ])\\n\\nI want to apply these post_transforms to my inference outputs in predict_step(). What would be the best \"Lightning way\" to give access to my datamodule.post_transforms attribute to predict_step?\\ndef predict_step(self, batch: Any, batch_idx: int):\\n        batch[\"pred\"] = self.forward(batch)\\n        post_transforms(batch)\\n\\nThanks in advance for your suggestions :)',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDAyOTgxcon': \"v_num in progress bar means the version number of this running, and the log file's save directory is version_{v_num}.\\nHow can i customize the directory's name, such as version_GAT, version_GCN.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDE5NjM3con': 'How to get predictions\\n  class OurModel(LightningModule):\\n      def __init__(self):\\n        super(OurModel,self).__init__()\\n        self.layer = MyModelV3()\\n      def forward(self,x):\\n        return self.layer(x)\\n\\n  def train_dataloader(self):\\n    return DataLoader(DataReader(train_df))\\n\\n  def training_step(self,batch,batch_idx):\\n    return loss\\n\\n  def test_dataloader(self):\\n    return DataLoader(DataReader(test_df))\\n    \\n  def test_step(self,batch,batch_idx):\\n    image,label=batch\\n    out=self(image)\\n    loss=self.criterion(out,label)\\n    return loss\\n\\n  def predict(self, batch):\\n        return self(batch) \\nI am not sure, how to use predict function. How to define data loader for predict function. I want to get predictions for test_df. But now idea how to do this.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDE5OTIzcon': 'I want to apply custom learning rate scheduler like below.\\nclass WarmupLRScheduler(torch.optim.lr_scheduler._LRScheduler):\\n    \"\"\"\\n    Warmup learning rate until `total_steps`\\n\\n    Args:\\n        optimizer (Optimizer): wrapped optimizer.\\n        configs (DictConfig): configuration set.\\n    \"\"\"\\n    def __init__(\\n            self,\\n            optimizer: Optimizer,\\n            configs: DictConfig,\\n    ) -> None:\\n        super(WarmupLRScheduler, self).__init__(optimizer, configs.lr_scheduler.init_lr)\\n        if configs.lr_scheduler.warmup_steps != 0:\\n            warmup_rate = configs.lr_scheduler.peak_lr - configs.lr_scheduler.init_lr\\n            self.warmup_rate = warmup_rate / configs.lr_scheduler.warmup_steps\\n        else:\\n            self.warmup_rate = 0\\n        self.update_steps = 1\\n        self.lr = configs.lr_scheduler.init_lr\\n        self.warmup_steps = configs.lr_scheduler.warmup_steps\\n\\n    def step(self, val_loss: Optional[torch.FloatTensor] = None):\\n        if self.update_steps < self.warmup_steps:\\n            lr = self.init_lr + self.warmup_rate * self.update_steps\\n            self.set_lr(self.optimizer, lr)\\n            self.lr = lr\\n        self.update_steps += 1\\n        return self.lr\\nBut I find that my custom lr schedulers doesn\\'t work in pytorch lightning.\\nI set lightning module\\'s configure_optimizers like below:\\ndef configure_optimizers(self):\\n    r\"\"\"\\n    Choose what optimizers and learning-rate schedulers to use in your optimization.\\n\\n\\n    Returns:\\n        - **Dictionary** - The first item has multiple optimizers, and the second has multiple LR schedulers\\n            (or multiple ``lr_dict``).\\n    \"\"\"\\n    SUPPORTED_OPTIMIZERS = {\\n        \"adam\": Adam,\\n        \"adamp\": AdamP,\\n        \"radam\": RAdam,\\n        \"adagrad\": Adagrad,\\n        \"adadelta\": Adadelta,\\n        \"adamax\": Adamax,\\n        \"adamw\": AdamW,\\n        \"sgd\": SGD,\\n        \"asgd\": ASGD,\\n        \"novograd\": Novograd,\\n    }\\n\\n    assert self.configs.model.optimizer in SUPPORTED_OPTIMIZERS.keys(), \\\\\\n        f\"Unsupported Optimizer: {self.configs.model.optimizer}\\\\n\" \\\\\\n        f\"Supported Optimizers: {SUPPORTED_OPTIMIZERS.keys()}\"\\n\\n    self.optimizer = SUPPORTED_OPTIMIZERS[self.configs.model.optimizer](\\n        self.parameters(),\\n        lr=self.configs.lr_scheduler.lr,\\n    )\\n    scheduler = SCHEDULER_REGISTRY[self.configs.lr_scheduler.scheduler_name](self.optimizer, self.configs)\\n\\n    if self.configs.lr_scheduler.scheduler_name == \"reduce_lr_on_plateau\":\\n        lr_scheduler = {\\n            \\'scheduler\\': scheduler,\\n            \\'monitor\\': \\'val_loss\\',\\n            \\'interval\\': \\'epoch\\',\\n        }\\n    elif self.configs.lr_scheduler.scheduler_name == \"warmup_reduce_lr_on_plateau\":\\n        lr_scheduler = {\\n            \\'scheduler\\': scheduler,\\n            \\'monitor\\': \\'val_loss\\',\\n            \\'interval\\': \\'step\\',\\n        }\\n    else:\\n        lr_scheduler = {\\n            \\'scheduler\\': scheduler,\\n            \\'interval\\': \\'step\\',\\n        }\\n\\n    return {\\n        \\'optimizer\\': self.optimizer,\\n        \\'lr_scheduler\\': lr_scheduler\\n    }\\nIf you fine some weird, please let me know. Thank you.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI1NDc2con': \"Hello. Please forgive the very basic question!\\nI really like the perks and freebies that come with letting the Lightning Trainer handle training. However, for some applications I do not need or want a dataloader, and so far I've not been able to figure out how to use the Trainer without fooling it by defining a dummy DataLoader that does nothing. 🙄\\nI would like to know if there is a 'nice' way to use the 'standard' Lightning setup of a LightningModule + Trainer that bypasses any dependence on a data loader.\\nThe context is that model (normalizing flow) inputs are generated on-demand by sampling from some known distribution, and the loss function is the KL divergence with respect to some other distribution whose un-normalised density function is known.\\nGrateful for any suggestions!\\nCheers,\\nJoe.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI1OTU0con': \"when i try to write a model, i got forward() takes 1 positional argument but 2 were given error, this is my code, i want to know the wrong plcace, thanks!!\\ni guess the error is in UpSample place, but i don't know why...\\nclass DownSample(nn.Module):\\n\\n    def __init__(self, in_planes: int, out_planes: int, kernel_size: int):\\n        super(DownSample, self).__init__()\\n\\n        self.down = nn.Sequential(\\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=2, padding=1),\\n            nn.BatchNorm2d(out_planes),\\n            nn.LeakyReLU()\\n        )\\n\\n        init_weight.initialize(self)\\n\\n    def forward(self, x):\\n        return self.down(x)\\n\\n\\nclass UpSample(nn.Module):\\n\\n    def __init__(self, in_planes: int, out_planes: int,\\n                 kernel_size: int, padding: int, output_padding: int,\\n                 apply_dropout: bool = False):\\n        super(UpSample, self).__init__()\\n\\n        self.up = nn.ModuleList()\\n\\n        self.up.append(\\n            nn.ConvTranspose2d(in_planes, out_planes, kernel_size, stride=2,\\n                               padding=padding, output_padding=output_padding),\\n        )\\n        self.up.append(nn.BatchNorm2d(out_planes))\\n        if apply_dropout:\\n            self.up.append(nn.Dropout())\\n        self.up.append(nn.LeakyReLU())\\n\\n        init_weight.initialize(self)\\n\\n    def forward(self, inputs):\\n        return self.up(inputs)\\nclass MyEncoder(nn.Module):\\n\\n    def __init__(self):\\n        super(MyEncoder, self).__init__()\\n\\n        down_stack = [\\n            pix2pix.DownSample(3, 64, 4),\\n            pix2pix.DownSample(64, 128, 4),\\n            pix2pix.DownSample(128, 256, 4),\\n            pix2pix.DownSample(256, 512, 4),\\n            pix2pix.DownSample(512, 512, 4),\\n            pix2pix.DownSample(512, 512, 4),\\n            pix2pix.DownSample(512, 512, 4),\\n            pix2pix.DownSample(512, 512, 4),\\n        ]\\n\\n        self.encoder = nn.ModuleList()\\n\\n        for item in down_stack:\\n            self.encoder.append(item)\\n\\n    def forward(self, inputs):\\n        feat = inputs\\n        for i in range(len(self.encoder)):\\n            feat = self.encoder[i](feat)\\n\\n        return feat\\n\\n\\nclass MyDecoder(nn.Module):\\n    def __init__(self):\\n        super(MyDecoder, self).__init__()\\n\\n        up_stack = [\\n            pix2pix.UpSample(512, 512, 4, 1, 1, True),\\n            pix2pix.UpSample(512, 512, 4, 1, 1, True),\\n            pix2pix.UpSample(512, 512, 4, 1, 1, True),\\n            pix2pix.UpSample(512, 512, 4, 1, 1, True),\\n            pix2pix.UpSample(512, 256, 4, 1, 1, True),\\n            pix2pix.UpSample(256, 128, 4, 1, 1, True),\\n            pix2pix.UpSample(256, 128, 4, 1, 1, True),\\n            pix2pix.UpSample(128, 64, 4, 1, 1, True),\\n        ]\\n\\n        self.up = nn.ModuleList()\\n\\n        for item in up_stack:\\n            self.up.append(item)\\n\\n    def forward(self, inputs):\\n\\n        return self.up(inputs)\\n\\n\\nclass MyNet(pl.LightningModule):\\n\\n    def __init__(self):\\n        super(MyNet, self).__init__()\\n\\n        self.encoder = MyEncoder()\\n        self.decoder = MyDecoder()\\n\\n    def forward(self, inputs):\\n        feat = self.encoder(inputs)\\n        feat = self.decoder(feat)\\n        return feat\\n\\n\\nif __name__ == '__main__':\\n    from torchsummaryX import summary\\n\\n    import torch\\n\\n    x = torch.ones((1, 3, 512, 512))\\n    u = UNet()\\n\\n    summary(model=u, x=x)\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI2NjI5con': 'Hello,\\nI have a jitted function within which I need to use the output of a neural network (trained using PyTorch Lightning). The pseudo code will make this clearer:\\nwhile True:\\n    x = sample_from_model() # ← numpy type, hence compatible with numba\\n    out = NN(torch.Tensor(x)) # ← incompatible with numba\\nIs there a way to circumvent this problem? First thing that comes to mind is to manually extract the weights and compute the forward pass.\\nThanks in advance,\\nPetar',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI2NzQycon': 'There is no train.test () in the lower version. How to test the test data set?\\nmy version: 0.4.6',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI3OTg1con': 'My dataset is large, with total CPU memory usage of 20 GB. I train on 2 nodes with 8 GPU. And I use slurm to train it. But I found that each process will consume 20 GB memory, which is equivelence to 80 GB each node. That\\'s not what I want. I want a node to consume only 20GB in total. Is there a way to do that?\\nclass DataModule(LightningDataModule):\\n    def __init__(self):\\n        super().__init__()\\n        self.batch_size = 1\\n        self.CT_dataset=np.load(\"./CT_dataset.npy\")#shape:(7000,1,512,512)\\n        self.MR_dataset=np.load(\"./MR_dataset.npy\")#shape:(7000,1,512,512)\\n        self.batch_size=1\\n        self.CT_dataset = torch.from_numpy(self.CT_dataset)\\n        self.CT_dataset = self.CT_dataset.float()\\n        self.MR_dataset = torch.from_numpy(self.MR_dataset)\\n        self.MR_dataset = self.MR_dataset.float()\\n        self.train_dataset, self.test_dataset = random_split(TensorDataset(self.MR_dataset,self.CT_dataset), [len(self.CT_dataset)-100, 100])\\n    def train_dataloader(self):\\n        return DataLoader(self.train_dataset, batch_size=self.batch_size)\\n    def test_dataloader(self):\\n        return DataLoader(self.test_dataset, batch_size=self.batch_size)\\nmodel = CycleGAN()\\nds = DataModule()\\nlogger = TensorBoardLogger(save_dir=\"./run\")\\ntrainer = pl.Trainer(max_epochs=1,fast_dev_run=False,profiler=\"pytorch\",overfit_batches=8,gpus=4,logger=logger,accelerator=\\'ddp\\',num_nodes=2,auto_scale_batch_size=\\'power\\',weights_summary=\\'full\\')\\ntrainer.fit(model, ds)\\ntrainer.test(model,datamodule=ds)\\nMy code will raise MemoryError: Unable to allocate 7.00 GiB for an array with shape (1879572480,) and data type int32, I can\\'t think of a way to solve it.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI3OTk4con': '🐛 Bug\\nWhen the dataset size is small (i.e. comparable to the minibatch size), it slows training down significantly.\\nNo GPU, batch size 64, dataset size 1024: 185 iterations/second\\nNo GPU, batch size 64, dataset size 100: 47 iterations/second\\n1 GPU, batch size 64, dataset size 1024: 110 iterations/second\\n1 GPU, batch size 64, dataset size 100: 23 iterations/second\\n1 GPU, batch size 800, dataset size 1024: 19 iterations/second\\n1 GPU, batch size 800, dataset size 10000: 90 iterations/second\\n1 GPU, batch size 64, dataset size 10000: 235 iterations/second\\nPlease reproduce using the BoringModel\\nimport os, sys\\nfrom argparse import ArgumentParser\\n\\nimport torch\\nfrom torch.utils.data import Dataset, DistributedSampler, DataLoader\\n\\nfrom pl_examples import cli_lightning_logo\\nfrom pytorch_lightning import LightningModule, Trainer\\nfrom pytorch_lightning.utilities.seed import seed_everything\\nfrom pytorch_lightning.callbacks.progress import ProgressBar, ProgressBarBase, tqdm, reset, convert_inf\\n\\nclass CustomProgressBar(ProgressBar):\\n    def init_train_tqdm(self) -> tqdm:\\n        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\\n        bar = tqdm(\\n            desc=\\'Training\\',\\n            initial=self.trainer.global_step,\\n            position=(2 * self.process_position),\\n            disable=self.is_disabled,\\n            leave=True,\\n            dynamic_ncols=True,\\n            file=sys.stdout,\\n            smoothing=0,\\n        )\\n        return bar\\n    def on_train_start(self, trainer, pl_module):\\n        super(ProgressBar, self).on_train_start(trainer, pl_module)\\n        self.main_progress_bar = self.init_train_tqdm()\\n        self.prev_train_gs = -1\\n        reset(self.main_progress_bar, self.trainer.max_steps)\\n\\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\\n        super(ProgressBar, self).on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx)\\n        if self.prev_train_gs != self.trainer.global_step and self._should_update(self.trainer.global_step, self.trainer.max_steps):\\n            self._update_bar(self.main_progress_bar)\\n            self.main_progress_bar.set_postfix(trainer.progress_bar_dict)\\n            self.prev_train_gs = self.trainer.global_step\\n\\n    def on_train_epoch_start(self, trainer, pl_module):\\n        super(ProgressBar, self).on_train_epoch_start(trainer, pl_module)\\n\\n    def on_train_end(self, trainer, pl_module):\\n        super(ProgressBar, self).on_train_end(trainer, pl_module)\\n\\nclass RandomDataset(Dataset):\\n    \"\"\"\\n    >>> RandomDataset(size=10, length=20)  # doctest: +ELLIPSIS\\n    <...bug_report_model.RandomDataset object at ...>\\n    \"\"\"\\n\\n    def __init__(self, size, length):\\n        self.len = length\\n        self.data = torch.randn(length, size)\\n\\n    def __getitem__(self, index):\\n        return self.data[index]\\n\\n    def __len__(self):\\n        return self.len\\n\\n\\nclass BoringModel(LightningModule):\\n    \"\"\"\\n    >>> BoringModel()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\\n    BoringModel(\\n      (layer): Linear(...)\\n    )\\n    \"\"\"\\n\\n    def __init__(self, train_data, test_data, bs):\\n        \"\"\"\\n        Testing PL Module\\n\\n        Use as follows:\\n        - subclass\\n        - modify the behavior for what you want\\n\\n        class TestModel(BaseTestModel):\\n            def training_step(...):\\n                # do your own thing\\n\\n        or:\\n\\n        model = BaseTestModel()\\n        model.training_epoch_end = None\\n\\n        \"\"\"\\n        super().__init__()\\n        self.layer1 = torch.nn.Linear(32, 32)\\n        self.layer2 = torch.nn.Linear(32, 32)\\n        self.layer3 = torch.nn.Linear(32, 2)\\n\\n        self.train_data = train_data\\n        self.test_data = test_data\\n        self.bs = bs\\n\\n    def forward(self, x):\\n        return self.layer3(torch.relu(self.layer2(torch.relu(self.layer1(x)))))\\n\\n    def loss(self, batch, prediction):\\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\\n\\n    def step(self, x):\\n        x = self.forward(x)\\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\\n        return out\\n\\n    def training_step(self, batch, batch_idx):\\n        output = self.forward(batch)\\n        loss = self.loss(batch, output)\\n        return {\"loss\": loss}\\n\\n    def training_step_end(self, training_step_outputs):\\n        return training_step_outputs\\n\\n    def training_epoch_end(self, outputs) -> None:\\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\\n\\n    def configure_optimizers(self):\\n        optimizer = torch.optim.Adam(list(self.layer1.parameters()) + list(self.layer2.parameters()) + list(self.layer3.parameters()), lr=0.001)\\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\\n        return [optimizer], [lr_scheduler]\\n\\n    def train_dataloader(self):\\n        train_loader = DataLoader(self.train_data, shuffle=True, num_workers=1, batch_size=self.bs)\\n        return train_loader\\n\\nparser = ArgumentParser()\\nparser.add_argument(\"--gpus\", type=int, default=0)\\nparser.add_argument(\"--num_processes\", type=int, default=1)\\nparser.add_argument(\"--dataset_size\", type=int, default=1024)\\nparser.add_argument(\"--mb_size\", type=int, default=64)\\nargs = parser.parse_args()\\n\\n\\ndef test_run():\\n    # data\\n    train_data = torch.randn(args.dataset_size, 32)\\n    test_data = torch.randn(256, 32)\\n\\n    # model\\n    model = BoringModel(train_data, test_data, bs=args.mb_size)\\n    trainer = Trainer(\\n        gpus=args.gpus,\\n        logger=False,\\n        max_steps=5000,\\n        limit_val_batches=0,\\n        num_processes=args.num_processes,\\n        weights_summary=None,\\n        reload_dataloaders_every_epoch=False,\\n        callbacks=[CustomProgressBar()]\\n    )\\n\\n    # fit\\n    trainer.fit(model)\\n\\n    print(f\"{trainer.accelerator_backend=}\")\\n    print(f\"{trainer.gpus=}\")\\n    print(f\"{trainer.num_processes=}\")\\n    print(f\"{trainer.global_step=}\")\\n\\nif __name__ == \"__main__\":\\n    test_run()\\nTo Reproduce\\nRun the following command: python bug_report.py --gpus 1 --dataset_size 10000 --mb_size 64\\nfor varying values of gpus, dataset_size, and mb_size.\\nExpected behavior\\nIterations/second is unaffected by dataset size.\\nEnvironment\\n\\nPyTorch Version (e.g., 1.0): 1.8.1\\nOS (e.g., Linux): Linux\\nHow you installed PyTorch (conda, pip, source): conda\\nPython version: 3.9\\n\\nAdditional context\\nMy guess is that this is caused by inter-epoch reloading of the dataset. The code should be restructured to pre-load a fixed number of minibatches ahead, rather than caring about the location of epoch boundaries.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI4MDczcon': 'Hey, as the title says, I want to access the DataModule or the DataLoader inside the on fit start hook. Is this possible and how can I do it? To be more specifc I want to access my model, when I have access to my DataModule, to get a batch of data, then use it to apply some pruning algorithm on my model.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDIwNDgwcon': \"I'm implementing the same functionality for validation_step and test_step.\\nCurrently, I have implemented it by calling to a shared function (val_and_test_step)\\n    def val_and_test_step(self, data_batch, batch_nb):\\n        output = shared_functionality(data_batch, batch_nb)\\n        return output\\n\\n    def validation_step(self, data_batch, batch_nb):\\n        return self.val_and_test_step(data_batch, batch_nb)\\n\\n    def test_step(self, data_batch, batch_nb):\\n        return self.val_and_test_step(data_batch, batch_nb) \\n\\nIs there a more pythonic way to implement the above?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDIxMTQ4con': \"this is my model __init__ func\\ndef __init__(self, num_classes: int, image_channels: int = 3, drop_rate: int = 0.5,\\n                 filter_config: tuple = (64, 128, 256, 512, 512), attention=False):\\nthis is my load code:\\nm = SegNet(num_classes=1)\\nmodel = m.load_from_checkpoint('checkpoints/epoch=99-step=312499.ckpt')\\nwhen i try to load a checkpoint model, i got this error:\\nTypeError: __init__() missing 1 required positional argument: 'num_classes'\\n\\nwho can help me?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDIxNDU1con': 'Can I set the epoch/step initial value to 1? Now the initial default is 0, it feels awkward when watching Tensorboard.\\nIn addition, can I bring plots in the training and valid in one tersorboard plot?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDIyMTkycon': 'Hi, I have 4 gpus on my machine. I want to select the third one, that has index 2. How do I pass an argument from CLI?\\nI call\\npython 4_pretrain_encoder.py --gpus 2 --max_epochs 5\\nbut it runs one script on two gpus instead of the third.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDMxMzAwcon': \"What's the difference between on_fit_start and on_train_start hooks in LightningModule?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDMxNDM4con': 'Hi, everyone.\\nI have a repo whose link is https://github.com/zhoufengfan/pytorch-lightning-cifar10.\\nWhen I run python train.py, it returns this error:\\nTraceback (most recent call last):\\n  File \"train.py\", line 21, in <module>\\n    trainer.fit(model, cifar10_dm)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\\n    self._run(model)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 756, in _run\\n    self.dispatch()\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 797, in dispatch\\n    self.accelerator.start_training(self)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 96, in start_training\\n    self.training_type_plugin.start_training(trainer)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 144, in start_training\\n    self._results = trainer.run_stage()\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 807, in run_stage\\n    return self.run_train()\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 842, in run_train\\n    self.run_sanity_check(self.lightning_module)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1107, in run_sanity_check\\n    self.run_evaluation()\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 962, in run_evaluation\\n    output = self.evaluation_loop.evaluation_step(batch, batch_idx, dataloader_idx)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 174, in evaluation_step\\n    output = self.trainer.accelerator.validation_step(args)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 226, in validation_step\\n    return self.training_type_plugin.validation_step(*args)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 322, in validation_step\\n    return self.model(*args, **kwargs)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\\n    result = self.forward(*input, **kwargs)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 619, in forward\\n    output = self.module(*inputs[0], **kwargs[0])\\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\\n    result = self.forward(*input, **kwargs)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py\", line 57, in forward\\n    output = self.module.validation_step(*inputs, **kwargs)\\n  File \"/root/code/test-of-pytorch-lightning/backbone.py\", line 42, in validation_step\\n    self.evaluate(batch, \\'val\\')\\n  File \"/root/code/test-of-pytorch-lightning/backbone.py\", line 32, in evaluate\\n    logits = self(x)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\\n    result = self.forward(*input, **kwargs)\\n  File \"/root/code/test-of-pytorch-lightning/backbone.py\", line 20, in forward\\n    out = self.model(x)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\\n    result = self.forward(*input, **kwargs)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/models/resnet.py\", line 220, in forward\\n    return self._forward_impl(x)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/models/resnet.py\", line 215, in _forward_impl\\n    x = self.fc(x)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\\n    result = self.forward(*input, **kwargs)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 93, in forward\\n    return F.linear(input, self.weight, self.bias)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\", line 1690, in linear\\n    ret = torch.addmm(bias, input, weight.t())\\nRuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`\\n\\nBut after I change the AVAIL_GPUS  in the hyper_var.py file to min(1, torch.cuda.device_count()), the bug disappear()(see the comment in the hyper_var.py file).\\nWhen I change the  AVAIL_GPUS  in the hyper_var.py file to \"1,2\", it returns this error:\\nTraceback (most recent call last):\\n  File \"train.py\", line 21, in <module>\\n    trainer.fit(model, cifar10_dm)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\\n    self._run(model)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 753, in _run\\nTraceback (most recent call last):\\n  File \"/root/code/test-of-pytorch-lightning/train.py\", line 21, in <module>\\n    self.pre_dispatch()\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 778, in pre_dispatch\\n    trainer.fit(model, cifar10_dm)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\\n    self.accelerator.pre_dispatch(self)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 108, in pre_dispatch\\n    self.training_type_plugin.pre_dispatch()\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 279, in pre_dispatch\\n    self._run(model)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 753, in _run\\n    self.barrier()\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 286, in barrier\\n    torch_distrib.barrier()\\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 1960, in barrier\\n    self.pre_dispatch()\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 778, in pre_dispatch\\n    self.accelerator.pre_dispatch(self)\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 108, in pre_dispatch\\n    work = _default_pg.barrier()\\n    self.training_type_plugin.pre_dispatch()\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 279, in pre_dispatch\\nRuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1607370172916/work/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8\\n    self.barrier()\\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 286, in barrier\\n    torch_distrib.barrier()\\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 1960, in barrier\\n    work = _default_pg.barrier()\\nRuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1607370172916/work/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8\\n\\nThe hardware parameters of my server are:\\nNVIDIA GeForce 3090\\nCUDA Version: 11.2\\nDriver Version: 460.39\\n\\nCan anyone help me to fix the bug?\\nThanks.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDMzMTY2con': \"My training_step returns {‘loss’: loss, ‘num’: len(batch)}, so I could calculate mean loss at the end of epoch. But now I get this warning, that my training_step returns None and loss doesn't display in the progress bar. How can I return dict from training_step and display loss in the progress bar at the same tim?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDMzNzM4con': 'how to use LightningDataModule  to process tfrecords data, anyone can give a Tutorial',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ1MDEzcon': 'I am experimenting with the following repository. Keiku/PyTorch-Lightning-CIFAR10: \"Not too complicated\" training code for CIFAR-10 by PyTorch Lightning\\nI have implemented two methods, one is to load CIFAR-10 from torchvision and the other is to load CIFAR-10 as a custom dataset. Also, I have implemented two models: a lightweight model (eg scratch resnet18, timm MobileNet V3, etc.) and a relatively heavy model (eg scratch resnet50, timm resnet152).\\nAfter some experiments, I found the following.\\n\\nGPU usage remains high (nearly 100%) on any model when loading CIFAR-10 with torchvision\\nWhen loading CIFAR-10 as a custom dataset, GPU usage remains relatively high (still temporarily zero) for heavy models\\nWhen loading CIFAR-10 as a custom dataset, GPU usage remains low (going back and forth between 0% and 100%) for lightweight models (resnet18, MobileNetV3)\\n\\nIn this situation, is there a problem with the implementation code of the custom dataset? Also, please let me know if there is a way to increase GPU usage even for lightweight models.\\nI am experimenting in the following EC2 g4dn.xlarge environment.\\n⋊> ~ lsb_release -a                                                    (base) 21:45:51\\nNo LSB modules are available.\\nDistributor ID: Ubuntu\\nDescription:    Ubuntu 18.04.5 LTS\\nRelease:        18.04\\nCodename:       bionic\\n⋊> ~ nvidia-container-cli info                                         (base) 21:48:20\\nNVRM version:   450.80.02\\nCUDA version:   11.0\\n\\nDevice Index:   0\\nDevice Minor:   0\\nModel:          Tesla T4\\nBrand:          Tesla\\nGPU UUID:       GPU-ba54be15-066e-e7e5-87d0-84b8ac2672c6\\nBus Location:   00000000:00:1e.0\\nArchitecture:   7.5',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ1Mzg2con': \"Hi,\\nSince the 1.3.3, I cannot use the flag accumulate_grad_batches for the training of GANs.\\nIt is immediately reproducible on the GAN example given in pl_examples.domain_templates.generative_adversarial_net.\\nFor any version higher than 1.3.2, in the Trainer, if accumulate_grad_batches is set to any value higher than 1, after one batch (batch_idx == 1) the following error arises:\\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\\n\\nThis is due to the fact that the g_loss does not require gradients, because the generator's output does not requires any gradients.\\nIs it intended?\\nIf yes, what is the workaround to train GANs while still accumulating gradients?\\nThanks in advance,\\nGuillaume\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ2Mjkzcon': 'I am experimenting with the following repository. Keiku/PyTorch-Lightning-CIFAR10: \"Not too complicated\" training code for CIFAR-10 by PyTorch Lightning\\nIt is implemented as follows. I was able to train/validation. But test has not been implemented yet.\\nimport os\\n\\nimport hydra\\nimport torch\\nfrom hydra.core.hydra_config import HydraConfig\\nfrom omegaconf import DictConfig, OmegaConf\\nfrom pytorch_lightning import Trainer, seed_everything\\nfrom pytorch_lightning.callbacks import ModelCheckpoint\\nfrom pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\\n\\nfrom datamodule import LitCIFAR10DataModule\\nfrom model import LitCIFAR10Model\\n\\n\\n@hydra.main(config_path=\"./configs\", config_name=\"default.yaml\")\\ndef main(cfg: DictConfig) -> None:\\n\\n    if \"experiments\" in cfg.keys():\\n        cfg = OmegaConf.merge(cfg, cfg.experiments)\\n\\n    seed_everything(0)\\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = cfg.runs.gpu_id\\n\\n    if cfg.runs.logger == \"wandb\":\\n        logger = WandbLogger(name=cfg.model.classifier, project=\"cifar10\")\\n    elif cfg.runs.logger == \"tensorboard\":\\n        logger = TensorBoardLogger(cfg.train.tensorboard_dir, name=cfg.model.classifier)\\n\\n    checkpoint = ModelCheckpoint(monitor=\"acc/val\", mode=\"max\", save_last=True)\\n\\n    trainer = Trainer(\\n        fast_dev_run=cfg.runs.dev,\\n        logger=logger if not (cfg.runs.dev or cfg.runs.evaluate) else None,\\n        gpus=-1,\\n        deterministic=True,\\n        weights_summary=None,\\n        log_every_n_steps=1,\\n        max_epochs=cfg.train.num_epochs,\\n        checkpoint_callback=checkpoint,\\n        precision=cfg.runs.precision,\\n        resume_from_checkpoint=cfg.train.checkpoint,\\n    )\\n\\n    datamodule = LitCIFAR10DataModule(cfg)\\n    model = LitCIFAR10Model(cfg)\\n\\n    if cfg.runs.evaluate:\\n        model = LitCIFAR10Model.load_from_checkpoint(checkpoint_path=cfg.test.checkpoint)\\n        trainer.test(model, datamodule.test_dataloader())\\n    else:\\n        trainer.fit(model, datamodule)\\n        trainer.test()\\n\\nif __name__ == \"__main__\":\\n    main()\\nI tried using load_from_checkpoint for test, but I get the following error. How can I solve it?\\n⋊> /m/n/k/c/PyTorch-Lightning-CIFAR10 on main ⨯\\npipenv run python train.py +experiments=test_exp01 hydra.run.dir=outputs/test_exp01\\nGlobal seed set to 0\\nGPU available: True, used: True\\nTPU available: None, using: 0 TPU cores\\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\\nTraceback (most recent call last):\\n  File \"train.py\", line 48, in main\\n    model = LitCIFAR10Model.load_from_checkpoint(checkpoint_path=cfg.test.checkpoint)\\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/core/saving.py\", line 159, in load_from_checkpoint\\n    model = cls._load_model_state(checkpoint, strict=strict, **kwargs)\\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/core/saving.py\", line 199, in _load_model_state\\n    model = cls(**_cls_kwargs)\\nTypeError: __init__() missing 1 required positional argument: \\'cfg\\'\\n\\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\\n⋊> /m/n/k/c/PyTorch-Lightning-CIFAR10 on main ⨯',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ2Njgzcon': \"Hi there,\\nI am trying to implement a data module however I keep getting an error that I cannot understand. I normally setup my data module as:\\nclass dataModule(pl.LightningDataModule):\\n    def __init__(self, batch_size, csv_file, data_dir):\\n        super().__init__()\\n        self.csv_file = csv_file\\n        self.data_dir = data_dir\\n        self.batch_size = batch_size\\n        self.preprocess = None\\n        self.transform = None\\n        self.train_set = None\\n        self.val_set = None\\n        self.test_set = None\\n    \\n    def get_augmentation_transform(self):\\n        augment = tio.Compose([\\n            tio.RandomAffine(),\\n            tio.RandomFlip(p = 0.25),\\n            tio.RandomGamma(p=0.25),\\n            tio.RandomNoise(p=0.25),\\n            tio.RandomMotion(p=0.1),\\n            tio.RandomBiasField(p=0.25),\\n        ])\\n        return augment\\n\\n    def setup(self, stage=None):\\n        \\n        subjList = fmriDataset(csv_file = self.csv_file,\\n                              root_dir = self.data_dir)\\n    \\n        train_size, val_size = int(0.7 * len(subjList)), int(0.2 * len(subjList))\\n        test_size = len(subjList) - train_size - val_size\\n        \\n        if stage == 'fit' or stage is None:\\n            self.train_dataset, self.val_dataset, _ = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\\n\\n        if stage == 'test' or stage is None:\\n            _, _, self.test_dataset = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\\n\\n        augment = self.get_augmentation_transform()\\n\\n        self.train_set = tio.SubjectsDataset(self.train_dataset, transform=augment)\\n        self.val_set = tio.SubjectsDataset(self.val_dataset, transform=None)\\n        self.test_set = tio.SubjectsDataset(self.test_dataset, transform=None)\\n\\n    def train_dataloader(self):\\n        return DataLoader(self.train_set, self.batch_size, shuffle=True, num_workers=27)\\n\\n    def val_dataloader(self):\\n        return DataLoader(self.val_set, self.batch_size, num_workers=27)\\n\\n    def test_dataloader(self):\\n        return DataLoader(self.test_set, self.batch_size, num_workers=27)\\nHowever, when I call trainer.tune(model = model, datamodule = data) I get the error:\\nAttributeError: 'dataModule' object has no attribute 'test_dataset'\\n\\nHowever, if I change these lines\\nif stage == 'fit' or stage is None:\\n    self.train_dataset, self.val_dataset, _ = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\\n\\nif stage == 'test' or stage is None:\\n    _, _, self.test_dataset = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\\nto a single line:\\nself.train_dataset, self.val_dataset, self.test_dataset = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\\neverything works. Is there something basic that I have missed?\\n\\nFor completeness:\\n\\nPytorch version: 1.9.0\\nPytorch-lightning version: 1.3.7\\n\\nAnd I initialise the data module/model/trainer with:\\ndata = dataModule(data_dir = '/home/data/', csv_dir = '/home/scanList.csv', batch_size = 24)\\n\\nmodel = cnnRnnClassifier()\\n\\nearly_stop_callback = Earlystopping(\\n    monitor = 'val_loss',\\n    min_delta = 1e-4,\\n    patience = 10,\\n    Verbose = True,\\n    mode = 'min')\\n\\ntrainer = Trainer(\\n    gpus =  1,\\n    fast_dev_run = False,\\n    max_epochs = 100,\\n    weights_summary = 'full',\\n    callbacks = [early_stop_callback],\\n    auto_lr_find = True,\\n    precision = 16)\\n\\ntrainer.tune(model = model, datamodule = data)\\ntrainer.fit(model = model, datamodule = data)\\ntrainer.test(model = model, datamodule = data)\\nThanks in advance for your help!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ3MDYzcon': 'Hi! I have found a weird behavior when using ModelCheckpoint, if I have a metric that I want to save in my filename and it has a \"/\" on it it will create nested directories. For example\\ncheckpoint_callback = ModelCheckpoint(\\n        monitor=\\'val/acc\\',\\n        dirpath=checkpoints_dir,\\n        filename=\\'checkpoint_{epoch:02d}-{val/acc}\\',\\n        save_top_k=-1,\\n     )\\nThis one will create one extra folder per checkpoint:\\ncheckpoints/base_lstm/checkpoint_epoch=00-val/acc=0.04-v1.ckp\\ncheckpoints/base_lstm/checkpoint_epoch=00-val/acc=0.05-v1.ckp\\nIs there any way to make the modelcheckpoint callback store the \"val/acc\" value while not using the string \"val/acc\" to reference it? Something like:\\ncheckpoints/base_lstm/checkpoint_epoch=00-valAcc=0.04-v1.ckp\\nI think is quite standard to use \"/\" on tensorboard to be able to use the inbuilt tabs to better group metrics.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ5MjE2con': '🐛 Bug\\n\\nImport error after installing pytorch-lightning on google colab TPU instance.\\n\\nColab link: https://colab.research.google.com/drive/1ssH3PwBh6skcIze440LwHn5R5dUnih7Q?usp=sharing\\nCode to reproduce:\\n!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\\n!pip install pytorch-lightning\\n\\nimport pytorch_lightning as pl\\n\\nConsole output:\\nWARNING:root:Waiting for TPU to be start up with version pytorch-1.8...\\nWARNING:root:Waiting for TPU to be start up with version pytorch-1.8...\\nWARNING:root:TPU has started up successfully with version pytorch-1.8\\n---------------------------------------------------------------------------\\nImportError                               Traceback (most recent call last)\\n<ipython-input-2-efd8697dde72> in <module>()\\n----> 1 import pytorch_lightning as pl\\n\\n9 frames\\n/usr/local/lib/python3.7/dist-packages/torch_xla/__init__.py in <module>()\\n    126 import torch\\n    127 from ._patched_functions import _apply_patches\\n--> 128 import _XLAC\\n    129 \\n    130 \\n\\nImportError: /usr/local/lib/python3.7/dist-packages/_XLAC.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN2at11result_typeERKNS_6TensorEN3c106ScalarE\\n\\nI followed the instructions from the docs: https://pytorch-lightning.readthedocs.io/en/stable/advanced/tpu.html. I also tried xla version 1.7, 1.6, and pl version 1.1.8.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQwNDk0con': \"Hi there!\\nI am trying to build a very basic CNN for a binary classification task however I am getting an odd dimensionality issue.\\nMy CNN is:\\nclass convNet(nn.Module):\\n    def __init__(self):\\n        \\n        super().__init__()\\n        self.conv2d_1 = nn.Conv2d(193, 193, kernel_size=3)\\n        self.conv2d_2 = nn.Conv2d(193, 193, kernel_size=3)\\n        self.conv2d_3 = nn.Conv2d(193, 193, kernel_size=3)\\n        self.maxpool = nn.MaxPool2d(2)\\n    \\n    def forward(self, x):\\n        x = self.conv2d_1(x)\\n        x = F.relu(x)\\n        x = self.maxpool(x)\\n        x = self.conv2d_2(x)\\n        x = F.relu(x)\\n        x = self.maxpool(x)\\n        x = self.conv2d_3(x)\\n        return F.relu(x)\\nAnd my lightning module is:\\nclass classifier(pl.LightningModule):\\n    \\n    def __init__(self, learning_rate = float):\\n        super().__init__()\\n        \\n        self.learning_rate = learning_rate\\n        self.cnn =covNet()\\n        self.flat = nn.Flatten()\\n        \\n        self.fc1 = nn.Linear(450076, 100)\\n        self.fc2 = nn.Linear(100, 10)\\n        self.fc3 = nn.Linear(10, 1)\\n        \\n        self.dropout = nn.Dropout(p = 0.2)\\n        \\n        self.criterion = nn.BCEWithLogitsLoss()\\n        \\n        self.accuracy = tm.Accuracy()\\n\\n    def prepare_batch(self, batch):\\n        img = batch['image'][tio.DATA]\\n        img = torch.squeeze(img)\\n        diagnosis = batch['diagnosis']\\n        return img, diagnosis\\n\\n    def forward(self, x):\\n        cnn_out = self.cnn(x)\\n        flat = self.flat(cnn_out)\\n        fc1_out = self.dropout(F.relu(self.fc1(flat)))\\n        fc2_out = self.dropout(F.relu(self.fc2(fc1_out)))\\n        fc3_out = F.relu(self.fc3(fc2_out))\\n        return fc3_out\\n    \\n    def training_step(self, batch, batch_idx):\\n        x, y = self.prepare_batch(batch)\\n        y = y.view(y.size(0), -1)\\n        y = y.type(torch.float)\\n        y_hat = self.forward(x)\\n        train_loss = self.criterion(y_hat, y)\\n        self.log('train_loss', train_loss, prog_bar = True)\\n        return train_loss\\n    \\n    def validation_step(self, batch, batch_idx):\\n        x, y = self.prepare_batch(batch)\\n        y = y.view(y.size(0), -1)\\n        y = y.type(torch.float)\\n        y_hat = self.forward(x)\\n        val_loss = self.criterion(y_hat, y)\\n        self.log('val_loss', val_loss, prog_bar = True)\\n        return val_loss\\n        \\n    def test_step(self, batch, batch_idx):\\n        x, y = self.prepare_batch(batch)\\n        y = y.view(y.size(0), -1)\\n        y = y.type(torch.float)\\n        y_hat = self.forward(x)\\n        testAcc = self.accuracy(y_hat, y)\\n        self.log_dict({'test_acc': testAcc})\\n        return testAcc\\n\\n    def configure_optimizers(self):\\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\\n        return optimizer\\nThe odd part is that the validation sanity completes and I get 75% through the first epoch when the validation loop starts before I get the error:\\n\\nAnd I have checked that all my inputs have the same dimensions of [193, 229, 193]. Have I missed something obvious? (sorry if it is obvious)\\nAny help would be greatly appreciated!\\nFor completeness:\\n\\npytorch version: 1.9.0\\npytorch lightning version: 1.3.7\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQwNTU3con': 'Hello i managed to implement training accuracy per epoch with training_epoch_end but i want to do the same with validation accuracy with validation_epoch_end but i get an error of \"too many indices for tensor of dimension 0\" when train.fit() . I am using pytorch-lightning==1.2.8  . Thanks in advance.\\nError is:\\n<ipython-input-31-41c968828bca> in validation_epoch_end(self, outputs)\\n     39     predictions = []\\n     40     for output in outputs:\\n---> 41       for out_labels in output[\"labels\"].detach().cpu():\\n     42         labels.append(out_labels)\\n     43       for out_predictions in output[\"predictions\"].detach().cpu():\\n\\nIndexError: too many indices for tensor of dimension 0\\n\\ndef validation_step(self, batch, batch_idx):\\n   input_ids = batch[\"input_ids\"]\\n   attention_mask = batch[\"attention_mask\"]\\n   labels = batch[\"labels\"]\\n   loss, outputs = self(input_ids, attention_mask, labels)\\n   self.log(\"val_loss\", loss, prog_bar=True, logger=True)\\n   return loss\\n\\n def validation_epoch_end(self, outputs):\\n\\n   labels = []\\n   predictions = []\\n   for output in outputs:\\n     for out_labels in output[\"labels\"].detach().cpu():\\n       labels.append(out_labels)\\n     for out_predictions in output[\"predictions\"].detach().cpu():\\n       predictions.append(out_predictions)\\n\\n   labels = torch.stack(labels).int()\\n   predictions = torch.stack(predictions)\\n\\n   validation_acc = accuracy(predictions, labels)\\n   self.logger.experiment.add_scalar(\"Validation Accuracy\", validation_acc, self.current_epoch)',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQxODI1con': 'I need to plot confusion_matrix in tensotboard ,how to do it ?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQzODk2con': 'The device of the metric return by validation_step is GPU, related code is\\ndef validation_step(self, batch, batch_idx):\\n    x, y = batch\\n    if y.device != self.device:\\n        y = y.to(self.device)\\n    y_hat = self(x)\\n    loss = self.loss(y_hat, y)    # loss.device is cuda.\\n    self.log(\\'valid loss\\', loss.item())\\n    return loss\\nAfter an epoch of validation compeleted when using earlystopping, follow error occured:\\n  File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 871, in run_train\\n    self.train_loop.run_training_epoch()\\n  File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\training_loop.py\", line 584, in run_training_epoch\\n    self.trainer.run_evaluation(on_epoch=True)\\n  File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 1011, in run_evaluation\\n    self.evaluation_loop.on_evaluation_end()\\n  File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\evaluation_loop.py\", line 102, in on_evaluation_end\\n    self.trainer.call_hook(\\'on_validation_end\\', *args, **kwargs)\\n  File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 1228, in call_hook\\n    trainer_hook(*args, **kwargs)\\n  File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\callback_hook.py\", line 227, in on_validation_end\\n    callback.on_validation_end(self, self.lightning_module)\\n  File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\callbacks\\\\early_stopping.py\", line 173, in on_validation_end\\n    self._run_early_stopping_check(trainer)\\n  File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\callbacks\\\\early_stopping.py\", line 193, in _run_early_stopping_check\\n    should_stop, reason = self._evalute_stopping_criteria(current, trainer)\\n  File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\callbacks\\\\early_stopping.py\", line 226, in _evalute_stopping_criteria\\n    elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):\\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\\n\\nThis error didn\\'t appear until I updated the version of pytorch_lightning.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDU0NTI5con': \"It's weired. I have a single machine with 8 GPUSs and now 0~4 are full load.\\nI would like to use parallel training on 5 6 7 GPUs but cannot assign the task to them.\\nSome of my codes:\\n`    parser = argparse.ArgumentParser(description='Solver')\\nparser.add_argument('--config', required=True, type=str)\\nfargs = parser.parse_args()\\nargs = parse_config(fargs.config)\\ndata = DDPMData(args.Data)\\ndata.train_dataloader()\\ndata.test_dataloader()\\n\\nnum_cls = data.n_classes\\nshape = data.data_shapes\\nargs.Model.Unet.kwargs.num_classes = num_cls\\nargs.Model.DiscreteDiffusion.kwargs.num_class = num_cls\\nargs.Model.DiscreteDiffusion.kwargs.shape = shape\\nmodel = DDDPM(args)\\n\\nwandb_logger = WandbLogger()\\n\\ncallbacks = []\\ncallbacks.append(ModelCheckpoint(save_last=True,every_n_train_steps=600))\\ncallbacks.append(LearningRateMonitor(logging_interval='step'))\\n\\ntrainer = pl.Trainer(callbacks=callbacks,max_steps=args.max_steps, \\n                accelerator='dp', gpus=[5,6,7],\\n                logger = wandb_logger, check_val_every_n_epoch=120,\\n                num_sanity_val_steps=0)\\ntrainer.fit(model, data)\\n\\n`\\nAnd now nvidia-smi shows:\\n+-----------------------------------------------------------------------------+\\n| NVIDIA-SMI 470.42.01    Driver Version: 470.42.01    CUDA Version: 11.4     |\\n|-------------------------------+----------------------+----------------------+\\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\\n|                               |                      |               MIG M. |\\n|===============================+======================+======================|\\n|   0  NVIDIA GeForce ...  On   | 00000000:04:00.0 Off |                  N/A |\\n| 63%   72C    P2   151W / 200W |   7997MiB /  8119MiB |     85%      Default |\\n|                               |                      |                  N/A |\\n+-------------------------------+----------------------+----------------------+\\n|   1  NVIDIA GeForce ...  On   | 00000000:06:00.0 Off |                  N/A |\\n| 71%   77C    P2   134W / 200W |   8109MiB /  8119MiB |     98%      Default |\\n|                               |                      |                  N/A |\\n+-------------------------------+----------------------+----------------------+\\n|   2  NVIDIA GeForce ...  On   | 00000000:07:00.0 Off |                  N/A |\\n| 82%   83C    P2   143W / 200W |   7405MiB /  8119MiB |     89%      Default |\\n|                               |                      |                  N/A |\\n+-------------------------------+----------------------+----------------------+\\n|   3  NVIDIA GeForce ...  On   | 00000000:08:00.0 Off |                  N/A |\\n| 80%   83C    P2   158W / 200W |   7263MiB /  8119MiB |     59%      Default |\\n|                               |                      |                  N/A |\\n+-------------------------------+----------------------+----------------------+\\n|   4  NVIDIA GeForce ...  On   | 00000000:0C:00.0 Off |                  N/A |\\n| 78%   82C    P2   148W / 200W |   5719MiB /  8119MiB |    100%      Default |\\n|                               |                      |                  N/A |\\n+-------------------------------+----------------------+----------------------+\\n|   5  NVIDIA GeForce ...  On   | 00000000:0D:00.0 Off |                  N/A |\\n|  0%   24C    P8     7W / 200W |      4MiB /  8119MiB |      0%      Default |\\n|                               |                      |                  N/A |\\n+-------------------------------+----------------------+----------------------+\\n|   6  NVIDIA GeForce ...  On   | 00000000:0E:00.0 Off |                  N/A |\\n|  0%   34C    P8     8W / 200W |      4MiB /  8119MiB |      0%      Default |\\n|                               |                      |                  N/A |\\n+-------------------------------+----------------------+----------------------+\\n|   7  NVIDIA GeForce ...  On   | 00000000:0F:00.0 Off |                  N/A |\\n|  0%   25C    P8     7W / 200W |      4MiB /  8119MiB |      0%      Default |\\n|                               |                      |                  N/A |\\n+-------------------------------+----------------------+----------------------+\\n+-----------------------------------------------------------------------------+\\n| Processes:                                                                  |\\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\\n|        ID   ID                                                   Usage      |\\n|=============================================================================|\\n|    0   N/A  N/A      4082      C   python                           7993MiB |\\n|    1   N/A  N/A     13619      C   python                           8105MiB |\\n|    2   N/A  N/A      4082      C   python                           7401MiB |\\n|    3   N/A  N/A      4082      C   python                           7259MiB |\\n|    4   N/A  N/A     24206      C   python                           5715MiB |\\n+-----------------------------------------------------------------------------+\\nand when I run the codes, it will assign task to GPU:2 , I don't know why..\\n RuntimeError: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 2; 7.93 GiB total capacity; 117.19 MiB already allocated; 45.50 MiB free; 120.00 MiB reserved in total by PyTorch)\\nAny idea..?\\n##############################\\ntorch = 1.7.1\\npl = 1.3.8\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDU2MjU5con': \"I am trying to create multiple model using loop as below.\\nfor client in clients:\\n    t.manual_seed(10)\\n    client['model'] = LinearNN(learning_rate = args.lr, i_s = args.input_size,  h1_s = args.hidden1, h2_s = args.hidden2, n_c = args.output, client=client)\\n    client['optim'] = optim.Adam(client['model'].parameters(), lr= args.lr)\\n\\nHowever, trainer.fit() is an async method. To train multiple models, I need to put trainer.fit() in a loop as follows\\nfor client in clients:\\n     trainer = pl.Trainer(\\n     max_epochs=args.epochs+1,\\n     progress_bar_refresh_rate=20,\\n     )\\n     trainer.fit(client['model'])\\n\\nAs this is an async method, it gives an error\\n\\nAttributeError: can't set attribute\\n\\nas it doesn't wait for finishing trainer.fit().\\nIs there any way to do that?\\nThanks in advance.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDU5MDQxcon': \"This is my training and validation steps  \\n def training_step(self,batch,batch_idx):\\n    self.log('train/loss', loss, on_epoch=True,prog_bar=True)\\n    self.log('train/iou', iou, on_epoch=True,prog_bar=True)\\n    return loss  \\n\\ndef validation_step(self,batch,batch_idx):\\n    self.log('val/loss', loss, on_epoch=True,prog_bar=True)\\n    self.log('val/iou', iou, on_epoch=True,prog_bar=True)\\n    return loss\\n\\n\\nThis is what logs displayed\\n360/361 [16:07<00:02, 2.69s/it, loss=0.148, v_num=0, val/loss=0.664, val/iou=0.234, train/loss_step=0.129, train/iou_step=0.379, train/loss_epoch=0.210, train/iou_epoch=0.371]\\n\\nI am confused what is the difference between loss and val/loss? If there is val/loss which makes sense as I output it, then what is loss. There is val/iou, but there is not any train/iou, and train/loss as in val case. why? Is train/iou_epoch is for epoch or for all previous epochs?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDU5Mzc4con': \"Hi, I'm trying to use integrate pytorch lightning into my current pipeline. But I'm having some difficulties in using multiple dataloaders. In my current use case, let's say I have 10 dataloaders in my pipeline, but for each training step, I'm only sampling data from 5 of them. Is it doable in pytorch lightning? I can sample from 10 dataloaders each step but that would be a waste to system IO and GPU memory. Thanks for any help!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDU5NjMwcon': 'Hei everyone\\nI am working on a small framework based on PyTorch lightning to perform some experiments.\\nOur models always are a composition out of two networks (backbone and some header) as we want to be able to replace them independently.\\nclass EncoderHeaderModel(pl.LightningModule):\\n\\n        def __init__(self, backbone: Union[pl.LightningModule, torch.nn.Module],\\n                 header: Union[pl.LightningModule, torch.nn.Module]):\\n        super().__init__()\\n\\n        # sanity check if the last layer of the backbone is compatible with the first layer of the header\\n\\n        self.backbone = backbone\\n        self.header = header\\n\\n    def forward(self, x):\\n        x = self.backbone(x)\\n        x = self.header(x)\\n        return x\\n\\nNow we have the problem with saving the state_dict of these two models separately. To store the whole model we are using the model_checkpoint callback which works fine.\\nIs there an easy way to save the models each time model_checkpoint would save the whole model (I am already experimenting with a subclass of model_checkpoint)? Or should we after the training just extract the state_dicts out of the checkpoint? Any idea how to make this in a \"clean\" way?\\nThanks',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDUxOTU2con': 'I am training an image classifier on tpu and am getting errors after execution of the first epoch.\\nhttps://colab.research.google.com/drive/1Lgz0mF6UiLirsDltPQH5HtctmVvb7gHm?usp=sharing',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDY2OTI3con': 'Just wondering when backward gets called when you are accumulating over (say 8) batches. I had put a breakpoint in on_after_backward() and that seemed to be only getting called on the 8th iteration of training. According to this answer, in order to save on GPU memory, it seems its best to call loss.backward() on each iteration.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDY5NjEycon': 'Is it possible to use shared filesystem for DDP init_group in pytorch lighting? If so how what should I do to the Trainer?\\nThanks!',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDYzNjQycon': 'I have a loss module which is part of my lightning module with its own inner pretrained vgg network.\\nThe problem comes when I am trying to use the checkpoint (that is saved automatically) to resume training or to test my model.\\nThen I get an error  unexpected key(s) in state_dict pytorch lightning which points to the keys of the network which is part of the loss function.\\nIs there a way to load my model properly?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDc2ODEzcon': 'I know that parameters are indirectly synced in multi-gpu via grad-syncing.  But how to sync buffers that are not updated via gradient?\\nI find that I can use all_reduce() or all_gather() method manually in ddp doc, but what pytorch-lightning does under the hood? Does it sync buffers automatically or offer an interface to do that?\\nConsidering pl handles single and multi-gpu without any need to change Modules, do it manually will result in ugly judgement code whether the training is in ddp mode inside an independent module.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDc3MDE4con': \"I'm migrating my repository to pytorch-lightning and I get the following error:\\nRuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.\\nThe CNNLSTM model seems to be the problem, what should I do?\\n[My repository]\\nKeiku/Action-Recognition-CNN-LSTM: Action recognition tutorial using UCF-101 dataset. https://github.com/Keiku/Action-Recognition-CNN-LSTM\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDc5ODU4con': 'I have a training regime that is disk-speed bound, because instances are loaded from disk.\\nI would like to train multiple models with one dataloader. That way, I can do model selection over many models, but reduce the number of disk reads. Is this possible?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDcwNjIzcon': \"Hi!\\nI'm training my neural network with Pytorch Lightning and MONAI (a PyTorch-based framework for deep learning in healthcare imaging). Because my training dataset is small, I need to perform data augmentation using random transforms.\\nContext\\nI use MONAI's CacheDataset (basically, a PyTorch Dataset with cache mechanism). From what I understood, , CacheDataset will cache the consistent result of the transforms until the first random transform, then reuse the cache content and apply the remaining random transforms (such as Gaussian noise, random intensity shift, etc.) for every epoch. As a result, the dataloader trains the network with a different dataset at each epoch.\\nIn the video presenting the reload_dataloaders_every_epoch flag, William Falcon mentions that:\\n\\nBy default, Lighthning only loads your dataset once (so that you don't occur the cost of downloading that data and process it every single time). On every epoch, Lightning shuffles the data and feeds it into the training loop.\\n\\nMy questions\\nDid Lightning add a cache mechanism to load the data once? Must I use the reload_dataloader_every_epoch flag to do data augmentation or else my random transforms will only be applied once (therefore defeating my data augmentation goal)?\\nThanks in advance for your explanation :)\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDcwNzE3con': 'I am looking to implement n parallel independent ensembles. My idea is the following:\\nclass DeepEnsemble(LightningModule):\\n    def __init__(self, cfg):\\n        super().__init__(cfg)\\n        self.net = nn.ModuleList([configure_network(self.cfg) for _ in range(self.cfg.METHOD.ENSEMBLE)])\\n\\n    def configure_optimizers(self):\\n        return [torch.optim.Adam(net.parameters(), lr=self.cfg.SOLVER.LR) for net in self.net]\\n\\n    def forward(self, x):\\n        x = [net.forward(x) for net in self.net]\\n        return x\\n\\n    def training_step(self, batch, batch_idx, optimizer_idx):\\n        image, label = batch[\"image\"], batch[\"label\"]\\n        logits = self.forward(image)\\n        loss = [self.criterion(logit, label) for logit in logits]\\n\\n        mean_logit = torch.stack(logits, dim=-1).mean(dim=-1)\\n\\n        metrics = self.log_metrics(mean_logit, label, \\'train\\')\\n\\n        return loss\\n\\n    def validation_step(self, batch, batch_idx):\\n        image, label = batch[\"image\"], batch[\"label\"]\\n        logits = self.forward(image)\\n\\n        mean_logit = torch.stack(logits, dim=-1).mean(dim=-1)\\n\\n        metrics = self.log_metrics(mean_logit, label, \\'val\\')\\n\\n        return metrics[self.cfg.CKPT.MONITOR]\\n\\n    def test_step(self, batch, batch_idx):\\n        pass\\nI have n networks and n optimisers. My solution works (I think), but the training_step gets called with a new optimizer_idx every time, which indicates that Pytorch Lightning expects to only train 1 network per training_step. Therefore, my solution is very inefficient, because n^2 forward passes are executed instead of n. If I only do the forward pass for the ith network, then I can\\'t compute metrics based on all ensembles (e.g. disagreement) unless I write some very inelegant if statements.\\nIn addition It would be nice to have all forward passes done in parallel instead of sequential like in this list comprehension.\\nSo what is the most elegant way to train an ensemble and still access all predictions for metric logging together?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDcxODUxcon': 'Hi,\\nI’m trying to train a model on 2 GPUs. I do this by specifying Trainer(..., gpus=2). ddp_spawn should automatically be selected for the method, but I instead get the following message + error:\\nUserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(\\naccelerator=\"dp\"|\"ddp\"|\"ddp2\")`. Setting `accelerator=\"ddp_spawn\"` for you.\\n  \\'You requested multiple GPUs but did not specify a backend, e.g.\\'\\nGPU available: True, used: True\\nTPU available: False, using: 0 TPU cores\\nTraceback (most recent call last):\\n File \"train.py\", line 186, in <module>\\n    main(sys.argv[1:])\\n  File \"train.py\", line 173, in main\\n    print(f\"Logs for this experiment are being saved to {trainer.log_dir}\")\\n  File \".../pypi__pytorch_lightning_python3_deps/pytorch_lightning/trainer/properties.py\", line 137, in log_dir\\n    dirpath = self.accelerator.broadcast(dirpath)\\n  File \".../pypi__pytorch_lightning_python3_deps/pytorch_lightning/accelerators/accelerator.py\", line 436, in broadcast\\n    return self.training_type_plugin.broadcast(obj, src)\\n  File \".../pypi__pytorch_lightning_python3_deps/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 275, in broadcast\\n    return self.dist.broadcast(obj)\\n  File \".../pypi__pytorch_lightning_python3_deps/pytorch_lightning/distributed/dist.py\", line 33, in broadcast\\n    broadcast_object_list(obj, 0, group=group or _group.WORLD)\\n  File \".../pypi__torch_python3_deps/torch/distributed/distributed_c10d.py\", line 1700, in broadcast_object_list\\n    my_rank = get_rank()\\n  File \".../pypi__torch_python3_deps/torch/distributed/distributed_c10d.py\", line 725, in get_rank\\n    default_pg = _get_default_group()\\n  File \".../pypi__torch_python3_deps/torch/distributed/distributed_c10d.py\", line 358, in _get_default_group\\n    raise RuntimeError(\"Default process group has not been initialized, \"\\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\\n\\nI looked at the source code of ddp_spawn and it looks like it should print out a message when initializing ddp, but it didn’t.\\nCould I please have advice on how to correct this error.\\nThank you!',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDczMDQycon': 'I\\'m getting the following error after setting up an EC2 instance p3.8xlarge (so 4 GPUs) and setting gpus=4:\\n/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:524: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator=\"dp\"|\"ddp\"|\"ddp2\")`. Setting `accelerator=\"ddp_spawn\"` for you.\\n  \\'You requested multiple GPUs but did not specify a backend, e.g.\\'\\nGPU available: True, used: True\\nTPU available: False, using: 0 TPU cores\\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\\nTraceback (most recent call last):\\n  File \"train.py\", line 79, in <module>\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/tuner/tuning.py\", line 197, in lr_find\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 688, in tune\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/tuner/tuning.py\", line 54, in _tune\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/tuner/lr_finder.py\", line 250, in lr_find\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/tuner/tuning.py\", line 64, in _run\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 758, in _run\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 799, in dispatch\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 96, in start_training\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 122, in start_training\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 179, in start_processes\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 112, in start\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/context.py\", line 284, in _Popen\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 20, in __init__\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/reduction.py\", line 60, in dump\\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\", line 328, in reduce_storage\\nRuntimeError: unable to open shared memory object </torch_91130_1372465664> in read-write mode\\nMy code runs fine on a single gpu instance. Any idea what I need to look at here?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDg2Mzk2con': \"Hi,\\nI'm using an IterableDataset and need to manually split up the data depending on the GPU (I'm using DDP). The lightning docs say that replace_sampler_ddp doesn't automatically handle IterableDatasets, so I need to do this myself.\\nIs it possible to access the global rank or world size in a dataset (aka are there any utility functions available for this purpose)? I saw _get_rank here, but ideally I wouldn't use a private helper function.\\nI tried using:\\nfile_paths_filtered = file_paths[\\n    torch.distributed.get_rank() :: torch.distributed.get_world_size()\\n]\\n\\ninside the IterableDataset, but it fails because the default process group has not been setup yet (which makes sense):\\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\\n\\nIs there any way to access the global rank/world size? If not, how would you recommend I check which GPU the dataset is on without causing an issue with init_process_group not being called yet (should I maybe call this manually)?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNDg4MDgycon': 'Say I have a callback that changes a hyper-parameter of the underlying model before every epoch.\\nclass ChangeHyperParam(pl.Callback):\\n    def on_train_epoch_start(self, trainer, pl_module):\\n         pl_module.hyper_param1 = func(trainer.current_epoch)\\nWhat happens when I use this callback in DDP mode. Will this call be called on every GPU automatically? Or do I have to do something else?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDg4Mjgxcon': 'Hi everyone, I wonder what is the best practice to share a massive CPU tensor over multiple processes in pytorch-lightning DDP mode (read-only + single machine)?\\nI think torch.Storage.from_file with share=True may suit my needs, but I can’t find a way to save storage and read it as a tensor. (see here for details)\\nI also tried to copy training data to /dev/shm (reference) and run DDP with 8 GPUs, but nothing is different. The memory usage when running with 8 GPUs is the same as before, but I tested with a single process, loading the dataset may occupy more than 1 GB of memory. Am I missing something here?\\nFor torch.shared_memory, how should I pass the same reference to all processes in pytorch-lightning pure DDP mode?\\nThank you.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDg5NzQ0con': 'Hello,\\nI have a single GPU, but I would like to spawn multiple replicas on that single GPU and train a model with DDP. Of course, each replica would have to use a smaller batch size in order to fit in memory. (For my use case, I am not interested in having a single replica with a large batch size).\\nI tried to pass --gpus \"0,0\" to the Lightning Trainer, and it managed to spawn two processes on the same GPU:\\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\\ninitializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2\\n\\nBut in the end it crashed with RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:911, invalid usage.\\nPlease, is there any way to split a single GPU into multiple replicas with Lightning?\\nThanks!\\nP.S.: Ray has a really nice support for fractional GPUs: https://docs.ray.io/en/master/using-ray-with-gpus.html#fractional-gpus. I\\'ve never used them with Lightning, but maybe it could be a workaround?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDgwNTAwcon': 'I am training 5-fold CV with PyTorch Lightning in a for loop. I am also logging all the results to wandb. I want wanbd to reinitalize the run after each fold, but it seems to continue with the same run and it logs all the results to the same run. I also tried passing kwargs in the WandbLogger as mentioned in the docs here, with no luck.\\nHere\\'s a pseudo code of it:\\ndef run(fold):\\n    kwargs = {\\n        \"reinit\": True,\\n        \"group\": f\"{CFG[\\'exp_name\\']}\"\\n    }\\n    wandb_logger = WandbLogger(project=\\'<name>\\', \\n                        entity=\\'<entity>\\', \\n                        config = CFG,\\n                        name=f\"fold_{fold}\",\\n                        **kwargs\\n            )\\n    trainer = Trainer(\\n        precision=16,\\n        gpus=1,\\n        fast_dev_run=False,\\n        callbacks = [checkpoint_callback],\\n        logger=wandb_logger,\\n        progress_bar_refresh_rate=1,\\n        max_epochs=2,\\n        log_every_n_steps=1\\n\\n    )\\n\\n    trainer.fit(\\n        lit_model,\\n        data_module\\n    )\\n\\nif __name__ == \"__main__\":\\n    for fold in range(5):\\n        run(fold)',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDkwMDQ4con': 'class MyNet(pl.LightningModule):\\n    def __init__(self):\\n        self.m1 = MyMod1()\\n        self.m2 = MyMod2()\\nIf I implement different configure_optimizers for different submodules MyMod(also pl.LightningModule), is it correct that parameters in each MyMod will be updated by their own optimizers returned by configure_optimizers? If I only implement configure_optimizers for the top module, is it correct that parameters in submodules will be optimized by the same optimizer returned by configure_optimizers of the top module?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNDkwOTk3con': 'I have some preprocessing logic to run during datamodule setup process, but only in certain situations (mostly to try out different preprocessing steps while experimenting, but at other times, its due to the model I am using with the datamodule).\\nIs there a way to specify a set of data preprocessing steps to perform using callbacks? Reading the documentation, I could not find the correct hook to use.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTA0NjA3con': 'I have a pytorch lightning module that includes the following section:\\n`def training_step(self, batch, batch_idx):\\nlosses, tensors = self.shared_step(batch)\\nreturn losses\\ndef validation_step(self, batch, batch_idx):\\n    losses, tensors = self.shared_step(batch)\\n    return losses, tensors\\n\\ndef training_epoch_end(self, losses):\\n    my_function(losses)\\n\\ndef validation_epoch_end(self, outputs):\\n    my_function2(outputs)`\\n\\nThe losses variable is a dictionary that includes the key \"loss\". The input to validation_epoch_end() is a list of (losses, tensors) tuples from each of the batches as expected. The input to training_epoch_end() is a list of the correct length (the number of batches), but every element is the same losses dictionary from the final batch.\\nHow can I input the losses from every train batch into the training_epoch_end() method (like the inputs to validation_epoch_end())?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTA2MDQ1con': \"Hey gang!\\nI have written an Encoder model and a decoder model and I want to train them separately.\\nclass Decoder(pl.LightningModule):\\n    def __init__(self, encoder_model):#visualize_latent):\\n        super().__init__()\\n        self.encoder_model = encoder_model\\n\\nHowever, when I give my Decoder an Encoder hyperparameter, how do I make sure it will not be trained?\\nI actually asked that question before, but it wasn't answered for a while and I do not find sufficient support in the docs.\\nThanks in advance!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNTA3ODQ0con': 'I want to test again with ckpt. However I found Trainer resume_from_checkpoint only work for fit. Is there any particular reason for this?\\n\\n  \\n    \\n      pytorch-lightning/pytorch_lightning/trainer/trainer.py\\n    \\n    \\n        Lines 840 to 841\\n      in\\n      f3442db\\n    \\n  \\n  \\n    \\n\\n        \\n          \\n           if self.state.fn == TrainerFn.FITTING: \\n        \\n\\n        \\n          \\n               self.checkpoint_connector.resume_start() \\n        \\n    \\n  \\n\\n\\nIn [1]: from pytorch_lightning import seed_everything, Trainer\\n\\nIn [2]: from src import MMTSMatcher, WDCDataModule\\n\\nIn [3]: trainer = Trainer(resume_from_checkpoint=\"tests/mmtsmatcher_wdcdatamodule/shoes_small_false_32_0808-182020/checkpoints/epoch=05-valid_f1=81.22%.ckpt\", gpus=[4])\\nGPU available: True, used: True\\nTPU available: False, using: 0 TPU cores\\nIPU available: False, using: 0 IPUs\\n\\nIn [4]: trainer\\nOut[4]: <pytorch_lightning.trainer.trainer.Trainer at 0x7f2345178fa0>\\n\\nIn [5]: model = MMTSMatcher(model_name=\"bert-base-uncased\")\\n\\nIn [6]: data = WDCDataModule(training_size=\"small\", cate=\"shoes\")\\n\\nIn [7]: data\\nOut[7]: <src.datamodules.wdcdatamodule.WDCDataModule at 0x7f22ccd258e0>\\n\\nIn [10]: trainer.test(model, data)',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTA5MDI4con': \"https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html?highlight=on_epoch%20#pytorch_lightning.core.lightning.LightningModule.log.params.on_epoch\\n\\nI'm using horovod to train the model. I wonder if on_step and on_epoch average the metrics across all GPUs automatically. In other words, do we need to explicitly average the metrics in functions like training_epoch_end and validation_epoch_end?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNTAzNDg3con': 'The default implementation for these is logging a warning that nothing is implemented. \\n  \\n    \\n      pytorch-lightning/pytorch_lightning/core/hooks.py\\n    \\n    \\n         Line 529\\n      in\\n      963c267\\n    \\n  \\n  \\n    \\n\\n        \\n          \\n           rank_zero_warn(\"`train_dataloader` must be implemented to be used with the Lightning Trainer\") \\n        \\n    \\n  \\n\\n\\nWhy isn’t the default implementation to raise a NotImplementedError? This would make errors much clearer in case users forget to override these hooks',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTAzNTI2con': 'The decorator here is leaky: \\n  \\n    \\n      pytorch-lightning/pytorch_lightning/core/decorators.py\\n    \\n    \\n        Lines 73 to 108\\n      in\\n      963c267\\n    \\n  \\n  \\n    \\n\\n        \\n          \\n           def parameter_validation(fn: Callable) -> Callable: \\n        \\n\\n        \\n          \\n               \"\"\" \\n        \\n\\n        \\n          \\n               Validates that the module parameter lengths match after moving to the device. It is useful \\n        \\n\\n        \\n          \\n               when tying weights on TPU\\'s. \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n               Args: \\n        \\n\\n        \\n          \\n                   fn: ``model_to_device`` method \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n               Note: \\n        \\n\\n        \\n          \\n                   TPU\\'s require weights to be tied/shared after moving the module to the device. \\n        \\n\\n        \\n          \\n                   Failure to do this results in the initialization of new weights which are not tied. \\n        \\n\\n        \\n          \\n                   To overcome this issue, weights should be tied using the ``on_post_move_to_device`` model hook \\n        \\n\\n        \\n          \\n                   which is called after the module has been moved to the device. \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n               See Also: \\n        \\n\\n        \\n          \\n                   - `XLA Documentation <https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks>`_ \\n        \\n\\n        \\n          \\n               \"\"\" \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n               @wraps(fn) \\n        \\n\\n        \\n          \\n               def inner_fn(self, *args, **kwargs): \\n        \\n\\n        \\n          \\n                   pre_layer_count = len(list(self.model.parameters())) \\n        \\n\\n        \\n          \\n                   module = fn(self, *args, **kwargs) \\n        \\n\\n        \\n          \\n                   self.model.on_post_move_to_device() \\n        \\n\\n        \\n          \\n                   post_layer_count = len(list(self.model.parameters())) \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n                   if not pre_layer_count == post_layer_count: \\n        \\n\\n        \\n          \\n                       rank_zero_warn( \\n        \\n\\n        \\n          \\n                           f\"The model layers do not match after moving to the target device.\" \\n        \\n\\n        \\n          \\n                           \" If your model employs weight sharing on TPU,\" \\n        \\n\\n        \\n          \\n                           \" please tie your weights using the `on_post_move_to_device` model hook.\\\\n\" \\n        \\n\\n        \\n          \\n                           f\"Layer count: [Before: {pre_layer_count} After: {post_layer_count}]\" \\n        \\n\\n        \\n          \\n                       ) \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n                   return module \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n               return inner_fn \\n        \\n    \\n  \\n\\n\\nIt assumes it is called with the model_to_device method, and that self has access to model which implements on_post_move_to_device and has parameters defined.\\nThe hook here: \\n  \\n    \\n      pytorch-lightning/pytorch_lightning/core/hooks.py\\n    \\n    \\n        Lines 341 to 355\\n      in\\n      963c267\\n    \\n  \\n  \\n    \\n\\n        \\n          \\n               def on_post_move_to_device(self) -> None: \\n        \\n\\n        \\n          \\n                   \"\"\" \\n        \\n\\n        \\n          \\n                   Called in the ``parameter_validation`` decorator after :meth:`~pytorch_lightning.core.LightningModule.to` \\n        \\n\\n        \\n          \\n                   is called. This is a good place to tie weights between modules after moving them to a device. Can be \\n        \\n\\n        \\n          \\n                   used when training models with weight sharing properties on TPU. \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n                   Addresses the handling of shared weights on TPU: \\n        \\n\\n        \\n          \\n                   https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n                   Example:: \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n                       def on_post_move_to_device(self): \\n        \\n\\n        \\n          \\n                           self.decoder.weight = self.encoder.weight \\n        \\n\\n        \\n          \\n            \\n        \\n\\n        \\n          \\n                   \"\"\" \\n        \\n    \\n  \\n\\n\\nis only called by the TPU backend. Is it intended to be called by other plugins?\\nSince this is under the core/decorators.py, one might assume that this is more general. Should this be an implementation detail of the TPU plugins instead? @kaushikb11',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTE0MTA0con': 'I want to add noise to the gradients in pytorch lightning . Specifically, something similar to this paper: https://arxiv.org/pdf/1511.06807.pdf . Basically, I would compute the gradients and before the call to backward, i want to add noise.\\nWhat is the best way to achieve this in pytorch lightning?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTE2MjI1con': \"In my custom pl.LightningModule, I try to specify two learning rate schedulers (warming up in the first epoch, and multi-step scheduler in the following epochs) for 1 optimizer as follows:\\n    def configure_optimizers(self):\\n        args = self.args\\n\\n        optimizer = torch.optim.SGD(self.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\\n\\n        warmup_iters = min(1000, self.num_batches - 1)\\n        warmup_scheduler = {\\n                    'scheduler': utils.warmup_lr_scheduler(optimizer, warmup_iters, 1./1000),\\n                    'name': 'learning_rate',\\n                    'interval':'step',\\n                    'frequency': 1\\n                }\\n\\n        lr_scheduler = {\\n            'scheduler': torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.lr_steps, gamma=args.lr_gamma),\\n            'name': 'learning_rate',\\n            'interval':'epoch',\\n            'frequency': 1\\n        }\\n        return [optimizer], [warmup_scheduler, lr_scheduler]\\n\\nHowever, it seems like the learning rate in the optimizer only controlled by the first warmup_scheduler.\\nSo how can I create 2 learning rate schedulers for 1 optimizer?\\nMany thanks.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNTE3NDMwcon': 'I am fine-tuning hugging face transformer models, essentially exactly as shown in the following example found in the pytorch lightning docs:\\nhttps://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/text-transformers.html\\nWhere we instantiate the LightningModule doing something like this:\\nclass GLUETransformer(LightningModule):\\n\\n    def __init__(self, ... ):\\n        super().__init__()\\n        self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\\n        self.model = AutoModelForSequenceClassification.from_pretrained(\\n            model_name_or_path, config=self.config\\n        )\\nBut I have been confused about how I should be saving and loading checkpoints.\\nWhen saving checkpoints, should I be using\\nmymodel.model.save_pretrained(\"model_save_dir\"),\\nand reloading from this checkpoint using\\nAutoModelForSequenceClassification.from_pretrained(\"model_save_dir\"),\\nor saving with\\ntrainer.save_checkpoint(\"model_save_dir/checkpoint.ckpt\"),\\nand reloading with\\nGLUETransformer.load_from_checkpoint(\"model_save_dir/checkpoint.ckpt\")?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTE3NTExcon': 'how can i solve this problem, my net is DB,  data is coco text det\\nthis is error log\\nFile \"/home/cattree/PycharmProjects/torch-ocr/BoatNumber/ocr_det/train/train.py\", line 38, in <module>\\n    trainer.fit(model, data)\\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\\n    self._run(model)\\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 756, in _run\\n    self.dispatch()\\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 797, in dispatch\\n    self.accelerator.start_training(self)\\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 96, in start_training\\n    self.training_type_plugin.start_training(trainer)\\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 144, in start_training\\n    self._results = trainer.run_stage()\\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 807, in run_stage\\n    return self.run_train()\\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 869, in run_train\\n    self.train_loop.run_training_epoch()\\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 535, in run_training_epoch\\n    monitor_metrics = deepcopy(self.trainer.logger_connector.callback_metrics)\\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/copy.py\", line 146, in deepcopy\\n    y = copier(x, memo)\\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/copy.py\", line 230, in _deepcopy_dict\\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/copy.py\", line 146, in deepcopy\\n    y = copier(x, memo)\\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/copy.py\", line 230, in _deepcopy_dict\\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/copy.py\", line 153, in deepcopy\\n    y = copier(memo)\\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/torch/tensor.py\", line 55, in __deepcopy__\\n    raise RuntimeError(\"Only Tensors created explicitly by the user \"\\nRuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTE4OTIxcon': 'Here is DataReader\\nclass DataReader(torch.utils.data.Dataset):\\n  def __init__(self, df):\\n        super(DataReader,self).__init__()\\n        self.df = df\\n  def __len__(self):\\n        \\'Denotes the total number of samples\\'\\n        return len(self.df)\\n\\n  def __getitem__(self, index):\\n        \\'Generates one sample of data\\'\\n        # Select sample\\n        file = self.df.iloc[index,0]\\n        label=self.df.iloc[index,1]\\n        return data,label\\n\\nwhen i do\\nx=DataLoader(DataReader(train), batch_size = 2,collate_fn=my_collate)\\nb=next(iter(x))\\n\\nit worked. but when I call it inside LightningModule, it throws an error\\nclass OurModel(LightningModule):\\n  def __init__(self):\\n    super(OurModel,self).__init__()\\n    self.model =cnnmodel()\\n  def forward(self,x):\\n    x= self.model(x)\\n    return x\\n\\n  def configure_optimizers(self):\\n    return torch.optim.AdamW(params=self.parameters(),lr=self.lr )\\n\\n  def train_dataloader(self):\\n    return DataLoader(DataReader(train))\\n\\n  def training_step(self,batch,batch_idx):\\n    return loss\\n\\n  def val_dataloader(self):\\n    ds= DataLoader(DataReader(val))\\n    print(\\'ds\\',len(ds))\\n    return\\n    \\n  def validation_step(self,batch,batch_idx):\\n    print(\\'val step\\')\\n    return \\n\\nI am unable to figure out what is the error and how to resolve it.I tried to debug it, val_dataloader function is working, it print ds 7 but validation_step is not working, it should print val step, but its not printing it\\nEDIT\\nThis issue is because of numworker, setting numworker to 0, resolve the issue, but I am getting this warning\\nConsider increasing the value of thenum_workers argument (try 12 which is the number of cpus on this machine)`\\nTraceback (most recent call last):\\n  File \"<string>\", line 1, in <module>\\n  File \"C:\\\\Anaconda3\\\\lib\\\\multiprocessing\\\\spawn.py\", line 116, in spawn_main\\n    exitcode = _main(fd, parent_sentinel)\\n  File \"C:\\\\Anaconda3\\\\lib\\\\multiprocessing\\\\spawn.py\", line 126, in _main\\n    self = reduction.pickle.load(from_parent)\\nAttributeError: Can\\'t get attribute \\'DataReader\\' on <module \\'__main__\\' (built-in)>\\nTraceback (most recent call last):\\n  File \"<string>\", line 1, in <module>\\n  File \"C:\\\\Anaconda3\\\\lib\\\\multiprocessing\\\\spawn.py\", line 116, in spawn_main\\n    exitcode = _main(fd, parent_sentinel)\\n  File \"C:\\\\Anaconda3\\\\lib\\\\multiprocessing\\\\spawn.py\", line 126, in _main\\n    self = reduction.pickle.load(from_parent)\\nAttributeError: Can\\'t get attribute \\'DataReader\\' on <module \\'__main__\\' (built-in)>\\nTraceback (most recent call last):\\n  File \"<string>\", line 1, in <module>\\n  File \"C:\\\\Anaconda3\\\\lib\\\\multiprocessing\\\\spawn.py\", line 116, in spawn_main\\n    exitcode = _main(fd, parent_sentinel)\\n  File \"C:\\\\Anaconda3\\\\lib\\\\multiprocessing\\\\spawn.py\", line 126, in _main\\n    self = reduction.pickle.load(from_parent)\\nAttributeError: Can\\'t get attribute \\'DataReader\\' on <module \\'__main__\\' (built-in)>\\nTraceback (most recent call last):\\n  File \"<string>\", line 1, in <module>\\n  File \"C:\\\\Anaconda3\\\\lib\\\\multiprocessing\\\\spawn.py\", line 116, in spawn_main\\n    exitcode = _main(fd, parent_sentinel)\\n  File \"C:\\\\Anaconda3\\\\lib\\\\multiprocessing\\\\spawn.py\", line 126, in _main\\n    self = reduction.pickle.load(from_parent)\\nAttributeError: Can\\'t get attribute \\'DataReader\\' on <module \\'__main__\\' (built-in)>\\nTraceback (most recent call last):\\n\\n  File \"C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\torch\\\\utils\\\\data\\\\dataloader.py\", line 872, in _try_get_data\\n    data = self._data_queue.get(timeout=timeout)\\n\\n  File \"C:\\\\Anaconda3\\\\lib\\\\multiprocessing\\\\queues.py\", line 108, in get\\n    raise Empty\\n\\nEmpty\\n\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n\\n  File \"<ipython-input-9-5ebd04eb49d1>\", line 101, in <module>\\n    trainer.fit(model)\\n\\n  File \"C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 458, in fit\\n    self._run(model)\\n\\n  File \"C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 756, in _run\\n    self.dispatch()\\n\\n  File \"C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 797, in dispatch\\n    self.accelerator.start_training(self)\\n\\n  File \"C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\accelerators\\\\accelerator.py\", line 96, in start_training\\n    self.training_type_plugin.start_training(trainer)\\n\\n  File \"C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\plugins\\\\training_type\\\\training_type_plugin.py\", line 144, in start_training\\n    self._results = trainer.run_stage()\\n\\n  File \"C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 807, in run_stage\\n    return self.run_train()\\n\\n  File \"C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 842, in run_train\\n    self.run_sanity_check(self.lightning_module)\\n\\n  File \"C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 1107, in run_sanity_check\\n    self.run_evaluation()\\n\\n  File \"C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 949, in run_evaluation\\n    for batch_idx, batch in enumerate(dataloader):\\n\\n  File \"C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\torch\\\\utils\\\\data\\\\dataloader.py\", line 435, in __next__\\n    data = self._next_data()\\n\\n  File \"C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\torch\\\\utils\\\\data\\\\dataloader.py\", line 1068, in _next_data\\n    idx, data = self._get_data()\\n\\n  File \"C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\torch\\\\utils\\\\data\\\\dataloader.py\", line 1034, in _get_data\\n    success, data = self._try_get_data()\\n\\n  File \"C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\torch\\\\utils\\\\data\\\\dataloader.py\", line 885, in _try_get_data\\n    raise RuntimeError(\\'DataLoader worker (pid(s) {}) exited unexpectedly\\'.format(pids_str)) from e\\n\\nRuntimeError: DataLoader worker (pid(s) 4984, 20904) exited unexpectedly',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTExMzgzcon': \"Say I have a lightning model model = MyLightningModel() that contains an object that gets created and updated throughout training model.my_object. Upon loading the model from the checkpoint MyLightningModel.load_from_checkpoint(ckpt_path) I'm noticing the model.my_object attribute is not being saved and restored upon loading from checkpoint.\\nIs it possible to somehow ensure that these model attributes get saved in the checkpoint file and properly restored when loading the model from checkpoint?\\nThanks in advance for your help!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNTEzMDQ5con': 'hi  dear friends,\\nI wanted to train on cpu for debug purpose which can help me understand more about codes.\\nso in my trainer class, i didn\\'t pass gpus parameter,  the parameter looks like below:\\nargs of trainer  Namespace(accelerator=None, accumulate_grad_batches=1, adam_epsilon=1e-08, amp_backend=\\'native\\', amp_level=\\'O2\\', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=10, benchmark=False, bert_config_dir=\\'/Users/i052090/Downloads/segmentation/data/bertmany/bert-base-uncased\\', bert_dropout=0.2, bert_max_length=128, best_dev_f1=0.0, check_val_every_n_epoch=1, checkpoint_callback=True, data_dir=\\'data/conll03\\', dataname=\\'conll03\\', default_root_dir=\\'./conll03/spanner_bert-base-uncased_spMLen_usePruneTrue_useSpLenTrue_useSpMorphTrue_SpWtTrue_value0.5_38274488\\', deterministic=False, distributed_backend=None, fast_dev_run=False, final_div_factor=10000.0, flush_logs_every_n_steps=100, fp_epoch_result=\\'./conll03/spanner_bert-base-uncased_spMLen_usePruneTrue_useSpLenTrue_useSpMorphTrue_SpWtTrue_value0.5_38274488/epoch_results.txt\\', gpus=None, gradient_clip_algorithm=\\'norm\\', gradient_clip_val=1.0, label2idx_list=[(\\'O\\', 0), (\\'ORG\\', 1), (\\'PER\\', 2), (\\'LOC\\', 3), (\\'MISC\\', 4)], limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=1e-05, max_epochs=1, max_spanLen=4, max_steps=None, max_time=None, min_epochs=None, min_steps=None, modelName=\\'spanner_bert-base-uncased_spMLen_usePruneTrue_useSpLenTrue_useSpMorphTrue_SpWtTrue_value0.5\\', model_dropout=0.2, morph2idx_list=[(\\'isupper\\', 1), (\\'islower\\', 2), (\\'istitle\\', 3), (\\'isdigit\\', 4), (\\'other\\', 5)], morph_emb_dim=100, move_metrics_to_cpu=False, multiple_trainloader_mode=\\'max_size_cycle\\', n_class=5, neg_span_weight=0.5, num_nodes=1, num_processes=1, num_sanity_val_steps=2, optimizer=\\'adamw\\', overfit_batches=0.0, param_name=\\'epoch1_batchsize10_lr1e-5_maxlen128\\', plugins=None, precision=16, prepare_data_per_node=True, pretrained_checkpoint=\\'\\', process_position=0, profiler=None, progress_bar_refresh_rate=1, random_int=\\'38274488\\', reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, spanLen_emb_dim=100, span_combination_mode=\\'x,y\\', stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, tokenLen_emb_dim=50, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, use_morph=True, use_prune=True, use_spanLen=True, use_span_weight=True, use_tokenLen=False, val_check_interval=0.25, warmup_steps=0, weight_decay=0.01, weights_save_path=None, weights_summary=\\'top\\', workers=0)\\nBut it would give errors:\\nFile \"trainer.py\", line 572, in main\\ntrainer = Trainer.from_argparse_args(\\nFile \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py\", line 207, in from_argparse_args\\nreturn from_argparse_args(cls, args, **kwargs)\\nFile \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py\", line 52, in from_argparse_args\\nreturn cls(**trainer_kwargs)\\nFile \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 40, in insert_env_defaults\\nreturn fn(self, **kwargs)\\nFile \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 319, in init\\nself.accelerator_connector = AcceleratorConnector(\\nFile \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 136, in init\\nself.accelerator = self.select_accelerator()\\nFile \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 483, in select_accelerator\\nprecision_plugin=self.precision_plugin,\\nFile \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 220, in precision_plugin\\nself._precision_plugin = self.select_precision_plugin()\\nFile \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 350, in select_precision_plugin\\nraise MisconfigurationException(\\npytorch_lightning.utilities.exceptions.MisconfigurationException: You have asked for native AMP on CPU, but AMP is only available on GPU.\\nthanks,',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTI3MDA1con': 'I followed the steps from the documentation when it comes to returning a dictionary of DataLoaders. The problems occurs in the training_step method. I expect the batch parameter to be a dictionary, but in reality it is a string, the first key of the expected dictionary.\\ndef train_dataloader(self):\\n    dl1 =  DataLoader(IconDataset(self.train_icons_path),batch_size=128,\\n              collate_fn=self.triplet_mining.batch_hard_negatives,num_workers=5)\\n    dl2 = DataLoader(IconDataset(self.train_icons_path),batch_size=128,\\n              collate_fn=self.triplet_mining.batch_semi_hard_negative,num_workers=5)\\n    return {\\n            \"hard\": dl1,\\n            \"semi-hard\": dl2 }\\n\\nIn training_step\\ndef training_step(self,batch,batch_idx):\\n    print(type(batch))   # str\\n    print(batch)         # hard\\n\\nI don\\'t know if there is a problem with my collate_fn method. The batching was working all right when one single DataLoader was used.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTI4Mzcwcon': \"I got this question from someone around installing Lightning with conda\\nRunning both conda install -c conda-forge pytorch-lightning  and conda update pytorch-lightning resulted in the version 0.8.5 being used, instead of the latest (as of now, 1.4.2). https://anaconda.org/conda-forge/pytorch-lightning\\nI'm curious if anyone else has seen this issue. @Borda ?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNTIxMjUycon': 'Hi, I\\'m trying to save a model trained using deepspeed stage 2 using this code:\\ntrainer = pl.Trainer(\\n    gpus=4,\\n    plugins=DeepSpeedPlugin(\\n              stage=3,\\n              cpu_offload=True,\\n              partition_activations=True,),\\n    precision=16,\\n    accelerator=\"ddp\",\\n    )\\ntrainer.fit(model, train_dataloader)\\n\\nWith stage 2 it worked if I added this code:\\ntrainer = pl.Trainer(gpus=0,max_epochs=0,)\\ntrainer.fit(model, train_dataloader)\\npickle.dump(model,open(\"model.p\",\"wb\")\\n\\nBut using stage=3 I get this error:\\nTraceback (most recent call last):\\nFile \"t5-11b-regression.py\", line 227, in \\ntorch.save(model,fileName)\\nFile \"/home/ec2-user/.local/lib/python3.7/site-packages/torch/serialization.py\", line 379, in save\\n_save(obj, opened_zipfile, pickle_module, pickle_protocol)\\nFile \"/home/ec2-user/.local/lib/python3.7/site-packages/torch/serialization.py\", line 484, in _save\\npickler.dump(obj)\\nAttributeError: Can\\'t pickle local object \\'FP16_DeepSpeedZeroOptimizer_Stage3._register_hooks_recursively.._post_forward_module_hook\\'\\nI also tried saving using torch.save, but got same error. I also tried both pytorch-lightning version 1.3.8 and 1.4.1\\ncc: @SeanNaren',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTIxNDY5con': 'I am getting this weird error TypeError: \\'Subset\\' object is not callable\\nclass OurModel(LightningModule):\\n    def __init__(self):\\n        super(OurModel,self).__init__()\\n        #architecute\\n        self.model =  timm.create_model(\\'efficientnetv2_rw_s\\', pretrained=True)\\n        self.dataset=torchvision.datasets.ImageFolder(\\'./PSL DATA SET\\')\\n\\n        self.train, self.val, self.test = torch.utils.data.random_split(self.dataset, [1009, 250, 250])\\n\\n    def forward(self,x):\\n        x= self.model(x)\\n        return x\\n\\n    def val_dataloader(self):\\n        print(\\'val_dataloader\\')\\n        ds=DataLoader(MyLazyDataset(self.val,aug), batch_size = self.batch_size,num_workers=self.numworker, shuffle=False)\\n        print(len(ds))\\n        return ds\\n\\n    def validation_step(self,batch,batch_idx):\\n        print(\\'validation step\\')\\n        image,label=batch\\n        out=self(image).squeeze(1)\\n        loss=self.criterion(out,label.float())\\n        self.log(\\'val_loss\\', loss)#use by early stopping\\n        return loss\\n\\nprint statement shows len(ds) but it does not print \\'validation step\\', so data from val_dataloader is not moving to validation_step\\nComplete logs\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-7-8edf32f6d94d> in <module>\\n      8 #trainer.tune(model)\\n      9 \\n---> 10 trainer.fit(model)\\n\\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\\n    458         )\\n    459 \\n--> 460         self._run(model)\\n    461 \\n    462         assert self.state.stopped\\n\\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run(self, model)\\n    756 \\n    757         # dispatch `start_training` or `start_evaluating` or `start_predicting`\\n--> 758         self.dispatch()\\n    759 \\n    760         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\\n\\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\\n    797             self.accelerator.start_predicting(self)\\n    798         else:\\n--> 799             self.accelerator.start_training(self)\\n    800 \\n    801     def run_stage(self):\\n\\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\\n     94 \\n     95     def start_training(self, trainer: \\'pl.Trainer\\') -> None:\\n---> 96         self.training_type_plugin.start_training(trainer)\\n     97 \\n     98     def start_evaluating(self, trainer: \\'pl.Trainer\\') -> None:\\n\\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_training(self, trainer)\\n    142     def start_training(self, trainer: \\'pl.Trainer\\') -> None:\\n    143         # double dispatch to initiate the training loop\\n--> 144         self._results = trainer.run_stage()\\n    145 \\n    146     def start_evaluating(self, trainer: \\'pl.Trainer\\') -> None:\\n\\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_stage(self)\\n    807         if self.predicting:\\n    808             return self.run_predict()\\n--> 809         return self.run_train()\\n    810 \\n    811     def _pre_training_routine(self):\\n\\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\\n    842             self.progress_bar_callback.disable()\\n    843 \\n--> 844         self.run_sanity_check(self.lightning_module)\\n    845 \\n    846         self.checkpoint_connector.has_trained = False\\n\\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_sanity_check(self, ref_model)\\n   1110 \\n   1111             # run eval step\\n-> 1112             self.run_evaluation()\\n   1113 \\n   1114             self.on_sanity_check_end()\\n\\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_evaluation(self, on_epoch)\\n    930 \\n    931         # enable eval mode + no grads\\n--> 932         self.evaluation_loop.on_evaluation_model_eval()\\n    933         # ref model\\n    934         model = self.lightning_module\\n\\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py in on_evaluation_model_eval(self)\\n     87             model_ref.on_test_model_eval()\\n     88         else:\\n---> 89             model_ref.on_validation_model_eval()\\n     90 \\n     91     def on_evaluation_model_train(self) -> None:\\n\\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/core/hooks.py in on_validation_model_eval(self)\\n    195         Sets the model to eval during the val loop\\n    196         \"\"\"\\n--> 197         self.trainer.model.eval()\\n    198 \\n    199     def on_validation_model_train(self) -> None:\\n\\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in eval(self)\\n   1291             Module: self\\n   1292         \"\"\"\\n-> 1293         return self.train(False)\\n   1294 \\n   1295     def requires_grad_(self: T, requires_grad: bool = True) -> T:',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTIyMjk1con': 'Hi!\\nI\\'m a bit confused regarding the trainer\\'s flags. According to Lightning\\'s documentation, the default settings are:\\n\\namp_level=\\'O2\\'  (i.e., \"Almost FP16\" Mixed Precision, cf. Nvidia documentation)\\nprecision=32\\n\\nIsn\\'t it a bit contradictory ? Is the default training mode full precision or mixed precision?\\nThanks in advance for your clarification :)',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTM0Nzc4con': 'I use pytorch lightning to train a model but it always strangely fail at end: After validations completed, the trainer will start an epoch that bigger that max_epoch and causing GPU memory allocation failure (CUDA out of memory) right after this epoch (which should not run) started. For my example, I set max_epoch=5 so there should only be epoch 0-4. But there will always be an additional epoch-5 after 5 validations are done and a few seconds later the CUDA memory error will occur.\\nNotebook log:\\n\\nWandb system info:\\n\\nMy dataset should be fine as CUDA memory and system memory are stable during the training period, except the GPU memory surge at the very end. And here are my code for lightning module and training loop which I think may cause this trouble:\\nclass BaseModel(pl.LightningModule):\\n    def __init__(self, model_name=params[\\'model\\'], out_features=params[\\'out_features\\'], inp_channels=params[\\'inp_channels\\'], pretrained=True):\\n        super().__init__()\\n        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\\n        \\n        # Change output features. Input features keep the same.\\n        if model_name == \\'resnet18d\\':\\n            n_features = self.model.fc.in_features\\n            self.model.fc = nn.Linear(n_features, out_features, bias=True)\\n            \\n        if model_name == \\'nfnet_f1\\':\\n            n_features = self.model.head.fc.in_features\\n            self.model.head.fc = nn.Linear(n_features, out_features, bias=True)\\n            \\n        elif model_name == \\'efficientnet_b1\\':\\n            n_features = self.model.classifier.in_features\\n            self.model.classifier = nn.Linear(n_features, out_features, bias=True)\\n            \\n        self.criterion = nn.BCEWithLogitsLoss()\\n        \\n    def forward(self, x):\\n        output = self.model(x)\\n        return output\\n\\n    def training_step(self, batch, batch_idx):\\n        x, y = batch\\n        output = self.model(x)\\n        labels = y.unsqueeze(1)\\n        loss = self.criterion(output, labels)\\n\\n        try:\\n            auc = roc_auc_score(labels.detach().cpu(), output.sigmoid().detach().cpu())\\n            self.log(\\'auc\\', auc, on_step=True, prog_bar=True, logger=True)\\n            self.log(\\'Train Loss\\', loss, on_step=True, prog_bar=True, logger=True)\\n        except:\\n            pass\\n\\n        return {\\'loss\\': loss, \\'predictions\\': output, \"labels\": labels}\\n\\n    def training_epoch_end(self, outputs):\\n        preds = []\\n        labels = []\\n\\n        for output in outputs:\\n            preds += output[\\'predictions\\'].detach()\\n            labels += output[\\'labels\\'].detach()\\n\\n        preds = torch.stack(preds)\\n        labels = torch.stack(labels)\\n\\n        train_auc = roc_auc_score(labels.detach().cpu(), preds.sigmoid().detach().cpu())\\n        self.log(\\'mean_train_auc\\', train_auc, prog_bar=True, logger=True)\\n\\n    def validation_step(self, batch, batch_idx):\\n        x, y = batch\\n        output = self.model(x)\\n        labels = y.unsqueeze(1)\\n        loss = self.criterion(output, labels)\\n\\n        self.log(\\'val_loss\\', loss, on_step=True, prog_bar=True, logger=True)\\n        return {\\'predictions\\': output, \\'labels\\': labels}\\n\\n    def validation_epoch_end(self, outputs):\\n        preds = []\\n        labels = []\\n\\n        for output in outputs:\\n            preds += output[\\'predictions\\'].detach()\\n            labels += output[\\'labels\\'].detach()\\n\\n        preds = torch.stack(preds)\\n        labels = torch.stack(labels)\\n\\n        val_auc = roc_auc_score(labels.detach().cpu(), preds.sigmoid().detach().cpu())\\n        self.log(\\'val_auc\\', val_auc, prog_bar=True, logger=True)\\n\\n    def test_step(self, batch, batch_idx):\\n        x, y = batch\\n        output = self(x).sigmoid()\\n        return output\\n\\n    def configure_optimizers(self):\\n        param_optimizer = list(self.model.named_parameters())\\n        no_decay = [\\'bias\\', \\'LayerNorm.bias\\', \\'LayerNorm.weight\\'] # no decay\\n        optimizer_parameters = [\\n            {\\n                \\'params\\': [\\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\\n                ], # Do not optimize no decay parameters.\\n                \\'weight_decay\\': params[\\'weight_decay\\'],\\n            },\\n            {\\n                \\'params\\': [\\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\\n                ],\\n                \\'weight_decay\\': 0.0,\\n            }\\n        ]\\n\\n        optimizer = FusedAdam(optimizer_parameters, lr=params[\\'lr\\'])\\n\\n        scheduler = CosineAnnealingLR(optimizer,\\n                                      T_max=params[\\'T_max\\'],\\n                                      eta_min=params[\\'min_lr\\'],\\n                                      last_epoch=-1)\\n\\n        # Give out optimizer & scheduler for pytorch lightning in python dict format.\\n        return dict(optimizer=optimizer,\\n                    lr_scheduler=scheduler) # lr_scheduler for scheduler.  \\n\\nkfolds = StratifiedKFold(n_splits=params[\\'nfolds\\'], shuffle=True, random_state=params[\\'seed\\'])\\n\\nmodel = BaseModel()\\n\\nfor fold, (trn_idx, val_idx) in enumerate(kfolds.split(train_df[\"id\"], train_df[\\'target\\'])):\\n    # Run first round.\\n    if fold != 0:\\n        continue\\n    \\n    # PL + wandb\\n    wandb_logger = WandbLogger(project=\\'G2Net-steady-exp\\',\\n                               config=params,\\n                               group=\\'Effnet-CQT\\',\\n                               job_type=\\'train\\',\\n                               name=f\\'Fold{fold}\\')\\n    print(f\"{\\'=\\'*20} Fold: {fold} {\\'=\\'*20}\")\\n    \\n    # Set up data module.\\n    train_data = train_df.loc[trn_idx]\\n    train_sample_data = data_sample(train_data)\\n    valid_data = train_df.loc[val_idx] # About 65k samples.\\n    data_module = DataModule(train_sample_data,\\n                             valid_data,\\n                             valid_data) # No test data yet.\\n    data_module.setup()\\n    \\n    # Add callbacks.\\n    early_stopping_callback = EarlyStopping(monitor=\\'val_auc\\',\\n                                            mode=\\'max\\',\\n                                            patience=5)\\n    checkpoint_callback = ModelCheckpoint(dirpath=\\'./checkpoints/\\',\\n                                          filename= f\\'fold-{fold}-best\\' + \\'-val_auc{val_auc:.3f}\\',\\n                                          save_top_k=2,\\n                                          verbose=True,\\n                                          monitor=\\'val_auc\\',\\n                                          mode=\\'max\\')\\n    \\n    trainer = pl.Trainer(gpus=1,\\n                         callbacks=[early_stopping_callback,\\n                                    checkpoint_callback],\\n                         max_epochs=params[\\'epochs\\'],\\n                         precision=params[\\'precision\\'],\\n                         progress_bar_refresh_rate=1,\\n                         stochastic_weight_avg=True,\\n                         logger=wandb_logger)\\n    \\n    trainer.fit(model, data_module)\\n\\nCan I get any clue about why this would happen and how to avoid it ? I\\'m new to pytorch lightning so there might be problems I\\'m not aware of. Thanks a lot!',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTM1Mzkxcon': 'I get the below error immediately when I set a CUDA seed or after a few epochs without seed and with val_workers>0. I also find that I get the error when I try reading from a pickled file containing PyTorch tensors (on cpu).\\nI have a small dataset, so I load all the data in the __init__ of my Dataset class. I then save it on my disk using pickle so I can save on dataloading time when I run my code again. Now, since I have 2 GPUs, DDP in pytorch-lightning starts 2 processes and each of these processes start reading from the pickle file. Both the training data and validation data are being read from pickle files.\\nMy error is quite similar to the the comment mentioned here - pytorch/pytorch#28950 (comment). Note that both the person who commented and I, have pin_memory=False although the title says otherwise.\\nAny ideas as to why this is happening will surely help me! Thank you.\\nPL version = 1.4.2 , torch version = \\'1.9.0+cu102\\', CUDA version = 10.2\\nValidation sanity check: 0it [00:00, ?it/s]/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val \\ndataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this m\\nachine) in the `DataLoader` init to improve performance.                                                                                                                      \\n  rank_zero_warn(                                                                                                                                                             \\nValidation sanity check:   0%|                                                                                                                          | 0/1 [00:00<?, ?it/s]\\n/home/usr/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subjec\\nt to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)                 \\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)                                                                                           \\n/home/usr/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subjec\\nt to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)                 \\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)                                                                                           \\nGlobal seed set to 42                                                                                                                                                         \\nGlobal seed set to 42                                                                                                                                                         \\nEpoch 0:  80%|█████████████████████████████████████████████████████████████████████████████████████▌                     | 4/5 [00:14<00:02,  2.80s/it, loss=4.33, v_num=d09et\\nerminate called after throwing an instance of \\'c10::CUDAError\\'                                                                                          | 0/1 [00:00<?, ?it/s]\\n  what():  CUDA error: initialization error                                                                                                                                   \\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.                                                        \\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.                                                                                                                        \\nException raised from insert_events at /pytorch/c10/cuda/CUDACachingAllocator.cpp:1089 (most recent call first):                                                              \\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x2b5f7135ca22 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10.so)               \\nframe #1: <unknown function> + 0x10d7e (0x2b5f710ecd7e in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)                                        \\nframe #2: c10::cuda::CUDACachingAllocator::raw_delete(void*) + 0x1a7 (0x2b5f710ee027 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)          \\nframe #3: c10::TensorImpl::release_resources() + 0x54 (0x2b5f713465a4 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10.so)                              \\nframe #4: <unknown function> + 0xa27e1a (0x2b5f1a569e1a in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libtorch_python.so)                                   \\n<omitting python frames>                                                                                                                                                      \\n                                                                                                                                                                              \\nterminate called after throwing an instance of \\'c10::CUDAError\\'                                                                                                               \\n  what():  CUDA error: initialization error                                                                                                                                   \\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.                                                        \\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.                                                                                                                        \\nException raised from insert_events at /pytorch/c10/cuda/CUDACachingAllocator.cpp:1089 (most recent call first):                                                              \\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x2b4b41756a22 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10.so)               \\nframe #1: <unknown function> + 0x10d7e (0x2b4b414e6d7e in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)                                        \\nframe #2: c10::cuda::CUDACachingAllocator::raw_delete(void*) + 0x1a7 (0x2b4b414e8027 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)          \\nframe #3: c10::TensorImpl::release_resources() + 0x54 (0x2b4b417405a4 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10.so)                              \\nframe #4: <unknown function> + 0xa27e1a (0x2b4aea963e1a in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libtorch_python.so)                                   \\n<omitting python frames>                                                                                                                                                      \\n                                                                                                                                                                              \\nTraceback (most recent call last):                                                     \\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 990, in _try_get_data\\nTraceback (most recent call last):                                             \\nFile \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 990, in _try_get_data\\n    data = self._data_queue.get(timeout=timeout)\\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 107, in get\\n    data = self._data_queue.get(timeout=timeout)\\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 107, in get\\n    if not self._poll(timeout):\\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 257, in poll\\n    if not self._poll(timeout):\\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 257, in poll\\n    return self._poll(timeout)\\n    return self._poll(timeout)\\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 424, in _poll\\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 424, in _poll\\n    r = wait([self], timeout)\\n    r = wait([self], timeout)\\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\\n    ready = selector.select(timeout)\\n    ready = selector.select(timeout)\\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/selectors.py\", line 415, in select\\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/selectors.py\", line 415, in select\\n    fd_event_list = self._selector.poll(timeout)\\n    fd_event_list = self._selector.poll(timeout)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\\n    _error_if_any_worker_fails()\\nRuntimeError: DataLoader worker (pid 3404) is killed by signal: Aborted. \\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/usr/mymodel/run.py\", line 22, in <module>\\n    _error_if_any_worker_fails()\\nRuntimeError: DataLoader worker (pid 3407) is killed by signal: Aborted. \\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/home/usr/mymodel/run.py\", line 22, in <module>\\n    main()\\n  File \"/home/usr/mymodel/run.py\", line 18, in main\\n   main()\\n  File \"/home/usr/mymodel/run.py\", line 18, in main\\n    return train(CFG)\\n  File \"/scratch/usr/mymodel/src/train.py\", line 110, in train\\n    return train(CFG)\\n  File \"/scratch/usr/mymodel/src/train.py\", line 110, in train\\n    trainer.fit(model,dm)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 553, in fit\\n    trainer.fit(model,dm)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 553, in fit\\n    self._run(model)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 918, in _run\\n    self._run(model)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 918, in _run\\n    self._dispatch()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 986, in _dispatch\\n    self._dispatch()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 986, in _dispatch\\n    self.accelerator.start_training(self)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\\n    self.accelerator.start_training(self)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\\n    self.training_type_plugin.start_training(trainer)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 161, in start_training\\n    self._results = trainer.run_stage()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 996, in run_stage\\n    return self._run_train()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_train\\n    self.training_type_plugin.start_training(trainer)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 161, in start_training\\n    self.fit_loop.run()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\\n    self._results = trainer.run_stage()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 996, in run_stage\\n    self.advance(*args, **kwargs)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\\n    return self._run_train()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_train\\n    epoch_output = self.epoch_loop.run(train_dataloader)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 112, in run\\n    self.on_advance_end()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 177, in on_advance_end\\n\\n   self.fit_loop.run()                                                                                                                                                       \\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\\n    self._run_validation()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 256, in _run_validation\\n    self.advance(*args, **kwargs)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\\n    epoch_output = self.epoch_loop.run(train_dataloader)\\n    self.val_loop.run()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 112, in run\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\\n    self.advance(*args, **kwargs)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\\n    self.on_advance_end()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 177, in on_advance_end\\n    dl_outputs = self.epoch_loop.run(\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\\n    self._run_validation()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 256, in _run_validation\\n    self.advance(*args, **kwargs)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 93, in advance\\n    self.val_loop.run()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\\n    batch_idx, batch = next(dataloader_iter)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\\n    self.advance(*args, **kwargs)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\\n    dl_outputs = self.epoch_loop.run(\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\\n    data = self._next_data()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1186, in _next_data\\n    self.advance(*args, **kwargs)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 93, in advance\\n    batch_idx, batch = next(dataloader_iter)\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\\n    idx, data = self._get_data()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1152, in _get_data\\n    data = self._next_data()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1186, in _next_data\\n    success, data = self._try_get_data()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1003, in _try_get_data\\n    idx, data = self._get_data()\\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1152, in _get_data\\n    raise RuntimeError(\\'DataLoader worker (pid(s) {}) exited unexpectedly\\'.format(pids_str)) from e\\nRuntimeError: DataLoader worker (pid(s) 3404) exited unexpectedly\\n    success, data = self._try_get_data()\\nile \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1003, in _try_get_data\\n    raise RuntimeError(\\'DataLoader worker (pid(s) {}) exited unexpectedly\\'.format(pids_str)) from e\\nRuntimeError: DataLoader worker (pid(s) 3407) exited unexpectedly',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTM2MDY5con': 'Hi . I want to calculate the inference time of my model. I am not sure where to the code for measuring the time. I thought its better to do it inside predict_step of the LightningModule. Something like this\\nclass LitModule(pl.LightningModule):\\n        def __init__(self, hparams):\\n               ....\\n               self.starter, self.ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\\n               .....\\n        ......\\n        ......\\n         def predict_step(self, batch, batch_idx, dataloader_idx=None):\\n                 torch.cuda.empty_cache()\\n                 minibatch_model_in, _ = batch\\n                 self.starter.record()\\n                 _ = self(minibatch_model_in)\\n                 self.ender.record()\\n                 # wait for gpu sync\\n                 torch.cuda.synchronize()\\n                 inference_time = self.starter.elapsed_time(self.ender)\\n                 return inference_time*1e-3\\n\\nAnd in the main funtion,\\ninference_metrics = trainer.predict(model=pl_model, datamodule=pl_data)\\n\\nAfter removing the initial measurements (considering GPU warm-up) and taking mean of 200 samples, I get 0.0196 seconds.\\nIf I do the measurement outside the LightningModule then I get a different value. This is how I measured\\n# Inferencing\\npl_data.setup()\\n# Initializing cuda event loggers\\nstarter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\\nmetrics = []\\npl_model.eval()\\nwith torch.no_grad():\\nfor batch in pl_data.train_dataloader():\\n      torch.cuda.empty_cache()\\n      minibatch_model_in, _, _, _, _, _ = batch\\n      starter.record()\\n        _, _, _ = pl_model(minibatch_model_in)\\n      # wait for gpu sync\\n       torch.cuda.synchronize()\\n       curr_time = starter.elapsed_time(ender)\\n       metrics.append(curr_time)\\n\\nUsing the above code, I get 0.027 seconds. Please someone tell me if I am doing something wrong. Which method is correct? Why is there a decrease in the inference time when measured within predict_step?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTM4ODc2con': 'I wonder how people do Hyperparameter Tuning with Lightning CLI?\\nAny suggestion of good practices?\\nThanks!',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTMxMDk5con': 'Hi. I have a ModelCheckpoint callback where I am monitoring the val_acc (validation set accuracy) with mode=max. After training my model using trainer.fit, I want to get the maximum value of val_acc. How can I get this value without re-evaluating my best model on the validation dataset? Thanks.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQ0NDA4con': 'When editing code in VSCode, the pytorch_lightning/__about__.py file keeps getting automatically updated\\n__version__ = \"1.5.0dev\" -> __version__ = \"20210827\"\\n\\nlike in 5f4f3c5#r697656347\\nDoes anyone know how to stop this from happening? Thanks!',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQ0NzQ3con': \"I'm currently running a lot of experiments and in order to track all of them in tensorboard I have to rename each experiment folder by hand (e.g. lightning_logs/version_0 -> lightning_logs/{unique_informative_exp_name}).\\nHowever, it would be better if I could pass the as an argument to Trainer.\\nI searched documentation thoroughly, but couldn't find anything related to names of experiments (I found only default_root_dir argument, however, it's not that interesting)\\nThe question is follows :\\nIs there a way to set experiment name on program level? I'm using default logger.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQ4NzY1con': \"I am defining a simple multi-class BERT classification model and then training it using pytorch-lightning. The code is in https://colab.research.google.com/drive/1os9mz7w7gmLBL_ZDvZ9K1saz9UA3rmD7?usp=sharing under class BertForMulticlassSequenceClassification(BertPreTrainedModel). The issue is that after training when I am loading the classifier model model = ClassTaggerModel.load_from_checkpoint(checkpoint_file) I get\\nSome weights of BertForMulticlassSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifiers.0.weight', 'classifiers.1.bias', 'classifiers.0.bias', 'classifiers.1.weight']\\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\\n\\nThe reason is probably because pl.LightningModule module has transformer's from_pretrained function that would normally downloads weights from huggingface. This is undesirable behaviour when loading from the trained checkpoint. Is there any feature in Pytroch-lightning that can help having different logic for these two cases (training vs loading). Thanks!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQwNzY5con': 'Hi. I have a function which generates a set of random values (hyperparameters) which are then used to create my model. I want to run this function only once, then use it to create my model and then start ddp training on this model.\\nHowever, with the current setup, when I start ddp, the randomize function gets called again, so now I have 2 GPU processes, each having initialized the model with different set of hyperparameters. (random values from both calls aren\\'t same)\\nIf I add if os.getenv(\"LOCAL_RANK\",0): before my randomize function, then there is no way for the second GPU process to access the hyperparameters generated by the first GPU process. How do I go about this ? Thanks.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQxNDIwcon': \"Hi there,\\nI'm trying to run pytorch-lightning training with deepspeed plugin and activation checkpoints to support bigger batch sizes, based on https://pytorch-lightning.readthedocs.io/en/stable/advanced/advanced_gpu.html#deepspeed-activation-checkpointing.\\nAs specified in the docs, running the model should be done using the checkpoint function. However, this function seems to return a tensor without gradients. When computing loss based on this value and returning from training_step, I'm getting an error\\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\\nMinimal code to reproduce\\nimport os\\n\\nimport deepspeed\\nimport pytorch_lightning as pl\\nimport torch\\nfrom deepspeed.ops.adam import FusedAdam\\nfrom pytorch_lightning.plugins import DeepSpeedPlugin\\nfrom torch import nn\\nfrom pytorch_lightning.utilities.types import STEP_OUTPUT\\nfrom torch.utils.data import DataLoader, RandomSampler\\n\\n\\nclass PlModel(pl.LightningModule):\\n    def __init__(self):\\n        super().__init__()\\n        self.model = nn.Linear(1, 1)\\n\\n    def forward(self, batch):\\n        return self.model(batch)\\n\\n    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\\n        res = deepspeed.checkpointing.checkpoint(self.model, batch)\\n        return nn.MSELoss()(res, torch.zeros_like(res, device=res.device))\\n\\n    def configure_optimizers(self):\\n        return FusedAdam(self.parameters(), lr=0.1)\\n\\n\\nif __name__ == '__main__':\\n    trainer = pl.Trainer(gpus=-1, precision=16, plugins=DeepSpeedPlugin(stage=3, partition_activations=True))\\n    model = PlModel()\\n    dataset = torch.rand(100, 1)\\n    dl = torch.utils.data.DataLoader(dataset, batch_size=1, num_workers=os.cpu_count(),\\n                                     sampler=RandomSampler(dataset))\\n    trainer.fit(model, dl)\\npytorch-lightning version: 1.3.3\\ndeepspeed version: 0.5.0\\nThanks!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQxODQwcon': 'I want to log accuracy to neptune.ai logger. I tried this way\\ndef test_epoch_end(self, outputs):\\n        pred=torch.cat([x[\"pred\"] for x in outputs]).detach().cpu().numpy()\\n        label=torch.cat([x[\"label\"] for x in outputs]).detach().cpu().numpy()\\n        pred=np.argmax(pred,1)\\n        acc=accuracy_score(label,pred)\\n        print(\\'acc\\',acc)\\n        self.logger.experiment.log_metric(\"acc\", acc)\\n\\nand i am getting this error\\n---------------------------------------------------------------------------\\nNeptunePossibleLegacyUsageException       Traceback (most recent call last)\\n<ipython-input-15-a549be9bb6fe> in <module>\\n----> 1 trainer.test(model)\\n\\nC:\\\\anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py in test(self, model, test_dataloaders, ckpt_path, verbose, datamodule)\\n    577 \\n    578         # run test\\n--> 579         results = self._run(model)\\n    580 \\n    581         assert self.state.stopped\\n\\nC:\\\\anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py in _run(self, model)\\n    754 \\n    755         # dispatch `start_training` or `start_evaluating` or `start_predicting`\\n--> 756         self.dispatch()\\n    757 \\n    758         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\\n\\nC:\\\\anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py in dispatch(self)\\n    791     def dispatch(self):\\n    792         if self.evaluating:\\n--> 793             self.accelerator.start_evaluating(self)\\n    794         elif self.predicting:\\n    795             self.accelerator.start_predicting(self)\\n\\nC:\\\\anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\accelerators\\\\accelerator.py in start_evaluating(self, trainer)\\n     97 \\n     98     def start_evaluating(self, trainer: \\'pl.Trainer\\') -> None:\\n---> 99         self.training_type_plugin.start_evaluating(trainer)\\n    100 \\n    101     def start_predicting(self, trainer: \\'pl.Trainer\\') -> None:\\n\\nC:\\\\anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\plugins\\\\training_type\\\\training_type_plugin.py in start_evaluating(self, trainer)\\n    146     def start_evaluating(self, trainer: \\'pl.Trainer\\') -> None:\\n    147         # double dispatch to initiate the test loop\\n--> 148         self._results = trainer.run_stage()\\n    149 \\n    150     def start_predicting(self, trainer: \\'pl.Trainer\\') -> None:\\n\\nC:\\\\anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py in run_stage(self)\\n    802 \\n    803         if self.evaluating:\\n--> 804             return self.run_evaluate()\\n    805         if self.predicting:\\n    806             return self.run_predict()\\n\\nC:\\\\anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py in run_evaluate(self)\\n   1042 \\n   1043         with self.profiler.profile(f\"run_{self.state.stage}_evaluation\"):\\n-> 1044             eval_loop_results = self.run_evaluation()\\n   1045 \\n   1046         # remove the tensors from the eval results\\n\\nC:\\\\anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py in run_evaluation(self, on_epoch)\\n    986 \\n    987         # lightning module method\\n--> 988         self.evaluation_loop.evaluation_epoch_end(outputs)\\n    989 \\n    990         # hook\\n\\nC:\\\\anaconda3\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\evaluation_loop.py in evaluation_epoch_end(self, outputs)\\n    206             if is_overridden(\\'test_epoch_end\\', model=model):\\n    207                 model._current_fx_name = \\'test_epoch_end\\'\\n--> 208                 model.test_epoch_end(outputs)\\n    209 \\n    210         else:\\n\\n<ipython-input-9-b08c6aa77d3f> in test_epoch_end(self, outputs)\\n    104         acc=accuracy_score(label,pred)\\n    105         print(\\'acc\\',acc)\\n--> 106         self.logger.experiment.log_metric(\"acc\", acc)\\n\\nC:\\\\anaconda3\\\\lib\\\\site-packages\\\\neptune\\\\new\\\\run.py in __getattr__(self, item)\\n    161     def __getattr__(self, item):\\n    162         if item in LEGACY_METHODS:\\n--> 163             raise NeptunePossibleLegacyUsageException()\\n    164         raise AttributeError(f\"\\'{self.__class__.__name__}\\' object has no attribute \\'{item}\\'\")\\n    165',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQxOTY5con': 'Hi, I am working on building a question and answering model using T5(Huggingface) with PyTorch Lightning Module and I am checking my loss and PyTorch Lightning Loss is not being matched.\\nclass UQAFineTuneModel(pl.LightningModule):\\n    def __init__(self):\\n        super().__init__()\\n        self.model = T5ForConditionalGeneration.from_pretrained(\\n            \"allenai/unifiedqa-t5-small\", return_dict=True\\n        )\\n        self.model.train()\\n    def forward(\\n        self,\\n        source_text_input_ids,\\n        source_text_attention_mask,\\n        target_text_input_ids=None,\\n    ):\\n        output = self.model(\\n            input_ids=source_text_input_ids,\\n            attention_mask=source_text_attention_mask,\\n            labels=target_text_input_ids,\\n        )\\n        return output.loss, output.logits\\n\\n    def training_step(self, batch, batch_idx):\\n        source_text_input_ids = batch[\"source_text_input_ids\"]\\n        source_text_attention_mask = batch[\"source_text_attention_mask\"]\\n        target_text_input_ids = batch[\"target_text_input_ids\"]\\n        # labels_attention_mask = batch[\"target_text_attention_mask\"]\\n        loss, outputs = self(\\n            source_text_input_ids, source_text_attention_mask, target_text_input_ids\\n        ) \\n        loss_mine = None  \\n        output = self.model(\\n            input_ids=source_text_input_ids,\\n            attention_mask=source_text_attention_mask,\\n            labels=target_text_input_ids,\\n        ) \\n        labels = batch[\"target_text_input_ids\"].clone() \\n        labels[labels == 0] = -100 \\n        if target_text_input_ids is not None:  \\n            loss_fct = CrossEntropyLoss(ignore_index=-100) \\n            loss_mine = loss_fct(output.logits.view(-1, outputs.size(-1)), labels.view(-1)) \\n            print(f\"loss_Hugginface: {loss.item()}, loss_mine : {loss_mine.item()}\") \\n        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\\n        return {\"loss\": loss, \"predictions\": outputs}\\n\\n\\nYou can see the above image, why loss is not same, help is very much needed, I asked the same question on HuggingFace but they told me to ask here, you can view that discussion here.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQzNzYycon': 'self.log(on_epoch=True) computes the metrics across GPUS and epoch batches automatically. How can we get the value of it when training in distributed mode?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTU1NjYzcon': \"Hello all - just wanted to discuss a use-case with CPU vs GPU PL install. We do normal training on GPUs, but when deploying for prediction we use CPUs and would like to keep the Docker container size as small as possible. It't not clear if it's possible to install pytorch-lightning with CPU-only torch distribution, which is much smaller. Is there any possible equivalent to pip install pytorch-lightning[cpu] . Thanks for suggestions!\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNTU2MTc0con': 'Hello,\\nI have been trying to debug OOM after a few iterations on my model.\\nAfter investigation I saw that in loops/batch/training_batch_loop.py there is this piece of code (L235):\\nresult = self.training_step_and_backward(split_batch, batch_idx, opt_idx, optimizer, hiddens)\\n  if result is not None:\\n     return_result.update(result)\\n     return return_result.loss\\nWhat I see is that return_result will keep the computation graph as it contains the loss. So I wonder what is the role of this variable? Also, where is the graph released, I could no go any further than \"_training_step_and_backward_closure\"? I don\\'t understand why my model runs fine for a few iterations then there is some increase in memory.',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTU3MTEwcon': \"Hey,\\nI am not sure if I can currently keep only the best and latest models as a wandb artifact using the WandbLogger? That is, I am looking for a behavior similar to log_model='all', but which keeps only the latest and best models and deletes previous checkpoints from the wandb artifacts of the experiment.\\nMy checkpoints weigh about 1GB and I don't want to keep the entire history of checkpoints with the log_model='all' flag, but rather only the best and the latest models. I thought about inheriting from the WandbLogger and following the guideline here:\\nhttps://gitbook-docs.wandb.ai/guides/artifacts/api#cleaning-up-unused-versions\\nAny thoughts?\\nMaybe this should be a feature request?\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNTU4NjIxcon': 'The great advantage and convenient feature of PyTorch-Lightning over vanilla Pytorch is the more convenient interface for optimization loops.  Instead of reinventing the wheel every time with manual train and test function, one can just define\\ntraining_step and validation_step, e.t.c.\\nThe setup of the optimizer is done in the configure_optimizers method https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers, where one can create optimizers and schedulers.\\nHowever, it seems, that this choice fixes the optimizer properties in the definition of the class, and often you would like to try several optimizers and change their params. One way to do it - is to pass additional arguments to the constructor of\\npl.LightningModule to be processed in configure_optimizers, where an appropriate optimizer will be chosen.\\nAlso one may try to access directly the optimizers outside the class, but probably it is an unsafe approach.\\nIs there some alternative way to reconfigure optimizer, like in Tensorflow, where models have complie method https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTUxNDE3con': 'Hi,\\nDo I need to manually set the random seed to ensure the synchronization of state (e.g. initialized parameters) across processes when using distributed training?',\n",
              " 'MDEwOkRpc2N1c3Npb24zNTY2Nzgycon': \"Is there an easy way to make a Lightning CLI work with a Lightning Module defined by data? This seems like a very common design pattern.\\nFor example (from the docs) it doesn't appear possible to easily convert the following to a Lightning CLI:\\n# init dm AND call the processing manually\\ndm = ImagenetDataModule()\\ndm.prepare_data()\\ndm.setup()\\n\\nmodel = LitModel(out_features=dm.num_classes, img_width=dm.img_width, img_height=dm.img_height)\\ntrainer.fit(model, dm)\\n\\nThe current CLI implementation works for model re-loading. However, it requires data-dependent attributes to be specified to the config files prior to fitting, which does not seem ideal.\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNTY4NDY4con': \"use this code, i can get test data. But when i use pl data module to fit train model, i got dataloader returned 0 length error\\nimport os\\nfrom typing import Optional\\nimport PIL\\nimport cv2\\nimport json\\nimport copy\\nimport numpy as np\\nimport pytorch_lightning as pl\\nimport torch\\nfrom torchvision import transforms\\nfrom torch.utils.data import Dataset, random_split, DataLoader\\n\\nfrom det.det_modules import ResizeShortSize, IaaAugment, EastRandomCropData, MakeBorderMap, MakeShrinkMap\\n\\n\\ndef load_json(file_path: str):\\n    with open(file_path, 'r', encoding='utf8') as f:\\n        content = json.load(f)\\n    return content\\n\\n\\nclass ICDARDataset(Dataset):\\n    def __init__(self, json_path, img_path, is_train=True):\\n        self.ignore_tags = ['*', '###']\\n        self.load_char_annotation = False\\n        self.data_list = self.load_data(json_path, img_path)\\n        self.transform = transforms.Compose(\\n            [\\n                transforms.ToTensor(),\\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n            ]\\n        )\\n        self.iaa_augment = IaaAugment()\\n        self.east_random_crop_data = EastRandomCropData()\\n        self.make_border_map = MakeBorderMap()\\n        self.make_shrink_map = MakeShrinkMap()\\n        self.resize = ResizeShortSize(short_size=736, resize_text_polys=False)\\n        self.is_train = is_train\\n\\n    def load_data(self, json_path: str, img_path) -> list:\\n        data_list = []\\n        content = load_json(json_path)\\n        for item in content:\\n            p = os.path.join(img_path, item + '.jpg')\\n            polygons = []\\n            texts = []\\n            illegibility_list = []\\n            for annotation in content[item]:\\n                if len(annotation['points']) == 0 or len(annotation['transcription']) == 0:\\n                    continue\\n                polygons.append(annotation['points'])\\n                texts.append(annotation['transcription'])\\n                illegibility_list.append(annotation['illegibility'])\\n            data_list.append(\\n                {\\n                    'img_path': p,\\n                    'text_polys': np.array(polygons, dtype=object),\\n                    'texts': texts,\\n                    'ignore_tags': illegibility_list\\n                }\\n            )\\n        return data_list\\n\\n    def __getitem__(self, index):\\n        data = self.data_list[index]\\n        im = cv2.imread(data['img_path'])\\n        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\\n\\n        data['img'] = im\\n        data['shape'] = [im.shape[0], im.shape[1]]\\n        if self.is_train:\\n            data = self.iaa_augment(data)\\n            data = self.east_random_crop_data(data)\\n            data = self.make_border_map(data)\\n            data = self.make_shrink_map(data)\\n        else:\\n            data = self.resize(data)\\n        # resize = ResizeShortSize(short_size=736, resize_text_polys=False)\\n        # data = resize(data)\\n        data['img'] = self.transform(data['img'])\\n        data['text_polys'] = data['text_polys']\\n        return copy.deepcopy(data)\\n\\n    def __len__(self):\\n        return len(self.data_list)\\n\\n\\nclass DetCollectFN:\\n    def __init__(self, *args, **kwargs):\\n        pass\\n\\n    def __call__(self, batch):\\n        data_dict = {}\\n        to_tensor_keys = []\\n        for sample in batch:\\n            for k, v in sample.items():\\n                if k not in data_dict:\\n                    data_dict[k] = []\\n                if isinstance(v, (np.ndarray, torch.Tensor, PIL.Image.Image)):\\n                    if k not in to_tensor_keys:\\n                        to_tensor_keys.append(k)\\n                    if isinstance(v, np.ndarray):\\n                        v = torch.tensor(v)\\n                    if isinstance(v, PIL.Image.Image):\\n                        v = transforms.ToTensor()(v)\\n                data_dict[k].append(v)\\n        for k in to_tensor_keys:\\n            data_dict[k] = torch.stack(data_dict[k], 0)\\n        return data_dict\\n\\n\\nclass DBDataModule(pl.LightningDataModule):\\n    def __init__(self, train_json_path, train_img_path, val_json_path, val_img_path):\\n        super(DBDataModule, self).__init__()\\n        self.train = ICDARDataset(train_json_path, train_img_path, is_train=True)\\n        self.val = ICDARDataset(val_json_path, val_img_path, is_train=False)\\n\\n    def train_dataloader(self):\\n        return DataLoader(self.train, batch_size=32, num_workers=0, shuffle=True, collate_fn=DetCollectFN)\\n\\n    def val_dataloader(self):\\n        return DataLoader(self.val, batch_size=32, num_workers=0, collate_fn=DetCollectFN)\\n\\n\\nif __name__ == '__main__':\\n\\n    import torch\\n    from torch.utils.data import DataLoader\\n\\n    from matplotlib import pyplot as plt\\n\\n\\n    def show_img(imgs: np.ndarray, title='img'):\\n        from matplotlib import pyplot as plt\\n        color = (len(imgs.shape) == 3 and imgs.shape[-1] == 3)\\n        imgs = np.expand_dims(imgs, axis=0)\\n        for i, img in enumerate(imgs):\\n            plt.figure()\\n            plt.title('{}_{}'.format(title, i))\\n            plt.imshow(img, cmap=None if color else 'gray')\\n\\n\\n    def draw_bbox(img_path, result, color=(255, 0, 0), thickness=2):\\n        import cv2\\n        if isinstance(img_path, str):\\n            img_path = cv2.imread(img_path)\\n            # img_path = cv2.cvtColor(img_path, cv2.COLOR_BGR2RGB)\\n        img_path = img_path.copy()\\n        for point in result:\\n            # point = point.astype(int)\\n            cv2.polylines(img_path, [point], True, color, thickness)\\n        return img_path\\n\\n\\n    dataset = ICDARDataset('/home/data/OCRData/icdar2019/train/train.json', '/home/data/OCRData/icdar2019/train/images')\\n    print(len(dataset))\\n    train_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True, num_workers=0)\\n    for i, data in enumerate(train_loader):\\n        img = data['img']\\n        shrink_label = data['shrink_map']\\n        threshold_label = data['threshold_map']\\n\\n        print(threshold_label.shape, threshold_label.shape, img.shape)\\n        show_img(img[0].numpy().transpose(1, 2, 0), title='img')\\n        show_img((shrink_label[0].to(torch.float)).numpy(), title='shrink_label')\\n        show_img((threshold_label[0].to(torch.float)).numpy(), title='threshold_label')\\n        # img = draw_bbox(img[0].numpy().transpose(1, 2, 0), np.array(data['text_polys']))\\n        # show_img(img, title='draw_bbox')\\n        plt.show()\\n\\n        break\",\n",
              " 'MDEwOkRpc2N1c3Npb24zNTYyMjU2con': 'Hello :)\\nCurrently I use  trainer.predict(model=..., dataloaders=...) which returns the results of predict_step(...) in a list where each element in the list corresponds to one batch input to the predict_step function which I already implemented. I am looking for an predict_epoch_end kind of function to collect to batched predictions into one data structure but only found the possibility to define a callback on_prediction_epoch_end() but this has no return possibility. How should one best proceed to collect the batched predictions?\\ni.e. each predict_step() returns a tensor of shape batch_size x 10\\nand lets assume the dataloader gives 5 batches to predict. Then the trainer.predict() function will return a list of lenght 5 with each element beeing a tensor of shape batch_size x 10. However I would rather like to receive one tensor of shape 5*batch_size x 10.\\nThis is just a simple example to illustrate my problem. My actual return per prediction step is a little bit more involved and I would like to clean up the structure before returning it and also make this cleanup logic part of the module so that I dont have to remember the specifics.\\nIs there a way to do that in the current framework and if so how?\\nThanks in advance for any help!'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ir_relevant_docs = {key: [] for key in eval_df['id_x'].unique()}\n",
        "for i, row in eval_df.iterrows():\n",
        "    # we append in the case of a question ID being connected to\n",
        "    # multiple context IDs\n",
        "    ir_relevant_docs[row['id_x']].append(row['id_y'])\n",
        "# this must be in format {question_id: {set of context_ids}}\n",
        "ir_relevant_docs = {key: set(values) for key, values in ir_relevant_docs.items()}\n",
        "ir_relevant_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEeczB9SyUpQ",
        "outputId": "65a7f952-3ab7-4373-c8b4-819d6b84f269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'D_kwDOCqWgoM4AN-cL': {'D_kwDOCqWgoM4AN-cLcon'},\n",
              " 'D_kwDOCqWgoM4AN-x1': {'D_kwDOCqWgoM4AN-x1con'},\n",
              " 'D_kwDOCqWgoM4AN16B': {'D_kwDOCqWgoM4AN16Bcon'},\n",
              " 'D_kwDOCqWgoM4AN2Gd': {'D_kwDOCqWgoM4AN2Gdcon'},\n",
              " 'D_kwDOCqWgoM4AN49x': {'D_kwDOCqWgoM4AN49xcon'},\n",
              " 'D_kwDOCqWgoM4AN4Of': {'D_kwDOCqWgoM4AN4Ofcon'},\n",
              " 'D_kwDOCqWgoM4AN4Z4': {'D_kwDOCqWgoM4AN4Z4con'},\n",
              " 'D_kwDOCqWgoM4AN5Lv': {'D_kwDOCqWgoM4AN5Lvcon'},\n",
              " 'D_kwDOCqWgoM4AN637': {'D_kwDOCqWgoM4AN637con'},\n",
              " 'D_kwDOCqWgoM4AN6JX': {'D_kwDOCqWgoM4AN6JXcon'},\n",
              " 'D_kwDOCqWgoM4AN6mz': {'D_kwDOCqWgoM4AN6mzcon'},\n",
              " 'D_kwDOCqWgoM4AN6vA': {'D_kwDOCqWgoM4AN6vAcon'},\n",
              " 'D_kwDOCqWgoM4AN7kb': {'D_kwDOCqWgoM4AN7kbcon'},\n",
              " 'D_kwDOCqWgoM4AN88O': {'D_kwDOCqWgoM4AN88Ocon'},\n",
              " 'D_kwDOCqWgoM4AN8UH': {'D_kwDOCqWgoM4AN8UHcon'},\n",
              " 'D_kwDOCqWgoM4AN_7D': {'D_kwDOCqWgoM4AN_7Dcon'},\n",
              " 'D_kwDOCqWgoM4AN_8r': {'D_kwDOCqWgoM4AN_8rcon'},\n",
              " 'D_kwDOCqWgoM4AN_cS': {'D_kwDOCqWgoM4AN_cScon'},\n",
              " 'D_kwDOCqWgoM4ANn3D': {'D_kwDOCqWgoM4ANn3Dcon'},\n",
              " 'D_kwDOCqWgoM4ANo9F': {'D_kwDOCqWgoM4ANo9Fcon'},\n",
              " 'D_kwDOCqWgoM4ANogY': {'D_kwDOCqWgoM4ANogYcon'},\n",
              " 'D_kwDOCqWgoM4ANqDI': {'D_kwDOCqWgoM4ANqDIcon'},\n",
              " 'D_kwDOCqWgoM4ANrAK': {'D_kwDOCqWgoM4ANrAKcon'},\n",
              " 'D_kwDOCqWgoM4ANrY-': {'D_kwDOCqWgoM4ANrY-con'},\n",
              " 'D_kwDOCqWgoM4ANrkB': {'D_kwDOCqWgoM4ANrkBcon'},\n",
              " 'D_kwDOCqWgoM4ANrmA': {'D_kwDOCqWgoM4ANrmAcon'},\n",
              " 'D_kwDOCqWgoM4ANuM0': {'D_kwDOCqWgoM4ANuM0con'},\n",
              " 'D_kwDOCqWgoM4ANuQ1': {'D_kwDOCqWgoM4ANuQ1con'},\n",
              " 'D_kwDOCqWgoM4ANuaZ': {'D_kwDOCqWgoM4ANuaZcon'},\n",
              " 'D_kwDOCqWgoM4ANv6a': {'D_kwDOCqWgoM4ANv6bcon'},\n",
              " 'D_kwDOCqWgoM4ANv6b': {'D_kwDOCqWgoM4ANv6bcon'},\n",
              " 'D_kwDOCqWgoM4ANvwZ': {'D_kwDOCqWgoM4ANvwZcon'},\n",
              " 'D_kwDOCqWgoM4ANwT_': {'D_kwDOCqWgoM4ANwT_con'},\n",
              " 'D_kwDOCqWgoM4ANwn_': {'D_kwDOCqWgoM4ANwn_con'},\n",
              " 'D_kwDOCqWgoM4ANx-L': {'D_kwDOCqWgoM4ANx-Lcon'},\n",
              " 'D_kwDOCqWgoM4ANxgv': {'D_kwDOCqWgoM4ANxgvcon'},\n",
              " 'D_kwDOCqWgoM4ANxrd': {'D_kwDOCqWgoM4ANxrdcon'},\n",
              " 'D_kwDOCqWgoM4ANymu': {'D_kwDOCqWgoM4ANymucon'},\n",
              " 'D_kwDOCqWgoM4ANzLv': {'D_kwDOCqWgoM4ANzLvcon'},\n",
              " 'D_kwDOCqWgoM4AO0Fs': {'D_kwDOCqWgoM4AO0Fscon'},\n",
              " 'D_kwDOCqWgoM4AO0KS': {'D_kwDOCqWgoM4AO0KScon'},\n",
              " 'D_kwDOCqWgoM4AO0kZ': {'D_kwDOCqWgoM4AO0kZcon'},\n",
              " 'D_kwDOCqWgoM4AO1KV': {'D_kwDOCqWgoM4AO1KVcon'},\n",
              " 'D_kwDOCqWgoM4AO1fM': {'D_kwDOCqWgoM4AO1fMcon'},\n",
              " 'D_kwDOCqWgoM4AO1rw': {'D_kwDOCqWgoM4AO1rwcon'},\n",
              " 'D_kwDOCqWgoM4AO2uC': {'D_kwDOCqWgoM4AO2uCcon'},\n",
              " 'D_kwDOCqWgoM4AO2wI': {'D_kwDOCqWgoM4AO2wIcon'},\n",
              " 'D_kwDOCqWgoM4AO3Et': {'D_kwDOCqWgoM4AO3Etcon'},\n",
              " 'D_kwDOCqWgoM4AO3U-': {'D_kwDOCqWgoM4AO3U-con'},\n",
              " 'D_kwDOCqWgoM4AO4EP': {'D_kwDOCqWgoM4AO4EPcon'},\n",
              " 'D_kwDOCqWgoM4AO4uE': {'D_kwDOCqWgoM4AO4uEcon'},\n",
              " 'D_kwDOCqWgoM4AO6Tv': {'D_kwDOCqWgoM4AO6Tvcon'},\n",
              " 'D_kwDOCqWgoM4AO7KD': {'D_kwDOCqWgoM4AO7KDcon'},\n",
              " 'D_kwDOCqWgoM4AO7Ye': {'D_kwDOCqWgoM4AO7Yecon'},\n",
              " 'D_kwDOCqWgoM4AO7sj': {'D_kwDOCqWgoM4AO7sjcon'},\n",
              " 'D_kwDOCqWgoM4AO7tD': {'D_kwDOCqWgoM4AO7tDcon'},\n",
              " 'D_kwDOCqWgoM4AO89y': {'D_kwDOCqWgoM4AO89ycon'},\n",
              " 'D_kwDOCqWgoM4AO97e': {'D_kwDOCqWgoM4AO97econ'},\n",
              " 'D_kwDOCqWgoM4AO9hZ': {'D_kwDOCqWgoM4AO9hZcon'},\n",
              " 'D_kwDOCqWgoM4AOAac': {'D_kwDOCqWgoM4AOAaccon'},\n",
              " 'D_kwDOCqWgoM4AOCkt': {'D_kwDOCqWgoM4AOCktcon'},\n",
              " 'D_kwDOCqWgoM4AODJU': {'D_kwDOCqWgoM4AODJUcon'},\n",
              " 'D_kwDOCqWgoM4AOE7x': {'D_kwDOCqWgoM4AOE7xcon'},\n",
              " 'D_kwDOCqWgoM4AOGUx': {'D_kwDOCqWgoM4AOGUxcon'},\n",
              " 'D_kwDOCqWgoM4AOGVE': {'D_kwDOCqWgoM4AOGVEcon'},\n",
              " 'D_kwDOCqWgoM4AOHL0': {'D_kwDOCqWgoM4AOHL0con'},\n",
              " 'D_kwDOCqWgoM4AOH_m': {'D_kwDOCqWgoM4AOH_mcon'},\n",
              " 'D_kwDOCqWgoM4AOI4h': {'D_kwDOCqWgoM4AOI4hcon'},\n",
              " 'D_kwDOCqWgoM4AOI5p': {'D_kwDOCqWgoM4AOI5pcon'},\n",
              " 'D_kwDOCqWgoM4AOIRx': {'D_kwDOCqWgoM4AOIRxcon'},\n",
              " 'D_kwDOCqWgoM4AOIa1': {'D_kwDOCqWgoM4AOIa1con'},\n",
              " 'D_kwDOCqWgoM4AOIuB': {'D_kwDOCqWgoM4AOIuBcon'},\n",
              " 'D_kwDOCqWgoM4AOJ8D': {'D_kwDOCqWgoM4AOJ8Dcon'},\n",
              " 'D_kwDOCqWgoM4AOJpq': {'D_kwDOCqWgoM4AOJpqcon'},\n",
              " 'D_kwDOCqWgoM4AOKTv': {'D_kwDOCqWgoM4AOKTvcon'},\n",
              " 'D_kwDOCqWgoM4AOKiQ': {'D_kwDOCqWgoM4AOKiQcon'},\n",
              " 'D_kwDOCqWgoM4AOKtd': {'D_kwDOCqWgoM4AOKtdcon'},\n",
              " 'D_kwDOCqWgoM4AOL5N': {'D_kwDOCqWgoM4AOL5Ncon'},\n",
              " 'D_kwDOCqWgoM4AOL9I': {'D_kwDOCqWgoM4AOL9Icon'},\n",
              " 'D_kwDOCqWgoM4AOLA6': {'D_kwDOCqWgoM4AOLA6con'},\n",
              " 'D_kwDOCqWgoM4AOLTw': {'D_kwDOCqWgoM4AOLTwcon'},\n",
              " 'D_kwDOCqWgoM4AOM_J': {'D_kwDOCqWgoM4AOM_Jcon'},\n",
              " 'D_kwDOCqWgoM4AOMiz': {'D_kwDOCqWgoM4AOMizcon'},\n",
              " 'D_kwDOCqWgoM4AOMkC': {'D_kwDOCqWgoM4AOMkCcon'},\n",
              " 'D_kwDOCqWgoM4AONiM': {'D_kwDOCqWgoM4AONiMcon'},\n",
              " 'D_kwDOCqWgoM4AONtA': {'D_kwDOCqWgoM4AONtAcon'},\n",
              " 'D_kwDOCqWgoM4AOO-Y': {'D_kwDOCqWgoM4AOO-Ycon'},\n",
              " 'D_kwDOCqWgoM4AOOuk': {'D_kwDOCqWgoM4AOOukcon'},\n",
              " 'D_kwDOCqWgoM4AOOv9': {'D_kwDOCqWgoM4AOOv9con'},\n",
              " 'D_kwDOCqWgoM4AOP1k': {'D_kwDOCqWgoM4AOP1kcon'},\n",
              " 'D_kwDOCqWgoM4AOPZa': {'D_kwDOCqWgoM4AOPZacon'},\n",
              " 'D_kwDOCqWgoM4AOPhE': {'D_kwDOCqWgoM4AOPhEcon'},\n",
              " 'D_kwDOCqWgoM4AORWw': {'D_kwDOCqWgoM4AORWwcon'},\n",
              " 'D_kwDOCqWgoM4AOSKC': {'D_kwDOCqWgoM4AOSKCcon'},\n",
              " 'D_kwDOCqWgoM4AOSKg': {'D_kwDOCqWgoM4AOSKgcon'},\n",
              " 'D_kwDOCqWgoM4AOT-T': {'D_kwDOCqWgoM4AOT-Tcon'},\n",
              " 'D_kwDOCqWgoM4AOUM0': {'D_kwDOCqWgoM4AOUM0con'},\n",
              " 'D_kwDOCqWgoM4AOUfJ': {'D_kwDOCqWgoM4AOUfJcon'},\n",
              " 'D_kwDOCqWgoM4AOVh6': {'D_kwDOCqWgoM4AOVh6con'},\n",
              " 'D_kwDOCqWgoM4AOW_y': {'D_kwDOCqWgoM4AOW_ycon'},\n",
              " 'D_kwDOCqWgoM4AOXgl': {'D_kwDOCqWgoM4AOXglcon'},\n",
              " 'D_kwDOCqWgoM4AOXqQ': {'D_kwDOCqWgoM4AOXqQcon'},\n",
              " 'D_kwDOCqWgoM4AOY0y': {'D_kwDOCqWgoM4AOY0ycon'},\n",
              " 'D_kwDOCqWgoM4AOYR-': {'D_kwDOCqWgoM4AOYR-con'},\n",
              " 'D_kwDOCqWgoM4AOYvd': {'D_kwDOCqWgoM4AOYvdcon'},\n",
              " 'D_kwDOCqWgoM4AOZEQ': {'D_kwDOCqWgoM4AOZEQcon'},\n",
              " 'D_kwDOCqWgoM4AOZMw': {'D_kwDOCqWgoM4AOZMwcon'},\n",
              " 'D_kwDOCqWgoM4AOa1k': {'D_kwDOCqWgoM4AOa1kcon'},\n",
              " 'D_kwDOCqWgoM4AOaI0': {'D_kwDOCqWgoM4AOaI0con'},\n",
              " 'D_kwDOCqWgoM4AObNj': {'D_kwDOCqWgoM4AObNjcon'},\n",
              " 'D_kwDOCqWgoM4AOb_Z': {'D_kwDOCqWgoM4AOb_Zcon'},\n",
              " 'D_kwDOCqWgoM4AOd26': {'D_kwDOCqWgoM4AOd26con'},\n",
              " 'D_kwDOCqWgoM4AOd6S': {'D_kwDOCqWgoM4AOd6Scon'},\n",
              " 'D_kwDOCqWgoM4AOdFs': {'D_kwDOCqWgoM4AOdFscon'},\n",
              " 'D_kwDOCqWgoM4AOdZ5': {'D_kwDOCqWgoM4AOdZ5con'},\n",
              " 'D_kwDOCqWgoM4AOeB9': {'D_kwDOCqWgoM4AOeB9con'},\n",
              " 'D_kwDOCqWgoM4AOemV': {'D_kwDOCqWgoM4AOemVcon'},\n",
              " 'D_kwDOCqWgoM4AOgOw': {'D_kwDOCqWgoM4AOgOwcon'},\n",
              " 'D_kwDOCqWgoM4AOgPG': {'D_kwDOCqWgoM4AOgPGcon'},\n",
              " 'D_kwDOCqWgoM4AOgZD': {'D_kwDOCqWgoM4AOgZDcon'},\n",
              " 'D_kwDOCqWgoM4AOkGz': {'D_kwDOCqWgoM4AOkGzcon'},\n",
              " 'D_kwDOCqWgoM4AOoRX': {'D_kwDOCqWgoM4AOoRXcon'},\n",
              " 'D_kwDOCqWgoM4AOp8R': {'D_kwDOCqWgoM4AOp8Rcon'},\n",
              " 'D_kwDOCqWgoM4AOqUS': {'D_kwDOCqWgoM4AOqUScon'},\n",
              " 'D_kwDOCqWgoM4AOqoj': {'D_kwDOCqWgoM4AOqojcon'},\n",
              " 'D_kwDOCqWgoM4AOqpi': {'D_kwDOCqWgoM4AOqpicon'},\n",
              " 'D_kwDOCqWgoM4AOrGl': {'D_kwDOCqWgoM4AOrGlcon'},\n",
              " 'D_kwDOCqWgoM4AOsH4': {'D_kwDOCqWgoM4AOsH4con'},\n",
              " 'D_kwDOCqWgoM4AOsH5': {'D_kwDOCqWgoM4AOsH5con'},\n",
              " 'D_kwDOCqWgoM4AOsPR': {'D_kwDOCqWgoM4AOsPRcon'},\n",
              " 'D_kwDOCqWgoM4AOsmI': {'D_kwDOCqWgoM4AOsmIcon'},\n",
              " 'D_kwDOCqWgoM4AOtq7': {'D_kwDOCqWgoM4AOtq7con'},\n",
              " 'D_kwDOCqWgoM4AOtvt': {'D_kwDOCqWgoM4AOtvtcon'},\n",
              " 'D_kwDOCqWgoM4AOuBR': {'D_kwDOCqWgoM4AOuBRcon'},\n",
              " 'D_kwDOCqWgoM4AOuUN': {'D_kwDOCqWgoM4AOuUNcon'},\n",
              " 'D_kwDOCqWgoM4AOuWg': {'D_kwDOCqWgoM4AOuWgcon'},\n",
              " 'D_kwDOCqWgoM4AOwH-': {'D_kwDOCqWgoM4AOwH-con'},\n",
              " 'D_kwDOCqWgoM4AOwQY': {'D_kwDOCqWgoM4AOwQYcon'},\n",
              " 'D_kwDOCqWgoM4AOwcv': {'D_kwDOCqWgoM4AOwcvcon'},\n",
              " 'D_kwDOCqWgoM4AOwkL': {'D_kwDOCqWgoM4AOwkLcon'},\n",
              " 'D_kwDOCqWgoM4AOxVN': {'D_kwDOCqWgoM4AOxVNcon'},\n",
              " 'D_kwDOCqWgoM4AOxzH': {'D_kwDOCqWgoM4AOxzHcon'},\n",
              " 'D_kwDOCqWgoM4AOy9z': {'D_kwDOCqWgoM4AOy9zcon'},\n",
              " 'D_kwDOCqWgoM4AOyUn': {'D_kwDOCqWgoM4AOyUncon'},\n",
              " 'D_kwDOCqWgoM4AOzN_': {'D_kwDOCqWgoM4AOzN_con'},\n",
              " 'D_kwDOCqWgoM4AOzRX': {'D_kwDOCqWgoM4AOzRXcon'},\n",
              " 'D_kwDOCqWgoM4APAR6': {'D_kwDOCqWgoM4APAR6con'},\n",
              " 'D_kwDOCqWgoM4APBbi': {'D_kwDOCqWgoM4APBbicon'},\n",
              " 'D_kwDOCqWgoM4APCns': {'D_kwDOCqWgoM4APCnscon'},\n",
              " 'D_kwDOCqWgoM4APFBW': {'D_kwDOCqWgoM4APFBWcon'},\n",
              " 'D_kwDOCqWgoM4APFIR': {'D_kwDOCqWgoM4APFIRcon'},\n",
              " 'D_kwDOCqWgoM4APFNC': {'D_kwDOCqWgoM4APFNCcon'},\n",
              " 'D_kwDOCqWgoM4APFqd': {'D_kwDOCqWgoM4APFqdcon'},\n",
              " 'D_kwDOCqWgoM4APGS0': {'D_kwDOCqWgoM4APGS0con'},\n",
              " 'D_kwDOCqWgoM4APGyZ': {'D_kwDOCqWgoM4APGyZcon'},\n",
              " 'D_kwDOCqWgoM4APH-L': {'D_kwDOCqWgoM4APH-Lcon'},\n",
              " 'D_kwDOCqWgoM4APJ2u': {'D_kwDOCqWgoM4APJ2ucon'},\n",
              " 'D_kwDOCqWgoM4APM3c': {'D_kwDOCqWgoM4APM3ccon'},\n",
              " 'D_kwDOCqWgoM4APNwR': {'D_kwDOCqWgoM4APNwRcon'},\n",
              " 'D_kwDOCqWgoM4APO41': {'D_kwDOCqWgoM4APO41con'},\n",
              " 'D_kwDOCqWgoM4APOIS': {'D_kwDOCqWgoM4APOIScon'},\n",
              " 'D_kwDOCqWgoM4APOMH': {'D_kwDOCqWgoM4APOMHcon'},\n",
              " 'D_kwDOCqWgoM4APPdA': {'D_kwDOCqWgoM4APPdAcon'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI0MA==': {'MDEwOkRpc2N1c3Npb244MjI0MA==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI0MQ==': {'MDEwOkRpc2N1c3Npb244MjI0MQ==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI0Ng==': {'MDEwOkRpc2N1c3Npb244MjI0Ng==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI1MQ==': {'MDEwOkRpc2N1c3Npb244MjI1MQ==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI1Mg==': {'MDEwOkRpc2N1c3Npb244MjI1Mg==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI1NQ==': {'MDEwOkRpc2N1c3Npb244MjI1NQ==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI1Nw==': {'MDEwOkRpc2N1c3Npb244MjI1Nw==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI2MA==': {'MDEwOkRpc2N1c3Npb244MjI2MA==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI3MQ==': {'MDEwOkRpc2N1c3Npb244MjI3MQ==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI3NA==': {'MDEwOkRpc2N1c3Npb244MjI3NA==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI3Ng==': {'MDEwOkRpc2N1c3Npb244MjI3Ng==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI3OA==': {'MDEwOkRpc2N1c3Npb244MjI3OA==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI4Mg==': {'MDEwOkRpc2N1c3Npb244MjI4Mg==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI4OQ==': {'MDEwOkRpc2N1c3Npb244MjI4OQ==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI5Nw==': {'MDEwOkRpc2N1c3Npb244MjI5Nw==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjI5OA==': {'MDEwOkRpc2N1c3Npb244MjI5OA==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjIwNw==': {'MDEwOkRpc2N1c3Npb244MjIwNw==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjIyMA==': {'MDEwOkRpc2N1c3Npb244MjIyMA==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjIyNw==': {'MDEwOkRpc2N1c3Npb244MjIyNw==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjMwMA==': {'MDEwOkRpc2N1c3Npb244MjMwMA==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MjMwMw==': {'MDEwOkRpc2N1c3Npb244MjMwMw==con'},\n",
              " 'MDEwOkRpc2N1c3Npb244MzU0MA==': {'MDEwOkRpc2N1c3Npb244MzU0MA==con'},\n",
              " 'MDEwOkRpc2N1c3Npb24xNjMyMDI5': {'MDEwOkRpc2N1c3Npb24xNjMyMDI5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24xOTI2Mzgz': {'MDEwOkRpc2N1c3Npb24xOTI2Mzgzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24yMTc5Njk3': {'MDEwOkRpc2N1c3Npb24yMTc5Njk3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkxNTQ4': {'MDEwOkRpc2N1c3Npb24yNzkxNTQ4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkxODc3': {'MDEwOkRpc2N1c3Npb24yNzkxODc3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkxOTMy': {'MDEwOkRpc2N1c3Npb24yNzkxOTMycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkxOTU0': {'MDEwOkRpc2N1c3Npb24yNzkxOTU0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyMjEx': {'MDEwOkRpc2N1c3Npb24yNzkyMjExcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyMjYx': {'MDEwOkRpc2N1c3Npb24yNzkyMjYxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyMzU2': {'MDEwOkRpc2N1c3Npb24yNzkyMzU2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyMzkw': {'MDEwOkRpc2N1c3Npb24yNzkyMzkwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNDcx': {'MDEwOkRpc2N1c3Npb24yNzkyNDcxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNDgy': {'MDEwOkRpc2N1c3Npb24yNzkyNDgycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTAz': {'MDEwOkRpc2N1c3Npb24yNzkyNTAzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTEy': {'MDEwOkRpc2N1c3Npb24yNzkyNTEycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTIz': {'MDEwOkRpc2N1c3Npb24yNzkyNTIzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTM0': {'MDEwOkRpc2N1c3Npb24yNzkyNTM0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTMw': {'MDEwOkRpc2N1c3Npb24yNzkyNTMwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTQ2': {'MDEwOkRpc2N1c3Npb24yNzkyNTQ2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24yNzkyNTU3': {'MDEwOkRpc2N1c3Npb24yNzkyNTU3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMTc5NTk3': {'MDEwOkRpc2N1c3Npb24zMTc5NTk3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMTc5Njcw': {'MDEwOkRpc2N1c3Npb24zMTc5Njcwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjIzNDA4': {'MDEwOkRpc2N1c3Npb24zMjIzNDA4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjMyNTUw': {'MDEwOkRpc2N1c3Npb24zMjMyNTUwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjMyNzk1': {'MDEwOkRpc2N1c3Npb24zMjMyNzk1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjMzMTgw': {'MDEwOkRpc2N1c3Npb24zMjMzMTgwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQ4NTU2': {'MDEwOkRpc2N1c3Npb24zMjQ4NTU2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQ5MTMx': {'MDEwOkRpc2N1c3Npb24zMjQ5MTMxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQ5ODE4': {'MDEwOkRpc2N1c3Npb24zMjQ5ODE4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQyMjE4': {'MDEwOkRpc2N1c3Npb24zMjQyMjE4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQyMjUy': {'MDEwOkRpc2N1c3Npb24zMjQyMjUycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQyNzg5': {'MDEwOkRpc2N1c3Npb24zMjQyNzg5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQzMTM4': {'MDEwOkRpc2N1c3Npb24zMjQzMTM4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjQzNDM5': {'MDEwOkRpc2N1c3Npb24zMjQzNDM5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjU0ODEx': {'MDEwOkRpc2N1c3Npb24zMjU0ODExcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjU1MDkx': {'MDEwOkRpc2N1c3Npb24zMjU1MDkxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjU1NDQ5': {'MDEwOkRpc2N1c3Npb24zMjU1NDQ5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjU2NDQ1': {'MDEwOkRpc2N1c3Npb24zMjU2NDQ1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjU4OTQz': {'MDEwOkRpc2N1c3Npb24zMjU4OTQzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjUwMjU0': {'MDEwOkRpc2N1c3Npb24zMjUwMjU0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjUwMzc1': {'MDEwOkRpc2N1c3Npb24zMjUwMzc1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjUxMDg5': {'MDEwOkRpc2N1c3Npb24zMjUxMDg5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjY0NDkw': {'MDEwOkRpc2N1c3Npb24zMjY0NDkwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjY1OTM5': {'MDEwOkRpc2N1c3Npb24zMjY1OTM5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjY5NDYx': {'MDEwOkRpc2N1c3Npb24zMjY5NDYxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjYwMjk0': {'MDEwOkRpc2N1c3Npb24zMjYwMjk0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjYwODA5': {'MDEwOkRpc2N1c3Npb24zMjYwODA5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjYyMjcy': {'MDEwOkRpc2N1c3Npb24zMjYyMjcycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjc3NDg5': {'MDEwOkRpc2N1c3Npb24zMjc3NDg5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjcwMTE3': {'MDEwOkRpc2N1c3Npb24zMjcwMTE3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjcwMTQ5': {'MDEwOkRpc2N1c3Npb24zMjcwMTQ5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjczNjgz': {'MDEwOkRpc2N1c3Npb24zMjczNjgzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjg1ODg0': {'MDEwOkRpc2N1c3Npb24zMjg1ODg0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjg2MjM2': {'MDEwOkRpc2N1c3Npb24zMjg2MjM2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjg5OTA3': {'MDEwOkRpc2N1c3Npb24zMjg5OTA3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjgxNTg5': {'MDEwOkRpc2N1c3Npb24zMjgxNTg5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk1NTY1': {'MDEwOkRpc2N1c3Npb24zMjk1NTY1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk2NjUz': {'MDEwOkRpc2N1c3Npb24zMjk2NjUzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk2ODM0': {'MDEwOkRpc2N1c3Npb24zMjk2ODM0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk3MTA2': {'MDEwOkRpc2N1c3Npb24zMjk3MTA2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk3NTY3': {'MDEwOkRpc2N1c3Npb24zMjk3NTY3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk3NjM1': {'MDEwOkRpc2N1c3Npb24zMjk3NjM1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjk5MDkz': {'MDEwOkRpc2N1c3Npb24zMjk5MDkzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjkxODc0': {'MDEwOkRpc2N1c3Npb24zMjkxODc0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjkyOTQz': {'MDEwOkRpc2N1c3Npb24zMjkyOTQzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMjkzNzMx': {'MDEwOkRpc2N1c3Npb24zMjkzNzMxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzA0NjU2': {'MDEwOkRpc2N1c3Npb24zMzA0NjU2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzA1ODA1': {'MDEwOkRpc2N1c3Npb24zMzA1ODA1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzA1OTk3': {'MDEwOkRpc2N1c3Npb24zMzA1OTk3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzA4MTQy': {'MDEwOkRpc2N1c3Npb24zMzA4MTQycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzAwMjAw': {'MDEwOkRpc2N1c3Npb24zMzAwMjAwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzAxMTc3': {'MDEwOkRpc2N1c3Npb24zMzAxMTc3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzAxNjgw': {'MDEwOkRpc2N1c3Npb24zMzAxNjgwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzAyNDAy': {'MDEwOkRpc2N1c3Npb24zMzAyNDAycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzE0MDUx': {'MDEwOkRpc2N1c3Npb24zMzE0MDUxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzE1NTUz': {'MDEwOkRpc2N1c3Npb24zMzE1NTUzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzEwODM0': {'MDEwOkRpc2N1c3Npb24zMzEwODM0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzExMzgw': {'MDEwOkRpc2N1c3Npb24zMzExMzgwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzExNDI0': {'MDEwOkRpc2N1c3Npb24zMzExNDI0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzEyMzgz': {'MDEwOkRpc2N1c3Npb24zMzEyMzgzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzEzODI5': {'MDEwOkRpc2N1c3Npb24zMzEzODI5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzI1MzIx': {'MDEwOkRpc2N1c3Npb24zMzI1MzIxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzI2NTIw': {'MDEwOkRpc2N1c3Npb24zMzI2NTIwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzI2NTUw': {'MDEwOkRpc2N1c3Npb24zMzI2NTUwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzI3MTU3': {'MDEwOkRpc2N1c3Npb24zMzI3MTU3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzI5MjQ0': {'MDEwOkRpc2N1c3Npb24zMzI5MjQ0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzIwNDE5': {'MDEwOkRpc2N1c3Npb24zMzIwNDE5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzM0MTk2': {'MDEwOkRpc2N1c3Npb24zMzM0MTk2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzM0MTk4': {'MDEwOkRpc2N1c3Npb24zMzM0MTk4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzM4MDM3': {'MDEwOkRpc2N1c3Npb24zMzM4MDM3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzM5NjY1': {'MDEwOkRpc2N1c3Npb24zMzM5NjY1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzMyMTA0': {'MDEwOkRpc2N1c3Npb24zMzMyMTA0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzMyNzMx': {'MDEwOkRpc2N1c3Npb24zMzMyNzMxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzMzNzUx': {'MDEwOkRpc2N1c3Npb24zMzMzNzUxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzMzODc0': {'MDEwOkRpc2N1c3Npb24zMzMzODc0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ1Njc0': {'MDEwOkRpc2N1c3Npb24zMzQ1Njc0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ2Mzk5': {'MDEwOkRpc2N1c3Npb24zMzQ2Mzk5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ2ODUx': {'MDEwOkRpc2N1c3Npb24zMzQ2ODUxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ3MzEy': {'MDEwOkRpc2N1c3Npb24zMzQ3MzEycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ3MzM1': {'MDEwOkRpc2N1c3Npb24zMzQ3MzM1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ3NDQz': {'MDEwOkRpc2N1c3Npb24zMzQ3NDQzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQ5MDYx': {'MDEwOkRpc2N1c3Npb24zMzQ5MDYxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQxMjI5': {'MDEwOkRpc2N1c3Npb24zMzQxMjI5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzQzNzY5': {'MDEwOkRpc2N1c3Npb24zMzQzNzY5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU0NTUy': {'MDEwOkRpc2N1c3Npb24zMzU0NTUycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU1MTUx': {'MDEwOkRpc2N1c3Npb24zMzU1MTUxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU1MjUz': {'MDEwOkRpc2N1c3Npb24zMzU1MjUzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU4MTIx': {'MDEwOkRpc2N1c3Npb24zMzU4MTIxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU4NDI1': {'MDEwOkRpc2N1c3Npb24zMzU4NDI1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzU5MDcx': {'MDEwOkRpc2N1c3Npb24zMzU5MDcxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUwMzU1': {'MDEwOkRpc2N1c3Npb24zMzUwMzU1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUwODY2': {'MDEwOkRpc2N1c3Npb24zMzUwODY2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUxNDc0': {'MDEwOkRpc2N1c3Npb24zMzUxNDc0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUxNDcy': {'MDEwOkRpc2N1c3Npb24zMzUxNDcycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUxNTAw': {'MDEwOkRpc2N1c3Npb24zMzUxNTAwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzUxNTEz': {'MDEwOkRpc2N1c3Npb24zMzUxNTEzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzY0MTE1': {'MDEwOkRpc2N1c3Npb24zMzY0MTE1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzY1MjM1': {'MDEwOkRpc2N1c3Npb24zMzY1MjM1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzY1OTU1': {'MDEwOkRpc2N1c3Npb24zMzY1OTU1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzY2NjQz': {'MDEwOkRpc2N1c3Npb24zMzY2NjQzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzY4OTgy': {'MDEwOkRpc2N1c3Npb24zMzY4OTgycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzYwODg1': {'MDEwOkRpc2N1c3Npb24zMzYwODg1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzYzNDA2': {'MDEwOkRpc2N1c3Npb24zMzYzNDA2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzc0OTYx': {'MDEwOkRpc2N1c3Npb24zMzc0OTYxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzc4OTQz': {'MDEwOkRpc2N1c3Npb24zMzc4OTQzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzcwNzMw': {'MDEwOkRpc2N1c3Npb24zMzcwNzMwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzcwOTA4': {'MDEwOkRpc2N1c3Npb24zMzcwOTA4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzcxOTM0': {'MDEwOkRpc2N1c3Npb24zMzcxOTM0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzczODE0': {'MDEwOkRpc2N1c3Npb24zMzczODE0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzg0MDEz': {'MDEwOkRpc2N1c3Npb24zMzg0MDEzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzg1NzEw': {'MDEwOkRpc2N1c3Npb24zMzg1NzEwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzg3NTAx': {'MDEwOkRpc2N1c3Npb24zMzg3NTAxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzg5NjMy': {'MDEwOkRpc2N1c3Npb24zMzg5NjMycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzgwMjMw': {'MDEwOkRpc2N1c3Npb24zMzgwMjMwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzgyODA2': {'MDEwOkRpc2N1c3Npb24zMzgyODA2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzgzNzgx': {'MDEwOkRpc2N1c3Npb24zMzgzNzgxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk1MDQ3': {'MDEwOkRpc2N1c3Npb24zMzk1MDQ3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk1MjAw': {'MDEwOkRpc2N1c3Npb24zMzk1MjAwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk2MjAy': {'MDEwOkRpc2N1c3Npb24zMzk2MjAycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk4NjAw': {'MDEwOkRpc2N1c3Npb24zMzk4NjAwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk5MTEw': {'MDEwOkRpc2N1c3Npb24zMzk5MTEwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzk5NDIy': {'MDEwOkRpc2N1c3Npb24zMzk5NDIycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzkwNzAx': {'MDEwOkRpc2N1c3Npb24zMzkwNzAxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzkyMjc2': {'MDEwOkRpc2N1c3Npb24zMzkyMjc2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zMzkyNDAw': {'MDEwOkRpc2N1c3Npb24zMzkyNDAwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA1OTg5': {'MDEwOkRpc2N1c3Npb24zNDA1OTg5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA3MTg3': {'MDEwOkRpc2N1c3Npb24zNDA3MTg3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA3NzMw': {'MDEwOkRpc2N1c3Npb24zNDA3NzMwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA3ODk2': {'MDEwOkRpc2N1c3Npb24zNDA3ODk2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA4NjYw': {'MDEwOkRpc2N1c3Npb24zNDA4NjYwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA5MDY5': {'MDEwOkRpc2N1c3Npb24zNDA5MDY5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDA5MzIw': {'MDEwOkRpc2N1c3Npb24zNDA5MzIwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDAxNjI5': {'MDEwOkRpc2N1c3Npb24zNDAxNjI5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDAyOTgx': {'MDEwOkRpc2N1c3Npb24zNDAyOTgxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDE5NjM3': {'MDEwOkRpc2N1c3Npb24zNDE5NjM3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDE5OTIz': {'MDEwOkRpc2N1c3Npb24zNDE5OTIzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI1NDc2': {'MDEwOkRpc2N1c3Npb24zNDI1NDc2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI1OTU0': {'MDEwOkRpc2N1c3Npb24zNDI1OTU0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI2NjI5': {'MDEwOkRpc2N1c3Npb24zNDI2NjI5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI2NzQy': {'MDEwOkRpc2N1c3Npb24zNDI2NzQycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI3OTg1': {'MDEwOkRpc2N1c3Npb24zNDI3OTg1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI3OTk4': {'MDEwOkRpc2N1c3Npb24zNDI3OTk4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDI4MDcz': {'MDEwOkRpc2N1c3Npb24zNDI4MDczcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDIwNDgw': {'MDEwOkRpc2N1c3Npb24zNDIwNDgwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDIxMTQ4': {'MDEwOkRpc2N1c3Npb24zNDIxMTQ4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDIxNDU1': {'MDEwOkRpc2N1c3Npb24zNDIxNDU1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDIyMTky': {'MDEwOkRpc2N1c3Npb24zNDIyMTkycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDMxMzAw': {'MDEwOkRpc2N1c3Npb24zNDMxMzAwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDMxNDM4': {'MDEwOkRpc2N1c3Npb24zNDMxNDM4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDMzMTY2': {'MDEwOkRpc2N1c3Npb24zNDMzMTY2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDMzNzM4': {'MDEwOkRpc2N1c3Npb24zNDMzNzM4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ1MDEz': {'MDEwOkRpc2N1c3Npb24zNDQ1MDEzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ1Mzg2': {'MDEwOkRpc2N1c3Npb24zNDQ1Mzg2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ2Mjkz': {'MDEwOkRpc2N1c3Npb24zNDQ2Mjkzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ2Njgz': {'MDEwOkRpc2N1c3Npb24zNDQ2Njgzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ3MDYz': {'MDEwOkRpc2N1c3Npb24zNDQ3MDYzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQ5MjE2': {'MDEwOkRpc2N1c3Npb24zNDQ5MjE2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQwNDk0': {'MDEwOkRpc2N1c3Npb24zNDQwNDk0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQwNTU3': {'MDEwOkRpc2N1c3Npb24zNDQwNTU3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQxODI1': {'MDEwOkRpc2N1c3Npb24zNDQxODI1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDQzODk2': {'MDEwOkRpc2N1c3Npb24zNDQzODk2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDU0NTI5': {'MDEwOkRpc2N1c3Npb24zNDU0NTI5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDU2MjU5': {'MDEwOkRpc2N1c3Npb24zNDU2MjU5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDU5MDQx': {'MDEwOkRpc2N1c3Npb24zNDU5MDQxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDU5Mzc4': {'MDEwOkRpc2N1c3Npb24zNDU5Mzc4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDU5NjMw': {'MDEwOkRpc2N1c3Npb24zNDU5NjMwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDUxOTU2': {'MDEwOkRpc2N1c3Npb24zNDUxOTU2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDY2OTI3': {'MDEwOkRpc2N1c3Npb24zNDY2OTI3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDY5NjEy': {'MDEwOkRpc2N1c3Npb24zNDY5NjEycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDYzNjQy': {'MDEwOkRpc2N1c3Npb24zNDYzNjQycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDc2ODEz': {'MDEwOkRpc2N1c3Npb24zNDc2ODEzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDc3MDE4': {'MDEwOkRpc2N1c3Npb24zNDc3MDE4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDc5ODU4': {'MDEwOkRpc2N1c3Npb24zNDc5ODU4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDcwNjIz': {'MDEwOkRpc2N1c3Npb24zNDcwNjIzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDcwNzE3': {'MDEwOkRpc2N1c3Npb24zNDcwNzE3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDcxODUx': {'MDEwOkRpc2N1c3Npb24zNDcxODUxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDczMDQy': {'MDEwOkRpc2N1c3Npb24zNDczMDQycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDg2Mzk2': {'MDEwOkRpc2N1c3Npb24zNDg2Mzk2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDg4MDgy': {'MDEwOkRpc2N1c3Npb24zNDg4MDgycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDg4Mjgx': {'MDEwOkRpc2N1c3Npb24zNDg4Mjgxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDg5NzQ0': {'MDEwOkRpc2N1c3Npb24zNDg5NzQ0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDgwNTAw': {'MDEwOkRpc2N1c3Npb24zNDgwNTAwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDkwMDQ4': {'MDEwOkRpc2N1c3Npb24zNDkwMDQ4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNDkwOTk3': {'MDEwOkRpc2N1c3Npb24zNDkwOTk3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTA0NjA3': {'MDEwOkRpc2N1c3Npb24zNTA0NjA3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTA2MDQ1': {'MDEwOkRpc2N1c3Npb24zNTA2MDQ1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTA3ODQ0': {'MDEwOkRpc2N1c3Npb24zNTA3ODQ0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTA5MDI4': {'MDEwOkRpc2N1c3Npb24zNTA5MDI4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTAzNDg3': {'MDEwOkRpc2N1c3Npb24zNTAzNDg3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTAzNTI2': {'MDEwOkRpc2N1c3Npb24zNTAzNTI2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTE0MTA0': {'MDEwOkRpc2N1c3Npb24zNTE0MTA0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTE2MjI1': {'MDEwOkRpc2N1c3Npb24zNTE2MjI1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTE3NDMw': {'MDEwOkRpc2N1c3Npb24zNTE3NDMwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTE3NTEx': {'MDEwOkRpc2N1c3Npb24zNTE3NTExcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTE4OTIx': {'MDEwOkRpc2N1c3Npb24zNTE4OTIxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTExMzgz': {'MDEwOkRpc2N1c3Npb24zNTExMzgzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTEzMDQ5': {'MDEwOkRpc2N1c3Npb24zNTEzMDQ5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTI3MDA1': {'MDEwOkRpc2N1c3Npb24zNTI3MDA1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTI4Mzcw': {'MDEwOkRpc2N1c3Npb24zNTI4Mzcwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTIxMjUy': {'MDEwOkRpc2N1c3Npb24zNTIxMjUycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTIxNDY5': {'MDEwOkRpc2N1c3Npb24zNTIxNDY5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTIyMjk1': {'MDEwOkRpc2N1c3Npb24zNTIyMjk1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTM0Nzc4': {'MDEwOkRpc2N1c3Npb24zNTM0Nzc4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTM1Mzkx': {'MDEwOkRpc2N1c3Npb24zNTM1Mzkxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTM2MDY5': {'MDEwOkRpc2N1c3Npb24zNTM2MDY5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTM4ODc2': {'MDEwOkRpc2N1c3Npb24zNTM4ODc2con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTMxMDk5': {'MDEwOkRpc2N1c3Npb24zNTMxMDk5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQ0NDA4': {'MDEwOkRpc2N1c3Npb24zNTQ0NDA4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQ0NzQ3': {'MDEwOkRpc2N1c3Npb24zNTQ0NzQ3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQ4NzY1': {'MDEwOkRpc2N1c3Npb24zNTQ4NzY1con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQwNzY5': {'MDEwOkRpc2N1c3Npb24zNTQwNzY5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQxNDIw': {'MDEwOkRpc2N1c3Npb24zNTQxNDIwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQxODQw': {'MDEwOkRpc2N1c3Npb24zNTQxODQwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQxOTY5': {'MDEwOkRpc2N1c3Npb24zNTQxOTY5con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTQzNzYy': {'MDEwOkRpc2N1c3Npb24zNTQzNzYycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTU1NjYz': {'MDEwOkRpc2N1c3Npb24zNTU1NjYzcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTU2MTc0': {'MDEwOkRpc2N1c3Npb24zNTU2MTc0con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTU3MTEw': {'MDEwOkRpc2N1c3Npb24zNTU3MTEwcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTU4NjIx': {'MDEwOkRpc2N1c3Npb24zNTU4NjIxcon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTUxNDE3': {'MDEwOkRpc2N1c3Npb24zNTUxNDE3con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTY2Nzgy': {'MDEwOkRpc2N1c3Npb24zNTY2Nzgycon'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTY4NDY4': {'MDEwOkRpc2N1c3Npb24zNTY4NDY4con'},\n",
              " 'MDEwOkRpc2N1c3Npb24zNTYyMjU2': {'MDEwOkRpc2N1c3Npb24zNTYyMjU2con'}}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ir_eval = InformationRetrievalEvaluator(\n",
        "    ir_queries, ir_corpus, ir_relevant_docs\n",
        ")"
      ],
      "metadata": {
        "id": "nQDHaYxCytGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ir_eval(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pdxx9TTHyvLr",
        "outputId": "e7ba48cc-5952-47b9-e009-5763fd2b07d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3492184893160091"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding Context"
      ],
      "metadata": {
        "id": "QngTM3JJ2g6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dv = pd.DataFrame()\n",
        "for index , row in dataset_for_model.iterrows():\n",
        "  eval_dv= eval_dv.append({\n",
        "      \"id\":row[\"id\"],\n",
        "      \"encoding\":model.encode(row['context']).tolist(),\n",
        "      \"answer\": row[\"answer\"],\n",
        "      \"context\":row[\"context\"]\n",
        "  } , ignore_index = True)\n",
        "  \n"
      ],
      "metadata": {
        "id": "JqGwY1m92im2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eval_dv.head())\n",
        "print(eval_dv.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "6VxBRErx4MQc",
        "outputId": "880eed4b-93c1-4b0b-d8d0-d69ebe912cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                             id  \\\n",
              "\u001b[1;36m0\u001b[0m  MDEwOkRpc2N1c3Npb24zNTE3NTEx   \n",
              "\u001b[1;36m1\u001b[0m  MDEwOkRpc2N1c3Npb24zNDI2NzQy   \n",
              "\u001b[1;36m2\u001b[0m  MDEwOkRpc2N1c3Npb24zNTQ4NzY1   \n",
              "\u001b[1;36m3\u001b[0m  MDEwOkRpc2N1c3Npb24zNDIyMTky   \n",
              "\u001b[1;36m4\u001b[0m            D_kwDOCqWgoM4AONtA   \n",
              "\n",
              "                                            encoding  \\\n",
              "\u001b[1;36m0\u001b[0m  \u001b[1m[\u001b[0m\u001b[1;36m0.2564251720905304\u001b[0m, \u001b[1;36m-0.7607596516609192\u001b[0m, \u001b[1;36m-0.3\u001b[0m\u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m1\u001b[0m  \u001b[1m[\u001b[0m\u001b[1;36m-0.08691879361867905\u001b[0m, \u001b[1;36m-0.4392246901988983\u001b[0m, \u001b[1;36m-0\u001b[0m\u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m2\u001b[0m  \u001b[1m[\u001b[0m\u001b[1;36m0.09483443945646286\u001b[0m, \u001b[1;36m-0.3724042475223541\u001b[0m, \u001b[1;36m-0\u001b[0m\u001b[33m...\u001b[0m.   \n",
              "\u001b[1;36m3\u001b[0m  \u001b[1m[\u001b[0m\u001b[1;36m0.2794538736343384\u001b[0m, \u001b[1;36m-0.4791562855243683\u001b[0m, \u001b[1;36m-0.2\u001b[0m\u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m4\u001b[0m  \u001b[1m[\u001b[0m\u001b[1;36m0.3358835279941559\u001b[0m, \u001b[1;36m-0.3714536726474762\u001b[0m, \u001b[1;36m-0.4\u001b[0m\u001b[33m...\u001b[0m   \n",
              "\n",
              "                                              answer  \\\n",
              "\u001b[1;36m0\u001b[0m  What Lightning version are you using?\\nThe fas\u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m1\u001b[0m  Hi\\nPyTorch Lightning \u001b[1;36m0.4\u001b[0m.\u001b[1;36m6\u001b[0m is extremely old. \u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m2\u001b[0m  It's because lightning instantiates the Lightn\u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m3\u001b[0m  python 4_pretrain_encoder.py --gpus \u001b[1;36m2\u001b[0m, --max_e\u001b[33m...\u001b[0m   \n",
              "\u001b[1;36m4\u001b[0m  Dear @mishooax,\\nYou are returning the batch f\u001b[33m...\u001b[0m   \n",
              "\n",
              "                                             context  \n",
              "\u001b[1;36m0\u001b[0m  how can i solve this problem, my net is DB,  d\u001b[33m...\u001b[0m  \n",
              "\u001b[1;36m1\u001b[0m  There is no train.test \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m in the lower version\u001b[33m...\u001b[0m  \n",
              "\u001b[1;36m2\u001b[0m  I am defining a simple multi-class BERT classi\u001b[33m...\u001b[0m  \n",
              "\u001b[1;36m3\u001b[0m  Hi, I have \u001b[1;36m4\u001b[0m gpus on my machine. I want to sel\u001b[33m...\u001b[0m  \n",
              "\u001b[1;36m4\u001b[0m  hi all,\\nMy model validation code \u001b[1m(\u001b[0msee below\u001b[1m)\u001b[0m \u001b[33m...\u001b[0m  \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                             id  \\\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  MDEwOkRpc2N1c3Npb24zNTE3NTEx   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  MDEwOkRpc2N1c3Npb24zNDI2NzQy   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  MDEwOkRpc2N1c3Npb24zNTQ4NzY1   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  MDEwOkRpc2N1c3Npb24zNDIyMTky   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>            D_kwDOCqWgoM4AONtA   \n",
              "\n",
              "                                            encoding  \\\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2564251720905304</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7607596516609192</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.08691879361867905</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4392246901988983</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09483443945646286</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3724042475223541</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>.   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2794538736343384</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4791562855243683</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>  <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3358835279941559</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3714536726474762</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "\n",
              "                                              answer  \\\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  What Lightning version are you using?\\nThe fas<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  Hi\\nPyTorch Lightning <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> is extremely old. <span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  It's because lightning instantiates the Lightn<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  python 4_pretrain_encoder.py --gpus <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, --max_e<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>  Dear @mishooax,\\nYou are returning the batch f<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   \n",
              "\n",
              "                                             context  \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  how can i solve this problem, my net is DB,  d<span style=\"color: #808000; text-decoration-color: #808000\">...</span>  \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  There is no train.test <span style=\"font-weight: bold\">()</span> in the lower version<span style=\"color: #808000; text-decoration-color: #808000\">...</span>  \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  I am defining a simple multi-class BERT classi<span style=\"color: #808000; text-decoration-color: #808000\">...</span>  \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  Hi, I have <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> gpus on my machine. I want to sel<span style=\"color: #808000; text-decoration-color: #808000\">...</span>  \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>  hi all,\\nMy model validation code <span style=\"font-weight: bold\">(</span>see below<span style=\"font-weight: bold\">)</span> <span style=\"color: #808000; text-decoration-color: #808000\">...</span>  \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m(\u001b[0m\u001b[1;36m428\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">428</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the Index"
      ],
      "metadata": {
        "id": "RdSpgtMU5q49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"d77b604e-7a23-4cb7-9b20-3ba3c3c7cc6f\"\n",
        "\n",
        "pinecone.init(api_key = API_KEY , environment='us-west1-gcp')\n"
      ],
      "metadata": {
        "id": "lPt6BmAZ4KC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"github-question-answer\" not in pinecone.list_indexes():\n",
        "  pinecone.create_index(\n",
        "      name=\"github-question-answer\" , dimension=model.get_sentence_embedding_dimension(), metric='cosine'\n",
        "    )"
      ],
      "metadata": {
        "id": "be9OYXvs6J6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_sentence_embedding_dimension()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjuDA4Oe68qK",
        "outputId": "59b457aa-1ece-411b-fbe2-ba9ea65e0455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Populate the index"
      ],
      "metadata": {
        "id": "6_7JbF037EVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = pinecone.Index('github-question-answer')"
      ],
      "metadata": {
        "id": "0ayLROSb7Gd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm  # progress bar\n",
        "\n",
        "upserts = [(v['id'], v['encoding'], {'text': \"Ans:\"+v[\"answer\"]+\"\\nContext:\"+v[\"context\"][:150]}) for row_index , v in eval_dv.iterrows()]\n",
        "print(len(eval_dv))\n",
        "# now upsert in chunks\n",
        "for i in tqdm(range(0, len(upserts), 50)):\n",
        "    i_end = i + 50\n",
        "    if i_end > len(upserts): i_end = len(upserts)\n",
        "    index.upsert(vectors=upserts[i:i_end])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "a12bfa112ac84ee0b8b509fb65998393",
            "4726fede19ae4b2eb347c04204642db5",
            "bc23e5310ee74c1383f5f5848bfdd726",
            "8b757aa8c33d4415955a4b76b946e4ac",
            "675a1e3451ff4ec6bf85e78dccd35a70",
            "198151522fb54fd8a43e6abf9fa3e437",
            "84ae291ead2845549a4fa7c9b8e3fd52",
            "651e59d62c024217b8940effb613be22",
            "144ea1b331cf46c68c7690b7f601b456",
            "9c8ceda11d964734af9309f7e05ac28f",
            "4664f8aea9db474d918d4c86887736c6"
          ]
        },
        "id": "FhREO8EJGNLl",
        "outputId": "9c912c1c-2570-45f2-9271-f7503d02303f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m428\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">428</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/9 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a12bfa112ac84ee0b8b509fb65998393"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making Query"
      ],
      "metadata": {
        "id": "pZ6frlNwGzXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query= \"What are hooks in pytorch lightning?\"\n",
        "xq = model.encode([query]).tolist()"
      ],
      "metadata": {
        "id": "nvtgFucbIi2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xc = index.query(xq , top_k = 3 , include_metadata = True)\n",
        "pprint(xc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "KgqXsFhLI19K",
        "outputId": "172b59a9-b62d-4f6d-f5dd-eba03c0311de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-37f296761bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxq\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0minclude_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Citation"
      ],
      "metadata": {
        "id": "eB96KiaJbf0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@inproceedings{reimers-2019-sentence-bert,\n",
        "    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n",
        "    author = \"Reimers, Nils and Gurevych, Iryna\",\n",
        "    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n",
        "    month = \"11\",\n",
        "    year = \"2019\",\n",
        "    publisher = \"Association for Computational Linguistics\",\n",
        "    url = \"https://arxiv.org/abs/1908.10084\",\n",
        "}"
      ],
      "metadata": {
        "id": "1pZaxb-Ebg7L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "7a4d39d9-03d9-4bca-ff43-348cf44a409d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-d8a27af54596>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    @inproceedings{reimers-2019-sentence-bert,\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}